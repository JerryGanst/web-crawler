"""
Plasway è¡Œä¸šæ¶ˆæ¯å¤šåˆ†åŒºçˆ¬è™«
æ”¯æŒå¤šåˆ†åŒºã€åˆ†é¡µã€ç®€å•çš„æ—¶é—´è§£æï¼ˆç›¸å¯¹/ç»å¯¹ï¼‰
ä¼˜å…ˆä½¿ç”¨ AppleScript æ§åˆ¶ Chrome è·å–é¡µé¢ï¼Œå›é€€åˆ° requests
"""
import time
import random
import platform
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set

from bs4 import BeautifulSoup
from urllib.parse import urlparse, urlunparse

from .base import BaseScraper

logger = logging.getLogger(__name__)


class PlaswaySectionScraper(BaseScraper):
    """
    æŒ‰åˆ†åŒºé…ç½®çš„ Plasway è¡Œä¸šæ–°é—»çˆ¬è™«
    é…ç½®ç¤ºä¾‹ï¼ˆcustom_scrapers.yamlï¼‰ï¼š
    plasway_industry:
      sections:
        - name: "market"
          url_template: "https://plasway.com/news/market?web=new&page={page}"
          container: ".news-item"
          fields:
            title: "h1 a"
            url: "h1 a"
            time: ".item-bottom p:nth-of-type(2) span:nth-of-type(1)"
            summary: ".item-content"
    """

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None):
        config = config or {}
        config.setdefault("display_name", "Plaswayè¡Œä¸šæ¶ˆæ¯")
        config.setdefault("category", "finance")
        super().__init__(name, config)

        self.sections: List[Dict[str, Any]] = config.get("sections", [])
        self.max_pages: int = config.get("max_pages", 3)
        self.date_cutoff_days: Optional[int] = config.get("date_cutoff_days", 7)
        
        # çˆ¬å–æ¨¡å¼ï¼šapplescript / requests / auto
        # auto = macOS ä¸Šä¼˜å…ˆ AppleScriptï¼Œå…¶ä»–ç³»ç»Ÿç”¨ requests
        self.scrape_mode: str = config.get("scrape_mode", "auto")
        self._applescript_available: Optional[bool] = None
        
        # åæ£€æµ‹ä¼˜åŒ–å‚æ•°
        self.max_requests_per_run: int = config.get("max_requests_per_run", 20)  # å•æ¬¡è¿è¡Œæœ€å¤§è¯·æ±‚æ•°
        self.shuffle_sections: bool = config.get("shuffle_sections", True)  # éšæœºåŒ– section é¡ºåº
        self.skip_probability: float = config.get("skip_probability", 0.1)  # 10% æ¦‚ç‡è·³è¿‡æŸé¡µï¼ˆæ¨¡æ‹Ÿäººç±»ï¼‰
        self.min_delay: float = config.get("min_delay", 2.0)  # æœ€å°å»¶è¿Ÿ
        self.max_delay: float = config.get("max_delay", 5.0)  # æœ€å¤§å»¶è¿Ÿ
        self._request_count: int = 0  # è¯·æ±‚è®¡æ•°å™¨

    def _load_applescript_module(self):
        """åŠ¨æ€åŠ è½½ applescript æ¨¡å—ï¼ˆç»•è¿‡ __init__.py çš„ selenium ä¾èµ–ï¼‰"""
        import importlib.util
        import os
        
        module_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            "pacong", "browser", "applescript.py"
        )
        spec = importlib.util.spec_from_file_location("applescript", module_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        return module

    def _check_applescript_available(self) -> bool:
        """æ£€æŸ¥ AppleScript æ˜¯å¦å¯ç”¨ï¼ˆmacOS + Chrome è¿è¡Œä¸­ï¼‰"""
        if self._applescript_available is not None:
            return self._applescript_available
        
        if platform.system() != "Darwin":
            self._applescript_available = False
            return False
        
        try:
            applescript = self._load_applescript_module()
            if not applescript.chrome_check_running():
                logger.info("ğŸŒ Chrome æœªè¿è¡Œï¼Œå°è¯•å¯åŠ¨...")
                if not applescript.chrome_start_if_needed():
                    logger.warning("âš ï¸ Chrome å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ° requests")
                    self._applescript_available = False
                    return False
            self._applescript_available = True
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ AppleScript ä¸å¯ç”¨: {e}ï¼Œå›é€€åˆ° requests")
            self._applescript_available = False
            return False

    def _fetch_with_applescript(self, url: str, wait_seconds: int = 8) -> Optional[str]:
        """ä½¿ç”¨ AppleScript æ§åˆ¶ Chrome è·å–é¡µé¢ HTML"""
        try:
            applescript = self._load_applescript_module()
            
            # å¯¼èˆªåˆ° URLï¼ˆå¤ç”¨å½“å‰ Tabï¼Œé¿å…æ‰“å¼€å¤ªå¤šçª—å£ï¼‰
            navigate_script = f'''
            tell application "Google Chrome"
                if not (exists window 1) then
                    make new window
                end if
                set URL of active tab of front window to "{url}"
            end tell
            '''
            applescript.execute_applescript(navigate_script)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(wait_seconds)
            
            # è·å– HTML
            get_html_script = '''
            tell application "Google Chrome"
                execute active tab of front window javascript "document.documentElement.outerHTML"
            end tell
            '''
            html_content = applescript.execute_applescript(get_html_script)
            
            if html_content:
                logger.debug(f"âœ… AppleScript è·å– {len(html_content)} å­—èŠ‚")
                return html_content
            return None
            
        except Exception as e:
            logger.warning(f"âš ï¸ AppleScript è¯·æ±‚å¤±è´¥: {e}")
            return None

    def _should_use_applescript(self) -> bool:
        """å†³å®šæ˜¯å¦ä½¿ç”¨ AppleScript"""
        if self.scrape_mode == "requests":
            return False
        if self.scrape_mode == "applescript":
            return self._check_applescript_available()
        # auto æ¨¡å¼ï¼šmacOS ä¸Šä¼˜å…ˆ AppleScript
        return self._check_applescript_available()

    def _human_like_delay(self, base_min: float = None, base_max: float = None):
        """æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„éšæœºå»¶è¿Ÿï¼Œå¶å°”æœ‰è¾ƒé•¿åœé¡¿"""
        min_d = base_min or self.min_delay
        max_d = base_max or self.max_delay
        
        # 5% æ¦‚ç‡æœ‰è¾ƒé•¿åœé¡¿ï¼ˆæ¨¡æ‹Ÿäººç±»é˜…è¯»/åˆ†å¿ƒï¼‰
        if random.random() < 0.05:
            delay = random.uniform(8.0, 15.0)
            logger.debug(f"ğŸ’¤ æ¨¡æ‹Ÿäººç±»è¾ƒé•¿åœé¡¿: {delay:.1f}s")
        else:
            delay = random.uniform(min_d, max_d)
        
        time.sleep(delay)

    def scrape(self) -> List[Dict[str, Any]]:
        items: List[Dict[str, Any]] = []
        seen_titles: Set[str] = set()
        self._request_count = 0  # é‡ç½®è®¡æ•°å™¨
        
        use_applescript = self._should_use_applescript()
        if use_applescript:
            print(f"  ğŸ ä½¿ç”¨ AppleScript æ¨¡å¼çˆ¬å– Plasway")
        else:
            print(f"  ğŸ“¡ ä½¿ç”¨ Requests æ¨¡å¼çˆ¬å– Plasway")
        
        # éšæœºåŒ– section é¡ºåºï¼Œæ‰“ç ´å›ºå®šè®¿é—®æ¨¡å¼
        sections_to_scrape = list(self.sections)
        if self.shuffle_sections:
            random.shuffle(sections_to_scrape)
            logger.debug(f"ğŸ”€ Section é¡ºåºå·²éšæœºåŒ–")

        for rule in sections_to_scrape:
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¯·æ±‚ä¸Šé™
            if self._request_count >= self.max_requests_per_run:
                logger.info(f"âš ï¸ è¾¾åˆ°è¯·æ±‚ä¸Šé™ ({self.max_requests_per_run})ï¼Œæå‰ç»“æŸ")
                break
            
            section_name = rule.get("name", "")
            url_tmpl = rule.get("url_template")
            if not url_tmpl:
                continue

            for page in range(1, self.max_pages + 1):
                # æ£€æŸ¥è¯·æ±‚ä¸Šé™
                if self._request_count >= self.max_requests_per_run:
                    break
                
                # éšæœºè·³è¿‡æŸäº›é¡µé¢ï¼ˆæ¨¡æ‹Ÿäººç±»ä¸ä¼šçœ‹å®Œæ¯ä¸€é¡µï¼‰
                if page > 1 and random.random() < self.skip_probability:
                    logger.debug(f"â© éšæœºè·³è¿‡ {section_name} ç¬¬ {page} é¡µ")
                    continue
                
                url = url_tmpl.format(page=page)
                html_content: Optional[str] = None
                
                if use_applescript:
                    # AppleScript æ¨¡å¼
                    html_content = self._fetch_with_applescript(url, wait_seconds=8)
                    if not html_content:
                        # AppleScript å¤±è´¥ï¼Œå›é€€åˆ° requests
                        logger.warning(f"âš ï¸ AppleScript å¤±è´¥ï¼Œå›é€€ requests: {url}")
                        resp = self.fetch(url)
                        html_content = resp.text if resp else None
                else:
                    # Requests æ¨¡å¼
                    resp = self.fetch(url)
                    html_content = resp.text if resp else None
                
                self._request_count += 1
                
                if not html_content:
                    break

                batch = self._parse_page(html_content, rule, section_name, seen_titles)
                if not batch:
                    break

                # å¦‚æœå…¨éƒ¨è¿‡æ—§ä¸”è®¾ç½®äº†æˆªæ­¢å¤©æ•°ï¼Œåˆ™æå‰åœæ­¢
                if self.date_cutoff_days:
                    newest_recent = any(
                        not self._is_older_than_cutoff(it.get("timestamp"))
                        for it in batch
                    )
                    if not newest_recent:
                        break

                items.extend(batch)
                
                # äººç±»åŒ–å»¶è¿Ÿï¼ˆAppleScript æ¨¡å¼ç•¥çŸ­ï¼Œå› ä¸ºå·²æœ‰é¡µé¢åŠ è½½ç­‰å¾…ï¼‰
                if use_applescript:
                    self._human_like_delay(1.0, 3.0)
                else:
                    self._human_like_delay()

            # Section ä¹‹é—´é¢å¤–ç­‰å¾…ï¼ˆæ›´é•¿ï¼‰
            self._human_like_delay(3.0, 6.0)
        
        logger.info(f"âœ… å®Œæˆ {self._request_count} ä¸ªè¯·æ±‚ï¼Œè·å– {len(items)} æ¡æ•°æ®")
        return [self.standardize_item(it) for it in items]

    def _parse_page(
        self,
        html: str,
        rule: Dict[str, Any],
        section_name: str,
        seen_titles: Set[str],
    ) -> List[Dict[str, Any]]:
        container_selector = rule.get("container", ".news-item")
        fields = rule.get("fields", {})
        title_sel = fields.get("title", "h1 a")
        url_sel = fields.get("url", "h1 a")
        time_sel = fields.get("time")
        summary_sel = fields.get("summary")
        source_sel = fields.get("source")

        soup = BeautifulSoup(html, "html.parser")
        elements = soup.select(container_selector)
        results: List[Dict[str, Any]] = []

        for elem in elements:
            # åŸºç¡€é“¾æ¥
            anchor = elem.select_one(url_sel)
            if not anchor:
                continue

            original_link = anchor.get("href", "")
            title = anchor.get_text(strip=True)

            # æœ‰äº›é¡µé¢ä¸­ <a> æ–‡æœ¬ä¸ºç©ºï¼Œæ ‡é¢˜å®é™…åœ¨æ‘˜è¦åŒºåŸŸçš„ç¬¬ä¸€è¡Œï¼Œåšé™çº§å›é€€
            if not title:
                fallback_sel = fields.get("title_fallback") or ".item-content span"
                fb_el = elem.select_one(fallback_sel)
                if fb_el:
                    full_text = fb_el.get_text(strip=True)
                    # æ ‡é¢˜é€šå¸¸æ˜¯ç¬¬ä¸€è¡Œï¼Œç”¨æ¢è¡Œæˆ–å¤šç©ºæ ¼åˆ†éš”
                    # å–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜ï¼ˆé™åˆ¶é•¿åº¦é¿å…æ‹¿åˆ°æ•´ä¸ªsummaryï¼‰
                    first_line = full_text.split('\n')[0].strip()
                    # å¦‚æœç¬¬ä¸€è¡Œä»ç„¶å¤ªé•¿ï¼ˆè¶…è¿‡100å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯ç”¨ç©ºæ ¼åˆ†éš”
                    if len(first_line) > 100:
                        parts = first_line.split('   ')  # Plaswayç”¨å¤šç©ºæ ¼åˆ†éš”
                        first_line = parts[0].strip() if parts else first_line[:100]
                    title = first_line if len(first_line) <= 100 else first_line[:100]

            if not title or not original_link or title in seen_titles:
                continue

            seen_titles.add(title)

            published_at = None
            if time_sel:
                time_el = elem.select_one(time_sel)
                if time_el:
                    published_at = self._parse_time_text(time_el.get_text(strip=True))

            summary = None
            if summary_sel:
                sum_el = elem.select_one(summary_sel)
                if sum_el:
                    summary = sum_el.get_text(strip=True)

            source = None
            if source_sel:
                src_el = elem.select_one(source_sel)
                if src_el:
                    source = src_el.get_text(strip=True)

            # ç”Ÿæˆå”¯ä¸€ IDï¼ˆç”¨äºé˜…è¯»å™¨ï¼‰
            news_id = hashlib.md5(f"{title}{original_link}".encode()).hexdigest()[:12]
            
            # æå–æ–‡ç« å†…å®¹ï¼ˆæ‘˜è¦ + æ­£æ–‡ï¼‰
            full_content = self._extract_article_content(elem, summary_sel)
            
            # å†³å®šæœ€ç»ˆ URLï¼šæœ‰å†…å®¹ç”¨é˜…è¯»å™¨ï¼Œå¦åˆ™ç”¨åˆ†ç±»é¡µ
            if full_content:
                final_url = f"http://localhost:8000/api/reader/{news_id}"
                content_available = True
            else:
                final_url = self._normalize_url(original_link, section_name)
                content_available = False
            
            item = {
                "title": title,
                "url": final_url,
                "id": news_id,
                "extra": {
                    "section": section_name,
                    "content_available": content_available,
                    "original_url": original_link,
                },
            }
            
            # ä¿å­˜å†…å®¹åˆ°ç¼“å­˜ï¼ˆç”¨äºé˜…è¯»å™¨ï¼‰
            if full_content:
                self._save_content_to_cache(news_id, {
                    "title": title,
                    "content": full_content,
                    "section": section_name,
                    "source": source or "Plasway",
                    "timestamp": published_at.isoformat() if published_at else None,
                    "original_url": original_link,
                })
            
            if published_at:
                item["timestamp"] = published_at.isoformat()
            if summary:
                item["extra"]["summary"] = summary
            if source:
                item["extra"]["source"] = source

            # å¦‚æœæœ‰æ—¶é—´é™åˆ¶ä¸”è¿‡æ—§ï¼Œè·³è¿‡
            if self.date_cutoff_days and self._is_older_than_cutoff(item.get("timestamp")):
                continue

            results.append(item)

        return results

    def _parse_time_text(self, text: str) -> Optional[datetime]:
        """
        è§£æç±»ä¼¼â€œ3å¤©å‰â€â€œ2å°æ—¶å‰â€æˆ–â€œ2024-12-05 10:30â€è¿™æ ·çš„æ—¶é—´ã€‚
        """
        if not text:
            return None

        now = datetime.now()
        try:
            if "å¤©å‰" in text:
                days = int(text.replace("å¤©å‰", "").strip() or 0)
                return now - timedelta(days=days)
            if "å°æ—¶å‰" in text:
                hours = int(text.replace("å°æ—¶å‰", "").strip() or 0)
                return now - timedelta(hours=hours)
            if "åˆ†é’Ÿå‰" in text:
                minutes = int(text.replace("åˆ†é’Ÿå‰", "").strip() or 0)
                return now - timedelta(minutes=minutes)
            # ç»å¯¹æ—¥æœŸ
            for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%d", "%Y/%m/%d"):
                try:
                    return datetime.strptime(text, fmt)
                except ValueError:
                    continue
        except Exception:
            return None
        return None

    def _is_older_than_cutoff(self, iso_ts: Optional[str]) -> bool:
        if not iso_ts or not self.date_cutoff_days:
            return False
        try:
            dt = datetime.fromisoformat(iso_ts)
        except Exception:
            return False
        return (datetime.now() - dt).days > self.date_cutoff_days

    def _extract_article_content(self, elem, summary_sel: Optional[str] = None) -> Optional[str]:
        """æå–æ–‡ç« çš„ä¸»è¦å†…å®¹"""
        try:
            content_parts = []
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æå–å†…å®¹
            content_selectors = [
                summary_sel,
                '.item-content',
                '.article-body',
                '.content',
                '.news-content',
                'p',
            ]
            
            for sel in content_selectors:
                if not sel:
                    continue
                content_elem = elem.select_one(sel)
                if content_elem:
                    text = content_elem.get_text(strip=True)
                    if text and len(text) > 20:  # è‡³å°‘20å­—ç¬¦
                        content_parts.append(text)
                        break
            
            if content_parts:
                return "\n\n".join(content_parts)
            return None
        except Exception as e:
            logger.warning(f"æå–å†…å®¹å¤±è´¥: {e}")
            return None

    def _save_content_to_cache(self, news_id: str, data: dict):
        """ä¿å­˜æ–‡ç« å†…å®¹åˆ° Redis ç¼“å­˜"""
        try:
            import redis
            import json
            import os
            
            redis_host = os.environ.get("REDIS_HOST", "localhost")
            redis_port = int(os.environ.get("REDIS_PORT", "49907"))
            
            client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
            key = f"trendradar:reader:{news_id}"
            
            # ä¿å­˜7å¤©
            client.setex(key, 7 * 24 * 3600, json.dumps(data, ensure_ascii=False))
            logger.debug(f"âœ… ä¿å­˜æ–‡ç« å†…å®¹: {news_id}")
        except Exception as e:
            logger.warning(f"ä¿å­˜å†…å®¹åˆ°ç¼“å­˜å¤±è´¥: {e}")

    def _normalize_url(self, url: str, section_name: str = "") -> str:
        """
        è§„èŒƒåŒ– Plasway URL
        
        ç”±äº Plasway æ–‡ç« é¡µé¢éœ€è¦ç™»å½•ï¼Œå°† URL æ”¹ä¸ºå¯¹åº”åˆ†ç±»çš„åˆ—è¡¨é¡µ
        """
        # Plasway æ–‡ç« éœ€è¦ç™»å½•ï¼Œæ”¹ä¸ºé“¾æ¥åˆ°åˆ†ç±»é¡µé¢ï¼ˆéœ€è¦ ?web=new å‚æ•°ï¼‰
        section_urls = {
            "market": "https://www.plasway.com/news/market?web=new",
            "innovation": "https://www.plasway.com/news/innovation?web=new", 
            "policy": "https://www.plasway.com/news/policy?web=new",
            "viewpoint": "https://www.plasway.com/news/viewpoint?web=new",
            "industry": "https://www.plasway.com/news/industry?web=new",
        }
        
        # è¿”å›å¯¹åº”åˆ†ç±»é¡µé¢ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›ä¸»é¡µ
        return section_urls.get(section_name, "https://www.plasway.com/news?web=new")
