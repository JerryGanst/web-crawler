"""
ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨
æ•´åˆ newsnow API å’Œè‡ªå®šä¹‰çˆ¬è™«ï¼Œæä¾›ç»Ÿä¸€çš„çˆ¬å–æ¥å£
"""
import yaml
import time
import random
import requests
from typing import List, Dict, Any, Optional
from datetime import datetime


class UnifiedDataSource:
    """ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        
        # æ³¨å†Œè‡ªå®šä¹‰çˆ¬è™«
        from .finance import register_finance_scrapers
        register_finance_scrapers()
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def get_platforms_by_category(self, category: str) -> List[Dict]:
        """è·å–æŒ‡å®šåˆ†ç±»çš„å¹³å°åˆ—è¡¨"""
        platforms = self.config.get("platforms", [])
        if category == "all":
            return platforms
        return [p for p in platforms if p.get("category") == category]
    
    def get_categories(self) -> Dict:
        """è·å–æ‰€æœ‰åˆ†ç±»"""
        return self.config.get("categories", {})
    
    def crawl_newsnow(self, platform_id: str) -> List[Dict]:
        """ä» newsnow API çˆ¬å–æ•°æ®"""
        url = f"https://newsnow.busiyi.world/api/s?id={platform_id}&latest"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json, text/plain, */*",
        }
        
        for retry in range(3):
            try:
                resp = requests.get(url, headers=headers, timeout=15)
                resp.raise_for_status()
                data = resp.json()
                
                if data.get("status") in ["success", "cache"]:
                    items = data.get("items", [])
                    return items
            except Exception:
                if retry < 2:
                    time.sleep(random.uniform(2, 4))
        return []
    
    def crawl_custom(self, scraper_name: str, scraper_config: Dict) -> List[Dict]:
        """ä½¿ç”¨è‡ªå®šä¹‰çˆ¬è™«çˆ¬å–"""
        from .factory import ScraperFactory
        
        scraper = ScraperFactory.create(scraper_name, scraper_config)
        if scraper:
            return scraper.scrape()
        return []
    
    def crawl_category(self, category: str, include_custom: bool = True) -> List[Dict]:
        """
        çˆ¬å–æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰æ•°æ®
        
        Args:
            category: åˆ†ç±»åç§° (finance, news, social, tech, all)
            include_custom: æ˜¯å¦åŒ…å«è‡ªå®šä¹‰çˆ¬è™«çš„æ•°æ®
        
        Returns:
            ç»Ÿä¸€æ ¼å¼çš„æ•°æ®åˆ—è¡¨
        """
        all_data = []
        platforms = self.get_platforms_by_category(category)
        category_info = self.get_categories().get(category, {})
        category_name = category_info.get("name", category)
        
        print(f"\nğŸ“‚ æ­£åœ¨çˆ¬å–ã€{category_name}ã€‘åˆ†ç±»")
        print("=" * 50)
        
        # 1. çˆ¬å– newsnow å¹³å°
        for p in platforms:
            pid = p["id"]
            pname = p["name"]
            print(f"  ğŸ”„ {pname} ({pid})...", end=" ")
            
            items = self.crawl_newsnow(pid)
            if items:
                # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                for item in items:
                    item["platform"] = pid
                    item["platform_name"] = pname
                    item["category"] = category
                    item["source"] = "newsnow"
                all_data.extend(items)
                print(f"âœ… {len(items)} æ¡")
            else:
                print("âŒ å¤±è´¥")
            
            time.sleep(random.uniform(0.5, 1.5))
        
        # 2. çˆ¬å–è‡ªå®šä¹‰æ•°æ®æºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if include_custom and category == "finance":
            print(f"\n  ğŸ“Š è‡ªå®šä¹‰è´¢ç»æ•°æ®æº:")
            
            # æ–°æµªå¤–æ±‡
            print(f"  ğŸ”„ æ–°æµªå¤–æ±‡...", end=" ")
            forex_data = self.crawl_custom("sina_forex", {})
            if forex_data:
                for item in forex_data:
                    item["source"] = "custom"
                all_data.extend(forex_data)
                print(f"âœ… {len(forex_data)} æ¡")
            else:
                print("âŒ å¤±è´¥")
            
            # CoinGecko
            print(f"  ğŸ”„ CoinGecko åŠ å¯†è´§å¸...", end=" ")
            crypto_data = self.crawl_custom("coingecko", {})
            if crypto_data:
                for item in crypto_data:
                    item["source"] = "custom"
                all_data.extend(crypto_data)
                print(f"âœ… {len(crypto_data)} æ¡")
            else:
                print("âŒ å¤±è´¥")
            
            # ä¸œæ–¹è´¢å¯Œä¾›åº”é“¾ä¼ä¸šåŠ¨æ€
            print(f"  ğŸ”„ ä¸œæ–¹è´¢å¯Œä¾›åº”é“¾åŠ¨æ€...", end=" ")
            supply_chain_data = self.crawl_custom("eastmoney_supply_chain", {})
            if supply_chain_data:
                for item in supply_chain_data:
                    item["source"] = "custom"
                    item["platform_name"] = "ä¸œæ–¹è´¢å¯Œ"
                all_data.extend(supply_chain_data)
                print(f"âœ… {len(supply_chain_data)} æ¡")
            else:
                print("âŒ å¤±è´¥")
        
        if include_custom and category == "tech":
            print(f"\n  ğŸ“Š è‡ªå®šä¹‰ç§‘æŠ€æ•°æ®æº:")
            
            # Hacker News
            print(f"  ğŸ”„ Hacker News...", end=" ")
            hn_data = self.crawl_custom("hackernews", {})
            if hn_data:
                for item in hn_data:
                    item["source"] = "custom"
                all_data.extend(hn_data)
                print(f"âœ… {len(hn_data)} æ¡")
            else:
                print("âŒ å¤±è´¥")
        
        print(f"\nğŸ“Š å…±è·å– {len(all_data)} æ¡æ•°æ®")
        return all_data
    
    def push_to_wework(self, data: List[Dict], category: str, webhook_url: str):
        """æ¨é€æ•°æ®åˆ°ä¼ä¸šå¾®ä¿¡"""
        if not webhook_url:
            print("âŒ æœªé…ç½®ä¼ä¸šå¾®ä¿¡ webhook")
            return
        
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯æ¨é€")
            return
        
        category_info = self.get_categories().get(category, {})
        category_name = category_info.get("name", category)
        now = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        # æŒ‰å¹³å°/æ¥æºåˆ†ç»„
        by_source = {}
        for item in data:
            source_name = item.get("platform_name", item.get("platform", "æœªçŸ¥"))
            if source_name not in by_source:
                by_source[source_name] = []
            by_source[source_name].append(item)
        
        # åˆ†æ‰¹å‘é€
        print(f"\nğŸ“¤ æ­£åœ¨æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆå…± {len(by_source)} æ‰¹ï¼‰...")
        
        success_count = 0
        batch_num = 1
        
        for source_name, items in by_source.items():
            lines = [f"## ğŸ“Š {category_name}çƒ­ç‚¹ ({now}) [{batch_num}]\n"]
            lines.append(f"### ğŸ“° {source_name}")
            
            for i, item in enumerate(items[:10], 1):
                title = item.get("title", "")
                url = item.get("url", "")
                if url:
                    lines.append(f"{i}. [{title}]({url})")
                else:
                    lines.append(f"{i}. {title}")
            
            message = "\n".join(lines)
            
            resp = requests.post(webhook_url, json={
                "msgtype": "markdown",
                "markdown": {"content": message}
            })
            
            if resp.status_code == 200 and resp.json().get("errcode") == 0:
                success_count += 1
            else:
                print(f"  âŒ ç¬¬ {batch_num} æ‰¹å¤±è´¥")
            
            batch_num += 1
            time.sleep(1)
        
        print(f"âœ… æ¨é€å®Œæˆï¼æˆåŠŸ {success_count}/{len(by_source)} æ‰¹")
