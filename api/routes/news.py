"""
æ–°é—»ç›¸å…³ API è·¯ç”±
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from datetime import datetime
from typing import Dict
from pathlib import Path

from ..cache import cache, CACHE_TTL
from ..models import CrawlRequest

router = APIRouter()

BASE_DIR = Path(__file__).parent.parent.parent

# ä¾›åº”é“¾å…³é”®è¯
SUPPLY_CHAIN_KEYWORDS = [
    "ç«‹è®¯", "æ­Œå°”", "è“æ€", "å¯Œè”", "å¯Œå£«åº·", "äº¬ä¸œæ–¹", "BOE",
    "æ¬£æ—ºè¾¾", "å¾·èµ›", "èˆœå®‡", "é¹é¼", "ä¸œå±±ç²¾å¯†", "é¢†ç›Š", "ç‘å£°",
    "è‹¹æœ", "Apple", "iPhone", "AirPods", "Vision Pro", "iPad", "Mac",
    "åä¸º", "Huawei", "é¸¿è’™", "Mate", "è£è€€",
    "å°ç±³", "OPPO", "vivo", "ä¸‰æ˜Ÿ", "Samsung",
    "æ¶ˆè´¹ç”µå­", "æœé“¾", "ä»£å·¥", "ä¾›åº”é“¾", "èŠ¯ç‰‡", "åŠå¯¼ä½“",
    "AI", "äººå·¥æ™ºèƒ½", "ç®—åŠ›", "GPU", "è‹±ä¼Ÿè¾¾"
]


def _crawl_news(category: str, include_custom: bool = True) -> Dict:
    """æ‰§è¡Œæ–°é—»çˆ¬å–"""
    from scrapers.unified import UnifiedDataSource
    
    unified = UnifiedDataSource()
    data = unified.crawl_category(category, include_custom=include_custom)
    
    return {
        "status": "success",
        "category": category,
        "data": data,
        "timestamp": datetime.now().isoformat(),
        "total": len(data)
    }


@router.get("/api/commodity-news")
async def get_commodity_news(refresh: bool = False):
    """è·å–å¤§å®—å•†å“æ–°é—»"""
    cache_key = "news:commodity"
    
    if refresh:
        try:
            print(f"ğŸ”„ ç”¨æˆ·è¯·æ±‚åˆ·æ–° commodity news...")
            result = _crawl_news("commodity", include_custom=True)
            result["cached"] = False
            cache.set(cache_key, result, ttl=CACHE_TTL)
            print(f"âœ… commodity news åˆ·æ–°å®Œæˆ: {result['total']} æ¡")
            return result
        except Exception as e:
            print(f"âŒ commodity news åˆ·æ–°å¤±è´¥: {e}")
            cached = cache.get(cache_key)
            if cached:
                cached["cached"] = True
                cached["error"] = str(e)
                return cached
            raise HTTPException(status_code=500, detail=f"çˆ¬å–å¤±è´¥: {str(e)}")
    
    cached = cache.get(cache_key)
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)
        return cached
    
    return {
        "status": "success",
        "category": "commodity",
        "data": [],
        "timestamp": None,
        "cached": False,
        "total": 0,
        "message": "æš‚æ— ç¼“å­˜æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–æœ€æ–°æ•°æ®"
    }


@router.get("/api/news/supply-chain")
async def get_supply_chain_news(refresh: bool = False):
    """è·å–ä¾›åº”é“¾ç›¸å…³æ–°é—»"""
    from .analysis import fetch_realtime_news
    
    cache_key = "news:supply-chain"
    
    if refresh:
        try:
            print(f"ğŸ”„ ç”¨æˆ·è¯·æ±‚åˆ·æ–° supply-chain...")
            news = fetch_realtime_news(SUPPLY_CHAIN_KEYWORDS)
            result = {
                "status": "success",
                "data": news,
                "timestamp": datetime.now().isoformat(),
                "total": len(news),
                "cached": False
            }
            cache.set(cache_key, result, ttl=CACHE_TTL)
            print(f"âœ… supply-chain åˆ·æ–°å®Œæˆ: {len(news)} æ¡")
            return result
        except Exception as e:
            print(f"âŒ supply-chain åˆ·æ–°å¤±è´¥: {e}")
            cached = cache.get(cache_key)
            if cached:
                cached["cached"] = True
                cached["error"] = str(e)
                return cached
            raise HTTPException(status_code=500, detail=f"çˆ¬å–å¤±è´¥: {str(e)}")
    
    cached = cache.get(cache_key)
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)
        return cached
    
    return {
        "status": "success",
        "data": [],
        "timestamp": None,
        "cached": False,
        "total": 0,
        "message": "æš‚æ— ç¼“å­˜æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–æœ€æ–°æ•°æ®"
    }


@router.get("/api/news/{category}")
async def get_news(category: str, include_custom: bool = True, refresh: bool = False):
    """è·å–æŒ‡å®šåˆ†ç±»çš„æ–°é—»"""
    cache_key = f"news:{category}"
    
    if refresh:
        try:
            print(f"ğŸ”„ ç”¨æˆ·è¯·æ±‚åˆ·æ–° {category}...")
            result = _crawl_news(category, include_custom)
            result["cached"] = False
            cache.set(cache_key, result, ttl=CACHE_TTL)
            print(f"âœ… {category} åˆ·æ–°å®Œæˆ: {result['total']} æ¡")
            return result
        except Exception as e:
            print(f"âŒ {category} åˆ·æ–°å¤±è´¥: {e}")
            cached = cache.get(cache_key)
            if cached:
                cached["cached"] = True
                cached["error"] = str(e)
                return cached
            raise HTTPException(status_code=500, detail=f"çˆ¬å–å¤±è´¥: {str(e)}")
    
    cached = cache.get(cache_key)
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)
        return cached
    
    return {
        "status": "success",
        "category": category,
        "data": [],
        "timestamp": None,
        "cached": False,
        "total": 0,
        "message": f"æš‚æ—  {category} ç¼“å­˜æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–æœ€æ–°æ•°æ®"
    }


@router.post("/api/crawl")
async def trigger_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """è§¦å‘çˆ¬å–ä»»åŠ¡"""
    from scrapers.unified import UnifiedDataSource
    
    try:
        unified = UnifiedDataSource()
        
        if request.category in ["supply-chain", "supply_chain"]:
            from .analysis import fetch_realtime_news
            keywords = SUPPLY_CHAIN_KEYWORDS
            data = fetch_realtime_news(keywords)
            
            cache.set("news:supply-chain", {
                "status": "success",
                "data": data,
                "timestamp": datetime.now().isoformat(),
                "total": len(data)
            }, ttl=CACHE_TTL)
            
            return {
                "status": "success",
                "category": request.category,
                "total": len(data),
                "message": f"å·²çˆ¬å– {len(data)} æ¡æ•°æ®"
            }
        
        data = unified.crawl_category(request.category, request.include_custom)
        
        cache.set(f"news:{request.category}", {
            "status": "success",
            "category": request.category,
            "data": data,
            "timestamp": datetime.now().isoformat(),
            "total": len(data)
        }, ttl=CACHE_TTL)
        
        return {
            "status": "success",
            "category": request.category,
            "total": len(data),
            "message": f"å·²çˆ¬å– {len(data)} æ¡æ•°æ®"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
