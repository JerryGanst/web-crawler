"""
AI åˆ†æç›¸å…³ API è·¯ç”±
"""
from fastapi import APIRouter, HTTPException
from datetime import datetime
from pathlib import Path
import os
import requests as req

from ..cache import cache, CACHE_TTL
from ..models import AnalysisRequest
from prompts import (
    get_supply_chain_analysis_prompt, 
    ANALYSIS_SYSTEM_PROMPT,
    get_market_analysis_prompt,
    MARKET_SYSTEM_PROMPT,
    precheck_news_quality  # V2æ–°å¢ï¼šæ–°é—»è´¨é‡é¢„æ£€
)

# ä» news.py å¯¼å…¥å®Œæ•´çš„é…ç½®
from .news import (
    OPTICAL_PARTNERS,
    CONNECTOR_PARTNERS, 
    POWER_PARTNERS,
    CUSTOMERS,
    SUPPLIERS
)

router = APIRouter()

BASE_DIR = Path(__file__).parent.parent.parent

# å¸‚åœºåˆ†æç¼“å­˜
_market_analysis_cache = {
    "content": None,
    "timestamp": None,
    "ttl": 1800  # 30åˆ†é’Ÿç¼“å­˜
}


def load_config():
    """åŠ è½½é…ç½®"""
    import yaml
    config_path = BASE_DIR / "config" / "config.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_ai_config():
    """è·å– AI é…ç½®"""
    config = load_config()
    ai_config = config.get("ai", {})
    
    internal_config = ai_config.get("internal", {})
    external_config = ai_config.get("external", {})
    
    internal_api_key = internal_config.get("api_key", "")
    internal_api_base = internal_config.get("api_base", "http://10.180.116.5:6410/v1")
    internal_model = internal_config.get("model", "Qwen_Qwen3-VL-235B-A22B-Instruct-FP8")
    
    external_api_key = external_config.get("api_key", "") or os.environ.get("AI_API_KEY", "")
    external_api_base = external_config.get("api_base", "https://api.siliconflow.cn/v1")
    external_model = external_config.get("model", "Pro/moonshotai/Kimi-K2-Thinking")
    external_thinking_level = external_config.get("thinking_level", "high")
    
    if not internal_config and not external_config:
        internal_api_key = ai_config.get("api_key", "")
        internal_api_base = ai_config.get("api_base", "https://api.siliconflow.cn/v1")
        internal_model = ai_config.get("model", "Qwen/Qwen2.5-7B-Instruct")
    
    return {
        "internal": {
            "api_key": internal_api_key,
            "api_base": internal_api_base,
            "model": internal_model
        },
        "external": {
            "api_key": external_api_key,
            "api_base": external_api_base,
            "model": external_model,
            "thinking_level": external_thinking_level
        }
    }


def call_ai_api(api_base: str, api_key: str, model: str, 
                system_prompt: str, user_prompt: str, 
                timeout: int = 180, max_tokens: int = 8000):
    """è°ƒç”¨ AI API (OpenAI å…¼å®¹æ ¼å¼)"""
    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    
    response = req.post(
        f"{api_base.rstrip('/')}/chat/completions",
        headers=headers,
        json={
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.7,
            "max_tokens": max_tokens
        },
        timeout=timeout
    )
    return response


def call_gemini_api(api_base: str, api_key: str, model: str,
                    system_prompt: str, user_prompt: str,
                    thinking_level: str = "high",
                    timeout: int = 180, max_tokens: int = 8000):
    """è°ƒç”¨ Gemini 3 Pro API (æ”¯æŒ thinkingConfig)"""
    url = f"{api_base.rstrip('/')}/models/{model}:generateContent"
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key
    }
    
    payload = {
        "contents": [
            {
                "parts": [
                    {"text": f"{system_prompt}\n\n{user_prompt}"}
                ]
            }
        ],
        "generationConfig": {
            "maxOutputTokens": max_tokens,
            # Gemini 3 å®˜æ–¹å»ºè®®ä¿æŒé»˜è®¤ 1.0
            "temperature": 1.0,
            "thinkingConfig": {
                "thinkingLevel": thinking_level
            }
        }
    }
    
    response = req.post(url, headers=headers, json=payload, timeout=timeout)
    return response


def parse_gemini_response(response):
    """è§£æ Gemini API å“åº”ï¼Œè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
    if response.status_code != 200:
        return None, f"Gemini API é”™è¯¯: {response.status_code} - {response.text}"
    
    result = response.json()
    
    # Gemini å“åº”æ ¼å¼: {"candidates": [{"content": {"parts": [{"text": "..."}]}}]}
    try:
        candidates = result.get("candidates", [])
        if candidates:
            parts = candidates[0].get("content", {}).get("parts", [])
            # è¿‡æ»¤æ‰ thought éƒ¨åˆ†ï¼Œåªå– text éƒ¨åˆ†
            text_parts = [p.get("text", "") for p in parts if "text" in p and "thought" not in p]
            content = "\n".join(text_parts)
            return content, None
    except Exception as e:
        return None, f"è§£æ Gemini å“åº”å¤±è´¥: {e}"
    
    return None, "Gemini å“åº”æ ¼å¼å¼‚å¸¸"


def fetch_realtime_news(keywords: list) -> list:
    """å®æ—¶æŠ“å–æ–°é—»"""
    all_news = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    # 1. ä¸œæ–¹è´¢å¯Œå¿«è®¯
    try:
        url = "https://push2ex.eastmoney.com/getAllStockBreakthrough?cb=callback"
        resp = req.get(url, headers=headers, timeout=10)
        text = resp.text
        if "callback(" in text:
            import json
            json_str = text.replace("callback(", "").rstrip(")")
            data = json.loads(json_str)
            for item in data.get("data", {}).get("list", [])[:20]:
                title = item.get("title", "")
                if any(kw in title for kw in keywords):
                    all_news.append({
                        "title": title,
                        "url": f"https://stock.eastmoney.com/a/{item.get('code', '')}.html",
                        "source": "ä¸œæ–¹è´¢å¯Œå¿«è®¯"
                    })
    except Exception as e:
        print(f"âš ï¸ ä¸œæ–¹è´¢å¯Œå¿«è®¯æŠ“å–å¤±è´¥: {e}")
    
    # 2. åŒèŠ±é¡ºå¿«è®¯
    try:
        url = "https://news.10jqka.com.cn/tapp/news/push/stock/?page=1&tag=&track=website&pagesize=50"
        resp = req.get(url, headers=headers, timeout=10)
        data = resp.json()
        for item in data.get("data", {}).get("list", []):
            title = item.get("title", "")
            if any(kw in title for kw in keywords):
                all_news.append({
                    "title": title,
                    "url": item.get("url", ""),
                    "source": "åŒèŠ±é¡º"
                })
    except Exception as e:
        print(f"âš ï¸ åŒèŠ±é¡ºæŠ“å–å¤±è´¥: {e}")
    
    # 3. æ–°æµªè´¢ç»
    try:
        url = "https://feed.mix.sina.com.cn/api/roll/get?pageid=153&lid=2516&k=&num=50&page=1"
        resp = req.get(url, headers=headers, timeout=10)
        data = resp.json()
        for item in data.get("result", {}).get("data", []):
            title = item.get("title", "")
            if any(kw in title for kw in keywords):
                all_news.append({
                    "title": title,
                    "url": item.get("url", ""),
                    "source": "æ–°æµªè´¢ç»"
                })
    except Exception as e:
        print(f"âš ï¸ æ–°æµªè´¢ç»æŠ“å–å¤±è´¥: {e}")
    
    # 4. ä¸œæ–¹è´¢å¯Œä¸ªè‚¡å…¬å‘Šï¼ˆé’ˆå¯¹è‚¡ç¥¨ä»£ç æœç´¢ï¼‰
    stock_codes = [kw for kw in keywords if kw.isdigit() and len(kw) == 6]
    for code in stock_codes[:8]:  # æœ€å¤šæŸ¥8ä¸ªè‚¡ç¥¨
        try:
            url = f"https://np-anotice-stock.eastmoney.com/api/security/ann?sr=-1&page_size=10&page_index=1&ann_type=A&stock_list={code}&f_node=0&s_node=0"
            resp = req.get(url, headers=headers, timeout=5)
            data = resp.json()
            for item in data.get("data", {}).get("list", [])[:5]:
                title = item.get("title", "")
                art_code = item.get("art_code", "")
                all_news.append({
                    "title": title,
                    "url": f"https://data.eastmoney.com/notices/detail/{code}/{art_code}.html",
                    "source": f"ä¸œæ–¹è´¢å¯Œå…¬å‘Š",
                    "stock_code": code,
                    "matched_keyword": code
                })
        except Exception as e:
            pass  # é™é»˜å¤±è´¥
    
    # 5. é›ªçƒä¸ªè‚¡è®¨è®ºï¼ˆé’ˆå¯¹è‚¡ç¥¨ä»£ç æœç´¢ï¼‰
    for code in stock_codes[:5]:
        try:
            # æ·±å¸‚ SZï¼Œæ²ªå¸‚ SH
            prefix = "SZ" if code.startswith(("0", "3")) else "SH"
            symbol = f"{prefix}{code}"
            url = f"https://xueqiu.com/statuses/stock_timeline.json?symbol={symbol}&count=10&source=all"
            resp = req.get(url, headers={**headers, "Cookie": "xq_a_token=test"}, timeout=5)
            data = resp.json()
            for item in data.get("list", [])[:5]:
                title = item.get("title", "") or item.get("text", "")[:100]
                if title:
                    all_news.append({
                        "title": title,
                        "url": f"https://xueqiu.com{item.get('target', '')}",
                        "source": f"é›ªçƒ-{code}",
                        "stock_code": code,
                        "matched_keyword": code
                    })
        except Exception as e:
            pass  # é™é»˜å¤±è´¥
    
    # 6. è´¢è”ç¤¾æœç´¢ï¼ˆè¦†ç›–æµ·å¤–å’Œéä¸Šå¸‚å…¬å¸ï¼‰
    company_keywords = [kw for kw in keywords if not kw.isdigit()][:25]
    for kw in company_keywords:
        try:
            url = "https://www.cls.cn/api/sw?app=cls-pc&os=web&sv=7.7.5"
            data = {"type": "telegram", "keyword": kw, "page": 1, "rn": 10}
            resp = req.post(url, json=data, headers=headers, timeout=5)
            result = resp.json()
            for item in result.get("data", {}).get("telegram", {}).get("data", [])[:5]:
                title = item.get("title", "") or item.get("descr", "")[:100]
                if title and len(title) > 8:
                    all_news.append({
                        "title": title,
                        "url": f"https://www.cls.cn/detail/{item.get('id', '')}",
                        "source": "è´¢è”ç¤¾",
                        "matched_keyword": kw
                    })
        except:
            pass
    
    # 7. è´¢è”ç¤¾æ·±åº¦æ–‡ç« æœç´¢
    for kw in company_keywords[:15]:
        try:
            url = "https://www.cls.cn/api/sw?app=cls-pc&os=web&sv=7.7.5"
            data = {"type": "article", "keyword": kw, "page": 1, "rn": 10}
            resp = req.post(url, json=data, headers=headers, timeout=5)
            result = resp.json()
            for item in result.get("data", {}).get("article", {}).get("data", [])[:3]:
                title = item.get("title", "")
                if title and len(title) > 8:
                    all_news.append({
                        "title": title,
                        "url": f"https://www.cls.cn/detail/{item.get('id', '')}",
                        "source": "è´¢è”ç¤¾æ·±åº¦",
                        "matched_keyword": kw
                    })
        except:
            pass
    
    # 8. å·¨æ½®èµ„è®¯ï¼ˆå®˜æ–¹å…¬å‘Šæºï¼‰
    for code in stock_codes[:6]:
        try:
            url = f"http://www.cninfo.com.cn/new/disclosure/stock?stockCode={code}&pageNum=1&pageSize=10"
            resp = req.get(url, headers=headers, timeout=5)
            data = resp.json()
            for item in data.get("classifiedAnnouncements", [])[:5]:
                for ann in item if isinstance(item, list) else [item]:
                    title = ann.get("announcementTitle", "")
                    if title:
                        all_news.append({
                            "title": title,
                            "url": f"http://www.cninfo.com.cn/new/disclosure/detail?announcementId={ann.get('announcementId', '')}",
                            "source": "å·¨æ½®èµ„è®¯",
                            "stock_code": code
                        })
        except:
            pass
    
    # 9. åŒèŠ±é¡ºç ”æŠ¥æœç´¢
    for kw in keywords[:10]:
        try:
            url = f"https://data.10jqka.com.cn/ajax/report/search?keyword={kw}&page=1&pagesize=10"
            resp = req.get(url, headers=headers, timeout=5)
            data = resp.json()
            for item in data.get("data", {}).get("list", [])[:3]:
                title = item.get("title", "")
                if title:
                    all_news.append({
                        "title": title,
                        "url": item.get("url", ""),
                        "source": "åŒèŠ±é¡ºç ”æŠ¥",
                        "matched_keyword": kw
                    })
        except:
            pass
    
    # 10. OFweek å…‰é€šè®¯/ç”µå­
    try:
        ofweek_keywords = [kw for kw in keywords if any(k in kw for k in ['å…‰', 'é€šä¿¡', 'æ—­åˆ›', 'æ–°æ˜“ç››', 'å¤©å­š', 'å…‰è¿…', 'Credo'])]
        for kw in ofweek_keywords[:5]:
            url = f"https://search.ofweek.com/search/?q={kw}&type=news"
            resp = req.get(url, headers=headers, timeout=5)
            # ç®€å•è§£æ
            import re
            titles = re.findall(r'<a[^>]*class="search-title"[^>]*>([^<]+)</a>', resp.text)
            links = re.findall(r'<a[^>]*class="search-title"[^>]*href="([^"]+)"', resp.text)
            for title, link in zip(titles[:3], links[:3]):
                all_news.append({
                    "title": title.strip(),
                    "url": link,
                    "source": "OFweek",
                    "matched_keyword": kw
                })
    except:
        pass
    
    # 11. å“”å“¥å“”ç‰¹ï¼ˆè¿æ¥å™¨ã€ç”µæºï¼‰
    try:
        bigbit_keywords = [kw for kw in keywords if any(k in kw for k in ['è¿æ¥å™¨', 'ç”µæº', 'å®‰è´¹è¯º', 'è«ä»•', 'TE', 'å¥¥æµ·', 'å°è¾¾', 'èˆªå˜‰'])]
        for kw in bigbit_keywords[:5]:
            url = f"https://www.big-bit.com/search/?q={kw}"
            resp = req.get(url, headers=headers, timeout=5)
            import re
            results = re.findall(r'<h3[^>]*><a[^>]*href="([^"]+)"[^>]*>([^<]+)</a>', resp.text)
            for link, title in results[:3]:
                all_news.append({
                    "title": title.strip(),
                    "url": f"https://www.big-bit.com{link}" if not link.startswith('http') else link,
                    "source": "å“”å“¥å“”ç‰¹",
                    "matched_keyword": kw
                })
    except:
        pass
    
    # 12. è…¾è®¯è´¢ç»ï¼ˆèšåˆæœç´¢ï¼‰
    try:
        for kw in keywords[:8]:
            url = f"https://news.qq.com/zt2020/page/feiyan.htm#/search?keyword={kw}&type=finance"
            # è…¾è®¯æ–°é—»éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨å¤‡ç”¨API
            api_url = f"https://i.news.qq.com/trpc.qqnews_web.kv_srv.kv_srv_http_proxy/list?sub_srv_id=24&srv_id=pc&offset=0&limit=10&strategy=1&ext={kw}"
            resp = req.get(api_url, headers=headers, timeout=5)
            data = resp.json()
            for item in data.get("data", {}).get("list", [])[:5]:
                title = item.get("title", "")
                if title and any(k in title for k in keywords):
                    all_news.append({
                        "title": title,
                        "url": item.get("url", ""),
                        "source": "è…¾è®¯è´¢ç»",
                        "matched_keyword": kw
                    })
    except:
        pass
    
    # 13. å’Œè®¯è´¢ç»
    try:
        url = "https://api.hexun.com/api/article/list?channelId=101&pageSize=30"
        resp = req.get(url, headers=headers, timeout=5)
        data = resp.json()
        for item in data.get("data", []):
            title = item.get("title", "")
            if any(kw in title for kw in keywords):
                all_news.append({
                    "title": title,
                    "url": item.get("url", ""),
                    "source": "å’Œè®¯è´¢ç»"
                })
    except:
        pass
    
    # 14. è¯åˆ¸æ—¶æŠ¥
    try:
        url = "https://api.stcn.com/api/article/getlist?channelId=16&pageSize=30"
        resp = req.get(url, headers=headers, timeout=5)
        data = resp.json()
        for item in data.get("data", {}).get("list", []):
            title = item.get("title", "")
            if any(kw in title for kw in keywords):
                all_news.append({
                    "title": title,
                    "url": item.get("url", f"https://www.stcn.com/article/detail/{item.get('id', '')}"),
                    "source": "è¯åˆ¸æ—¶æŠ¥"
                })
    except:
        pass
    
    # å»é‡ + æ¸…ç† HTML æ ‡ç­¾ + æ·»åŠ æ—¶é—´
    import re
    seen = set()
    unique_news = []
    current_time = datetime.now()
    
    for n in all_news:
        title = n.get("title", "")
        # æ¸…ç† HTML æ ‡ç­¾ï¼ˆå¦‚ <br>ï¼‰
        title = re.sub(r'<[^>]+>', ' ', title)
        title = re.sub(r'\s+', ' ', title).strip()
        
        if title not in seen and len(title) > 5:
            seen.add(title)
            # æ·»åŠ æŠ“å–æ—¶é—´ï¼ˆå¦‚æœåŸæ•°æ®æ²¡æœ‰æ—¶é—´ï¼‰
            news_item = {
                "title": title,
                "url": n.get("url", ""),
                "source": n.get("source", ""),
                "publish_time": n.get("publish_time") or n.get("time") or current_time.strftime("%Y-%m-%d %H:%M"),
                "matched_keyword": n.get("matched_keyword", "")
            }
            unique_news.append(news_item)
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    unique_news.sort(key=lambda x: x.get("publish_time", ""), reverse=True)
    
    return unique_news  # è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–°é—»ï¼Œä¸åšæ•°é‡é™åˆ¶


@router.get("/api/market-analysis")
async def get_market_analysis(refresh: bool = False):
    """è·å– AI ç”Ÿæˆçš„å¸‚åœºåˆ†ææŠ¥å‘Š"""
    
    # æ£€æŸ¥ç¼“å­˜
    if not refresh and _market_analysis_cache["content"]:
        cache_age = (datetime.now() - _market_analysis_cache["timestamp"]).total_seconds()
        if cache_age < _market_analysis_cache["ttl"]:
            return {
                "status": "success",
                "content": _market_analysis_cache["content"],
                "cached": True,
                "cache_age": int(cache_age),
                "timestamp": _market_analysis_cache["timestamp"].isoformat()
            }
    
    ai_config = get_ai_config()
    internal = ai_config["internal"]
    external = ai_config["external"]
    
    # è·å–å®æ—¶å¸‚åœºæ•°æ®
    from scrapers.commodity import CommodityScraper
    scraper = CommodityScraper()
    commodity_data = scraper.scrape()
    
    # å»é‡å¹¶æŒ‰å“ç±»æ±‡æ€»ï¼Œç¡®ä¿åŒ…æ‹¬å¡‘æ–™åœ¨å†…çš„æ‰€æœ‰å¤§å®—å“ç±»
    unique_map = {}
    for item in commodity_data:
        key = (item.get('chinese_name') or item.get('name', '')).strip().lower()
        if key and key not in unique_map:
            unique_map[key] = item
    deduped_data = list(unique_map.values())
    
    # å“ç±»é¡ºåºï¼Œç¡®ä¿å¡‘æ–™è¢«å•ç‹¬åˆ†ç±»å‡ºæ¥
    category_order = {'è´µé‡‘å±': 0, 'èƒ½æº': 1, 'å·¥ä¸šé‡‘å±': 2, 'å†œäº§å“': 3, 'å¡‘æ–™': 4}
    from collections import defaultdict
    categorized = defaultdict(list)
    for item in deduped_data:
        cat = item.get('category') or 'å…¶ä»–'
        categorized[cat].append(item)
    
    # æ„å»ºå•†å“æ•°æ®æ‘˜è¦ï¼ˆå…¨é‡ï¼Œä¸æˆªæ–­ï¼‰
    commodity_summary = []
    def _fmt_price(value):
        try:
            return f"{float(value):,.2f}".rstrip('0').rstrip('.')
        except Exception:
            return str(value)
    
    def _change_abs(item):
        try:
            return abs(float(item.get('change_percent') or 0))
        except Exception:
            return 0

    for cat in sorted(categorized.keys(), key=lambda c: category_order.get(c, 99)):
        items = categorized[cat]
        commodity_summary.append(f"## {cat}ï¼ˆ{len(items)}ç§ï¼‰")
        # æŒ‰ç»å¯¹æ¶¨è·Œå¹…æ’åºï¼Œä¾¿äºåˆ†ææ³¢åŠ¨
        for item in sorted(items, key=_change_abs, reverse=True):
            name = item.get('chinese_name') or item.get('name', '')
            price = _fmt_price(item.get('price') or item.get('current_price') or 0)
            change = item.get('change_percent', 0) or 0
            unit = item.get('unit', '')
            commodity_summary.append(f"- {name}: {price} {unit} ({'+' if change >= 0 else ''}{change}%)")
    
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    prompt = get_market_analysis_prompt(commodity_summary, today)
    
    used_model = ""
    used_api = ""
    # å¤–ç½‘é…ç½®æœ‰ key å°±ä¼˜å…ˆå¤–ç½‘ï¼ˆä¸å†é™å®š Geminiï¼‰
    prefer_external = bool(external.get("api_key"))

    def call_internal():
        print(f"ğŸ”„ å¸‚åœºåˆ†æ: å°è¯•å†…ç½‘ API...")
        resp = call_ai_api(
            internal["api_base"], internal["api_key"], internal["model"],
            MARKET_SYSTEM_PROMPT, prompt, timeout=10, max_tokens=1000
        )
        if resp.status_code != 200:
            raise Exception(f"å†…ç½‘ API è¿”å› {resp.status_code}")
        return resp, internal["model"], "å†…ç½‘"

    def call_external():
        if not external["api_key"]:
            raise Exception("æœªé…ç½®å¤–ç½‘ API Key")
        is_gemini = "generativelanguage.googleapis.com" in external["api_base"]
        if is_gemini:
            thinking_level = external.get("thinking_level", "low")  # å¸‚åœºåˆ†æç”¨ low ä»¥åŠ å¿«é€Ÿåº¦
            resp = call_gemini_api(
                external["api_base"], external["api_key"], external["model"],
                MARKET_SYSTEM_PROMPT, prompt,
                thinking_level=thinking_level,
                timeout=120, max_tokens=1000
            )
        else:
            resp = call_ai_api(
                external["api_base"], external["api_key"], external["model"],
                MARKET_SYSTEM_PROMPT, prompt, timeout=60, max_tokens=1000
            )
        if resp.status_code != 200:
            raise Exception(f"å¤–ç½‘ API è¿”å› {resp.status_code}")
        return resp, external["model"], "å¤–ç½‘"

    try:
        if prefer_external:
            response, used_model, used_api = call_external()
        else:
            response, used_model, used_api = call_internal()
    except Exception as e_first:
        print(f"âš ï¸ ä¸»ä¼˜å…ˆ API ä¸å¯ç”¨: {e_first}")
        try:
            if prefer_external:
                response, used_model, used_api = call_internal()
            else:
                response, used_model, used_api = call_external()
        except Exception as e_second:
            default_content = f"""**å¸‚åœºæ¦‚å†µ**
ä»Šæ—¥å¤§å®—å•†å“å¸‚åœºæ•´ä½“è¡¨ç°å¹³ç¨³ï¼Œè´µé‡‘å±æ¿å—å°å¹…æ³¢åŠ¨ï¼Œèƒ½æºä»·æ ¼ç»´æŒéœ‡è¡æ ¼å±€ã€‚

**é‡ç‚¹å…³æ³¨**
* é»„é‡‘ä»·æ ¼ç»´æŒé«˜ä½ï¼Œå…³æ³¨ç¾è”å‚¨æ”¿ç­–åŠ¨å‘
* åŸæ²¹ä»·æ ¼å—ä¾›éœ€å½±å“éœ‡è¡
* å·¥ä¸šé‡‘å±å—ç»æµæ•°æ®å½±å“

**æ“ä½œå»ºè®®**
ä¿æŒè§‚æœ›ï¼Œç­‰å¾…æ›´æ˜ç¡®çš„å¸‚åœºä¿¡å·ã€‚

---
*æ•°æ®æ›´æ–°: {today}*"""
            return {
                "status": "success",
                "content": default_content,
                "cached": False,
                "model": "fallback",
                "api_source": "é»˜è®¤",
                "timestamp": datetime.now().isoformat()
            }
    
    try:
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=f"AI APIè°ƒç”¨å¤±è´¥")
        
        # æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Gemini API æ¥å†³å®šè§£ææ–¹å¼
        is_gemini = "generativelanguage.googleapis.com" in external.get("api_base", "")
        
        if is_gemini:
            content, error = parse_gemini_response(response)
            if error:
                raise HTTPException(status_code=500, detail=error)
        else:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"]
            else:
                raise HTTPException(status_code=500, detail="æ— æ³•è§£æAIå“åº”")
        
        # æ›´æ–°ç¼“å­˜
        _market_analysis_cache["content"] = content
        _market_analysis_cache["timestamp"] = datetime.now()
        
        return {
            "status": "success",
            "content": content,
            "cached": False,
            "model": used_model,
            "api_source": used_api,
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆåˆ†æå¤±è´¥: {str(e)}")


@router.post("/api/generate-analysis")
async def generate_analysis(request: AnalysisRequest):
    """ä½¿ç”¨ AI ç”Ÿæˆä¾›åº”é“¾åˆ†ææŠ¥å‘Š"""
    
    ai_config = get_ai_config()
    internal = ai_config["internal"]
    external = ai_config["external"]
    
    # ==================== 1. è·å–ä¾›åº”é“¾æ–°é—»ï¼ˆå¼ºåˆ¶åˆ·æ–°ï¼‰ ====================
    print(f"ğŸ“¡ [æŠ¥å‘Šç”Ÿæˆ] æ­£åœ¨è·å–æœ€æ–°ä¾›åº”é“¾æ–°é—»...")
    
    # å…³é”®è¯ï¼ˆæ‰©å±•ï¼‰
    supply_chain_keywords = [
        "ç«‹è®¯", "æ­Œå°”", "è“æ€", "å¯Œè”", "å¯Œå£«åº·", "äº¬ä¸œæ–¹", "BOE",
        "æ¬£æ—ºè¾¾", "å¾·èµ›", "èˆœå®‡", "é¹é¼", "ä¸œå±±ç²¾å¯†", "é¢†ç›Š", "ç‘å£°",
        "è‹¹æœ", "Apple", "iPhone", "AirPods", "Vision Pro", "iPad", "Mac",
        "åä¸º", "Huawei", "å°ç±³", "OPPO", "vivo", "ä¸‰æ˜Ÿ", "Samsung",
        "æ¶ˆè´¹ç”µå­", "æœé“¾", "ä»£å·¥", "ä¾›åº”é“¾", "èŠ¯ç‰‡", "åŠå¯¼ä½“",
        "AI", "äººå·¥æ™ºèƒ½", "ç®—åŠ›", "GPU", "è‹±ä¼Ÿè¾¾", "NVIDIA",
        # å…³ç¨ç›¸å…³
        "å…³ç¨", "è´¸æ˜“æˆ˜", "ä¸­ç¾", "åˆ¶è£", "å‡ºå£ç®¡åˆ¶", "å®ä½“æ¸…å•",
        # åŸææ–™ç›¸å…³
        "é“œ", "é•", "é”¡", "é“", "é‡‘", "é“¶", "å¡‘æ–™", "PA66", "PBT", "ABS"
    ]
    
    # å®æ—¶æŠ“å–æœ€æ–°æ–°é—»
    realtime_news = fetch_realtime_news(supply_chain_keywords)
    try:
        from .news import _fetch_power_partner_news, _fetch_power_official_announcements
        power_news = _fetch_power_partner_news()
        official_news = _fetch_power_official_announcements()
        realtime_news.extend(power_news + official_news)
        print(f"âš¡ ç”µæºå®šå‘æ–°é—»: {len(power_news)}ï¼Œå®˜ç½‘å…¬å‘Š: {len(official_news)}")
    except Exception as e:
        print(f"âš ï¸ ç”µæºå®šå‘æŠ“å–å¤±è´¥: {e}")
    print(f"âœ… å®æ—¶æŠ“å–: {len(realtime_news)} æ¡æ–°é—»")
    
    # ä»ç¼“å­˜è·å–å·²æœ‰çš„ä¾›åº”é“¾æ–°é—»
    cached_supply = cache.get("news:supply-chain")
    cached_supply_news = cached_supply.get("data", []) if cached_supply else []
    print(f"âœ… ç¼“å­˜ä¾›åº”é“¾æ–°é—»: {len(cached_supply_news)} æ¡")
    
    # ä»ç¼“å­˜è·å–å…³ç¨æ–°é—»
    cached_tariff = cache.get("news:tariff")
    cached_tariff_news = cached_tariff.get("data", []) if cached_tariff else []
    print(f"âœ… ç¼“å­˜å…³ç¨æ–°é—»: {len(cached_tariff_news)} æ¡")
    
    # ==================== 2. è·å–å¤§å®—å•†å“æ•°æ® ====================
    print(f"ğŸ“Š [æŠ¥å‘Šç”Ÿæˆ] æ­£åœ¨è·å–å¤§å®—å•†å“ä»·æ ¼æ•°æ®...")
    commodity_summary = ""
    try:
        from scrapers.commodity import CommodityScraper
        scraper = CommodityScraper()
        commodity_data = scraper.scrape()
        
        if commodity_data:
            commodity_lines = ["**å½“å‰å¤§å®—å•†å“ä»·æ ¼ï¼ˆå®æ—¶æ•°æ®ï¼‰ï¼š**"]
            
            # é‡‘å±ç±»
            metals = [c for c in commodity_data if c.get('category') == 'é‡‘å±' or any(m in c.get('name', '') for m in ['é“œ', 'é“', 'é”Œ', 'é•', 'é”¡', 'é‡‘', 'é“¶'])]
            if metals:
                commodity_lines.append("\n**é‡‘å±ç±»åŸææ–™ï¼š**")
                for c in metals[:10]:
                    name = c.get('chinese_name') or c.get('name', '')
                    price = c.get('price', 0)
                    change = c.get('change_percent', 0)
                    unit = c.get('unit', '')
                    trend = 'â†‘' if change > 0 else ('â†“' if change < 0 else 'â†’')
                    commodity_lines.append(f"- {name}: {price} {unit} ({'+' if change >= 0 else ''}{change}% {trend})")
            
            # å¡‘æ–™/èƒ½æºç±»
            plastics = [c for c in commodity_data if any(p in c.get('name', '').upper() for p in ['PP', 'PE', 'PVC', 'ABS', 'PA', 'PBT', 'PC', 'å¡‘æ–™', 'OIL', 'åŸæ²¹'])]
            if plastics:
                commodity_lines.append("\n**å¡‘æ–™/èƒ½æºç±»åŸææ–™ï¼š**")
                for c in plastics[:10]:
                    name = c.get('chinese_name') or c.get('name', '')
                    price = c.get('price', 0)
                    change = c.get('change_percent', 0)
                    unit = c.get('unit', '')
                    trend = 'â†‘' if change > 0 else ('â†“' if change < 0 else 'â†’')
                    commodity_lines.append(f"- {name}: {price} {unit} ({'+' if change >= 0 else ''}{change}% {trend})")
            
            commodity_summary = "\n".join(commodity_lines)
            print(f"âœ… å¤§å®—å•†å“æ•°æ®: {len(commodity_data)} æ¡")
    except Exception as e:
        print(f"âš ï¸ è·å–å¤§å®—å•†å“æ•°æ®å¤±è´¥: {e}")
        commodity_summary = "âš ï¸ å¤§å®—å•†å“æ•°æ®è·å–å¤±è´¥ï¼Œè¯·å‚è€ƒå¸‚åœºå…¬å¼€æ•°æ®"
    
    # ==================== 3. åˆå¹¶æ‰€æœ‰æ–°é—»æ¥æº ====================
    all_news = []
    
    # 1) å‰ç«¯ä¼ å…¥çš„æ–°é—»
    if request.news:
        all_news.extend(list(request.news))
    
    # 2) å®æ—¶æŠ“å–çš„æ–°é—»
    all_news.extend(realtime_news)
    
    # 3) ç¼“å­˜çš„ä¾›åº”é“¾æ–°é—»
    all_news.extend(cached_supply_news)
    
    # 4) ç¼“å­˜çš„å…³ç¨æ–°é—»
    all_news.extend(cached_tariff_news)
    
    # å»é‡
    seen_titles = set()
    unique_news = []
    for n in all_news:
        title = n.get("title", "")
        if title and title not in seen_titles:
            seen_titles.add(title)
            unique_news.append(n)
    
    print(f"âœ… åˆå¹¶å»é‡åæ€»æ–°é—»: {len(unique_news)} æ¡")
    
    # ==================== 3.5 æ–°é—»è´¨é‡é¢„æ£€ï¼ˆV2æ–°å¢ï¼‰ ====================
    news_quality = precheck_news_quality(unique_news)
    print(f"ğŸ“Š æ–°é—»è´¨é‡è¯„åˆ†: {news_quality['quality_score']}/100")
    if news_quality['suggestions']:
        for suggestion in news_quality['suggestions']:
            print(f"   ğŸ’¡ {suggestion}")
    
    # ==================== 4. æ„å»ºæ–°é—»æ‘˜è¦ ====================
    news_summary = ""
    if unique_news:
        news_items = []
        # å–æœ€æ–°çš„50æ¡æ–°é—»ï¼ˆæ‰©å¤§èŒƒå›´ï¼‰
        for n in unique_news[:50]:
            title = n.get('title', '')
            url = n.get('url', '')
            source = n.get('source', '') or n.get('platform_name', '')
            publish_time = n.get('publish_time', '')
            time_str = f" ({publish_time})" if publish_time else ""
            if url:
                news_items.append(f"- [{title}]({url}) ã€{source}{time_str}ã€‘")
            else:
                news_items.append(f"- {title} ã€{source}{time_str}ã€‘")
        news_summary = "\n".join(news_items)
    
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    
    # ==================== 5. ä½¿ç”¨å®Œæ•´çš„é…ç½®åˆ—è¡¨ ====================
    # å‹å•†/ç«äº‰å¯¹æ‰‹ï¼ˆ18å®¶ï¼‰
    all_competitors = list(OPTICAL_PARTNERS.keys()) + list(CONNECTOR_PARTNERS.keys()) + list(POWER_PARTNERS.keys())
    # é¢å¤–æ·»åŠ æ¶ˆè´¹ç”µå­ç«äº‰å¯¹æ‰‹
    all_competitors.extend(['æ­Œå°”è‚¡ä»½', 'è“æ€ç§‘æŠ€', 'å·¥ä¸šå¯Œè”', 'é¹é¼æ§è‚¡', 'ä¸œå±±ç²¾å¯†', 'é¢†ç›Šæ™ºé€ ', 'ç‘å£°ç§‘æŠ€', 'æ¯”äºšè¿ªç”µå­'])
    competitors = request.competitors or list(set(all_competitors))  # å»é‡
    
    # ä¾›åº”å•†ï¼ˆä»SUPPLIERSé…ç½®ä¸­æå–æ‰€æœ‰ä¾›åº”å•†åç§°ï¼‰
    all_suppliers = []
    for category, suppliers in SUPPLIERS.items():
        all_suppliers.extend(list(suppliers.keys()))
    # é¢å¤–æ·»åŠ é‡è¦ä¾›åº”å•†
    all_suppliers.extend(['äº¬ä¸œæ–¹A', 'èˆœå®‡å…‰å­¦', 'æ¬£æ—ºè¾¾', 'å¾·èµ›ç”µæ± ', 'ä¿¡ç»´é€šä¿¡', 'é•¿ç›ˆç²¾å¯†', 'è“æ€ç§‘æŠ€'])
    upstream = request.upstream or list(set(all_suppliers))  # å»é‡
    
    # å®¢æˆ·ï¼ˆ10å®¶ï¼‰
    all_customers = list(CUSTOMERS.keys())
    downstream = request.downstream or all_customers
    
    print(f"ğŸ“‹ åˆ†æé…ç½®: {len(competitors)}å®¶å‹å•†, {len(upstream)}å®¶ä¾›åº”å•†, {len(downstream)}å®¶å®¢æˆ·")
    
    prompt = get_supply_chain_analysis_prompt(
        company_name=request.company_name,
        today=today,
        competitors=competitors,
        upstream=upstream,
        downstream=downstream,
        news_summary=news_summary,
        news_count=len(unique_news),
        commodity_summary=commodity_summary
    )
    
    used_model = ""
    used_api = ""
    
    try:
        print(f"ğŸ”„ å°è¯•å†…ç½‘ API: {internal['api_base']}")
        # å†…ç½‘è¶…æ—¶è®¾ä¸º15ç§’ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
        response = call_ai_api(
            internal["api_base"], internal["api_key"], internal["model"],
            ANALYSIS_SYSTEM_PROMPT, prompt, timeout=15, max_tokens=8000
        )
        
        if response.status_code == 200:
            used_model = internal["model"]
            used_api = "å†…ç½‘"
            print(f"âœ… å†…ç½‘ API è°ƒç”¨æˆåŠŸ")
        else:
            raise Exception(f"å†…ç½‘ API è¿”å› {response.status_code}")
            
    except Exception as e:
        print(f"âš ï¸ å†…ç½‘ API ä¸å¯ç”¨: {e}")
        print(f"ğŸ”„ åˆ‡æ¢åˆ°å¤–ç½‘ API: {external['api_base']}")
        
        if not external["api_key"]:
            raise HTTPException(
                status_code=400, 
                detail="å†…ç½‘ API ä¸å¯ç”¨ï¼Œä¸”æœªé…ç½®å¤–ç½‘ API Key"
            )
        
        try:
            # æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Gemini API
            is_gemini = "generativelanguage.googleapis.com" in external["api_base"]
            
            if is_gemini:
                # ä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä¸­çš„ thinking_levelï¼Œå¦åˆ™ä½¿ç”¨é…ç½®
                thinking_level = request.thinking_level or external.get("thinking_level", "high")
                print(f"ğŸ§  ä½¿ç”¨ Gemini 3 Pro (thinking_level={thinking_level})")
                response = call_gemini_api(
                    external["api_base"], external["api_key"], external["model"],
                    ANALYSIS_SYSTEM_PROMPT, prompt,
                    thinking_level=thinking_level,
                    timeout=300, max_tokens=8000  # Gemini 3 æ€è€ƒæ¨¡å¼éœ€è¦æ›´é•¿è¶…æ—¶
                )
            else:
                response = call_ai_api(
                    external["api_base"], external["api_key"], external["model"],
                    ANALYSIS_SYSTEM_PROMPT, prompt, timeout=180, max_tokens=8000
                )
            used_model = external["model"]
            used_api = "å¤–ç½‘"
            print(f"âœ… å¤–ç½‘ API è°ƒç”¨æˆåŠŸ")
        except Exception as e2:
            raise HTTPException(status_code=500, detail=f"å†…å¤–ç½‘ API å‡ä¸å¯ç”¨: å†…ç½‘({e}), å¤–ç½‘({e2})")
    
    try:
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=f"AI APIè°ƒç”¨å¤±è´¥: {response.text}")
        
        # æ£€æµ‹æ˜¯å¦ä½¿ç”¨ Gemini API æ¥å†³å®šè§£ææ–¹å¼
        is_gemini = "generativelanguage.googleapis.com" in external.get("api_base", "")
        
        if is_gemini:
            content, error = parse_gemini_response(response)
            if error:
                raise HTTPException(status_code=500, detail=error)
        else:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                content = result["choices"][0]["message"]["content"]
            else:
                raise HTTPException(status_code=500, detail="æ— æ³•è§£æAIå“åº”")
        
        return {
            "status": "success",
            "content": content,
            "model": used_model,
            "api_source": used_api,
            "news_count": len(unique_news),
            "news_quality": {
                "score": news_quality['quality_score'],
                "has_customer_news": news_quality['has_customer_news'],
                "has_competitor_news": news_quality['has_competitor_news'],
                "has_tariff_news": news_quality['has_tariff_news'],
                "suggestions": news_quality['suggestions']
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆåˆ†æå¤±è´¥: {str(e)}")
