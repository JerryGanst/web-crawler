"""
æ–°é—»ç›¸å…³ API è·¯ç”±

ä¼˜åŒ–ç­–ç•¥ï¼šç¼“å­˜ä¼˜å…ˆ + åå°å¼‚æ­¥åˆ·æ–°
- ç”¨æˆ·è¯·æ±‚æ—¶ç«‹å³è¿”å›ç¼“å­˜æ•°æ®ï¼ˆ<50msï¼‰
- åˆ·æ–°æ“ä½œåœ¨åå°å¼‚æ­¥æ‰§è¡Œ
- ä¸‹æ¬¡è¯·æ±‚è·å¾—æ–°æ•°æ®
"""
import os
import asyncio
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict
from urllib.parse import quote

from fastapi import APIRouter, HTTPException, BackgroundTasks

from ..cache import cache, CACHE_TTL
from ..models import CrawlRequest

router = APIRouter()

BASE_DIR = Path(__file__).parent.parent.parent

# åå°ä»»åŠ¡çº¿ç¨‹æ± 
_executor = ThreadPoolExecutor(max_workers=10, thread_name_prefix="news-bg")
# åœ¨æµ‹è¯•ç¯å¢ƒä¸‹é¿å…å¯åŠ¨åå°çº¿ç¨‹ï¼Œé˜²æ­¢å¹¶å‘å½±å“ç”¨ä¾‹ï¼ˆé€šè¿‡ PYTEST_CURRENT_TEST æ£€æµ‹ï¼‰
_TEST_ENV = "PYTEST_CURRENT_TEST" in os.environ

# è¿›è¡Œä¸­çš„åå°ä»»åŠ¡è·Ÿè¸ªï¼ˆé¿å…é‡å¤åˆ·æ–°ï¼‰
_pending_refreshes = set()
_refresh_lock = Lock()


# ==================== å‹å•†å…³é”®è¯é…ç½® ====================
# 18å®¶å‹å•†åˆ†ç±»åŠæœç´¢å…³é”®è¯

# å…‰ç”µæ¨¡å—å‹å•†ï¼ˆ6å®¶ï¼‰
OPTICAL_PARTNERS = {
    "Credo": ["Credo", "Credo Technology", "CRDO"],
    "æ—­åˆ›ç§‘æŠ€": ["ä¸­é™…æ—­åˆ›", "æ—­åˆ›", "æ—­åˆ›ç§‘æŠ€", "300308"],
    "æ–°æ˜“ç››": ["æ–°æ˜“ç››", "300502"],
    "å¤©å­šé€šä¿¡": ["å¤©å­šé€šä¿¡", "å¤©å­š", "300394"],
    "å…‰è¿…ç§‘æŠ€": ["å…‰è¿…ç§‘æŠ€", "å…‰è¿…", "002281"],
    "Finisar": ["Finisar", "è²å°¼è¨", "II-VI"],
}

# è¿æ¥å™¨å‹å•†ï¼ˆ8å®¶ï¼‰
CONNECTOR_PARTNERS = {
    "å®‰è´¹è¯º": ["Amphenol", "å®‰è´¹è¯º", "APH"],
    "è«ä»•": ["Molex", "è«ä»•", "è«è±å…‹æ–¯"],
    "TE": ["TE Connectivity", "TE", "æ³°ç§‘ç”µå­", "TEL"],
    "ä¸­èˆªå…‰ç”µ": ["ä¸­èˆªå…‰ç”µ", "158ç”µè¿æ¥å™¨", "002179"],
    "å¾—æ„ç²¾å¯†": ["å¾—æ„ç²¾å¯†", "å¾—æ„"],
    "æ„åè‚¡ä»½": ["æ„åè‚¡ä»½", "æ„å", "002897"],
    "é‡‘ä¿¡è¯º": ["é‡‘ä¿¡è¯º", "300252"],
    "åä¸°ç§‘æŠ€": ["åä¸°ç§‘æŠ€", "åæ——", "688100"],
}

# ç”µæºå‹å•†ï¼ˆ4å®¶ï¼‰
POWER_PARTNERS = {
    "å¥¥æµ·ç§‘æŠ€": ["å¥¥æµ·ç§‘æŠ€", "å¥¥æµ·", "002993"],
    "èˆªå˜‰": ["èˆªå˜‰", "èˆªå˜‰é©°æº"],
    "èµ›å°”åº·": ["èµ›å°”åº·", "Salcomp"],
    "å°è¾¾ç”µå­": ["å°è¾¾", "å°è¾¾ç”µå­", "Delta", "2308.TW"],
}
# ç”µæºå‹å•†å®˜ç½‘
POWER_PARTNER_SITES = {
    "å¥¥æµ·ç§‘æŠ€": "https://www.aohai.com",
    "èˆªå˜‰": "https://www.huntkey.com",
    "èµ›å°”åº·": "https://www.salcomp.com",
    "å°è¾¾ç”µå­": "https://www.deltaww.com",
}
# ç”µæºå‹å•†å®˜ç½‘æ–°é—»/å…¬å‘Šé¡µ
POWER_PARTNER_NEWS_PAGES = {
    "å¥¥æµ·ç§‘æŠ€": "https://www.aohai.com/news",
    "èˆªå˜‰": "https://www.huntkey.com/category/news",
    "èµ›å°”åº·": "https://www.salcomp.com/newsroom",
    "å°è¾¾ç”µå­": "https://www.deltaww.com/en-US/about-delta/news",
}

# åˆå¹¶æ‰€æœ‰å‹å•†å…³é”®è¯
def _get_partner_keywords():
    """è·å–æ‰€æœ‰å‹å•†å…³é”®è¯ï¼ˆæ‰å¹³åŒ–ï¼‰"""
    keywords = []
    for partner_dict in [OPTICAL_PARTNERS, CONNECTOR_PARTNERS, POWER_PARTNERS]:
        for company, kw_list in partner_dict.items():
            keywords.extend(kw_list)
    return keywords

PARTNER_KEYWORDS = _get_partner_keywords()

# ==================== ä¾›åº”é“¾å…³é”®è¯ ====================
SUPPLY_CHAIN_KEYWORDS = [
    # æ ¸å¿ƒæœé“¾/æ¶ˆè´¹ç”µå­ä¾›åº”å•†
    "ç«‹è®¯", "æ­Œå°”", "è“æ€", "å¯Œè”", "å¯Œå£«åº·", "äº¬ä¸œæ–¹", "BOE",
    "æ¬£æ—ºè¾¾", "å¾·èµ›", "èˆœå®‡", "é¹é¼", "ä¸œå±±ç²¾å¯†", "é¢†ç›Š", "ç‘å£°",
    # å“ç‰Œå®¢æˆ·
    "è‹¹æœ", "Apple", "iPhone", "AirPods", "Vision Pro", "iPad", "Mac",
    "åä¸º", "Huawei", "é¸¿è’™", "Mate", "è£è€€",
    "å°ç±³", "OPPO", "vivo", "ä¸‰æ˜Ÿ", "Samsung",
    # è¡Œä¸šå…³é”®è¯
    "æ¶ˆè´¹ç”µå­", "æœé“¾", "ä»£å·¥", "ä¾›åº”é“¾", "èŠ¯ç‰‡", "åŠå¯¼ä½“",
    "AI", "äººå·¥æ™ºèƒ½", "ç®—åŠ›", "GPU", "è‹±ä¼Ÿè¾¾",
    # å‹å•†å…³é”®è¯ï¼ˆè‡ªåŠ¨åˆå¹¶ï¼‰
] + PARTNER_KEYWORDS

# å…³ç¨æ”¿ç­–å…³é”®è¯
TARIFF_KEYWORDS = [
    "å…³ç¨", "è´¸æ˜“æˆ˜", "ä¸­ç¾è´¸æ˜“", "è¿›å£ç¨", "å‡ºå£ç®¡åˆ¶",
    "å®ä½“æ¸…å•", "åˆ¶è£", "åŠ å¾å…³ç¨", "å…³ç¨è±å…", "è´¸æ˜“æ‘©æ“¦",
    "å¯¹åå…³ç¨", "301æ¡æ¬¾", "tariff", "trade war",
    "åå€¾é”€", "åè¡¥è´´", "æµ·å…³", "è¿›å‡ºå£", "è´¸æ˜“æ”¿ç­–",
    "å…³ç¨æ¸…å•", "è±å…", "åˆ¶è£æ¸…å•", "å‡ºå£ç¦ä»¤", "è¿›å£é™åˆ¶"
]

# å¡‘æ–™ç›¸å…³å…³é”®è¯
PLASTICS_KEYWORDS = [
    "å¡‘æ–™", "PA66", "PBT", "PC", "ABS", "PP", "PE", "PVC", "HDPE", "LDPE",
    "èšä¸™çƒ¯", "èšä¹™çƒ¯", "èšæ°¯ä¹™çƒ¯", "å°¼é¾™", "æ ‘è„‚", "æ”¹æ€§å¡‘æ–™", "å·¥ç¨‹å¡‘æ–™",
    "æ³¨å¡‘", "å¡‘èƒ¶", "èšé…¯", "èšç¢³é…¸é…¯", "å¡‘æ–™ä»·æ ¼", "å¡‘æ–™åŸæ–™",
    "çŸ³åŒ–", "ä¹™çƒ¯", "ä¸™çƒ¯", "è‹¯ä¹™çƒ¯", "å¡‘æ–™åˆ¶å“"
]


def _crawl_news(category: str, include_custom: bool = True) -> Dict:
    """æ‰§è¡Œæ–°é—»çˆ¬å–"""
    from scrapers.unified import UnifiedDataSource
    
    unified = UnifiedDataSource()
    data = unified.crawl_category(category, include_custom=include_custom)
    
    # ç»Ÿè®¡æ•°æ®æ¥æºåˆ†å¸ƒ
    sources = {}
    for item in data:
        # æŒ‰ç…§ä¼˜å…ˆçº§æå–æ¥æºï¼šplatform_name > source > platform > æœªçŸ¥
        # ä¿®å¤é€»è¾‘ï¼šä¼˜å…ˆå– platform_name æˆ– sourceï¼Œé¿å…å–åˆ° newsnow
        source_name = item.get('platform_name') or item.get('source') or item.get('platform') or 'æœªçŸ¥'
        sources[source_name] = sources.get(source_name, 0) + 1
    
    return {
        "status": "success",
        "category": category,
        "data": data,
        "timestamp": datetime.now().isoformat(),
        "total": len(data),
        "sources": sources  # æ·»åŠ æ¥æºç»Ÿè®¡
    }


def _extract_text_from_html(html_text: str, limit: int = 1200) -> str:
    """ç®€æ˜“æ­£æ–‡æŠ½å–ï¼šå»æ‰æ ‡ç­¾/è„šæœ¬ï¼Œå‹ç¼©ç©ºç™½"""
    import re, html as html_lib
    if not html_text:
        return ""
    cleaned = re.sub(r"(?is)<script.*?>.*?</script>", " ", html_text)
    cleaned = re.sub(r"(?is)<style.*?>.*?</style>", " ", cleaned)
    cleaned = re.sub(r"(?s)<[^>]+>", " ", cleaned)
    cleaned = html_lib.unescape(cleaned)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return cleaned[:limit]


def _fetch_power_partner_news(max_per_company: int = 4) -> list:
    """
    é’ˆå¯¹ç”µæºå‹å•†çš„å®šå‘æŠ“å–ï¼ˆGoogle News RSSï¼‰ï¼Œå¹¶å°è¯•æå–æ­£æ–‡ã€‚
    è¿‡æ»¤æ‰é›ªçƒ/ç™¾åº¦ï¼Œé¿å…å™ªå£°ã€‚
    """
    import requests as req
    import xml.etree.ElementTree as ET
    
    results = []
    seen_titles = set()
    
    headers = {"User-Agent": "Mozilla/5.0 (TrendRadar/1.0)"}
    
    for company, keywords in POWER_PARTNERS.items():
        # å–å…¬å¸åç§°ä½œä¸ºæœç´¢å…³é”®è¯
        keyword = company
        feed_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        try:
            resp = req.get(feed_url, timeout=10, headers=headers)
            if resp.status_code != 200 or not resp.content:
                continue
            root = ET.fromstring(resp.content)
            items = root.findall(".//item")[:max_per_company]
            for item in items:
                title = item.findtext("title", "").strip()
                link = item.findtext("link", "").strip()
                pub = item.findtext("pubDate", "").strip()
                if not title or title in seen_titles:
                    continue
                seen_titles.add(title)
                
                content = ""
                try:
                    article = req.get(link, timeout=8, headers=headers)
                    if article.status_code == 200:
                        content = _extract_text_from_html(article.text)
                except Exception:
                    content = ""
                
                results.append({
                    "title": title,
                    "url": link,
                    "source": f"{company} ç›¸å…³",
                    "publish_time": pub,
                    "content": content
                })
        except Exception:
            continue

    return results


def _fetch_power_official_announcements(max_per_company: int = 5) -> list:
    """
    æŠ“å–ç”µæºå‹å•†å®˜ç½‘å…¬å‘Š/æ–°é—»é¡µå†…å®¹
    """
    import re
    import requests as req
    from urllib.parse import urljoin

    headers = {"User-Agent": "Mozilla/5.0 (TrendRadar/1.0)"}
    results = []

    for company, page_url in POWER_PARTNER_NEWS_PAGES.items():
        try:
            resp = req.get(page_url, timeout=12, headers=headers)
            if resp.status_code != 200 or not resp.text:
                continue
            html = resp.text

            links = []
            try:
                from bs4 import BeautifulSoup

                soup = BeautifulSoup(html, "html.parser")
                links = [(a.get_text(" ", strip=True), a.get("href")) for a in soup.find_all("a", href=True)]
            except Exception:
                raw_links = re.findall(r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>(.*?)</a>', html, flags=re.I | re.S)
                for href, text in raw_links:
                    clean_text = re.sub(r"<[^>]+>", " ", text)
                    clean_text = re.sub(r"\s+", " ", clean_text).strip()
                    links.append((clean_text, href))

            seen_titles = set()
            seen_urls = set()
            for text, href in links:
                if not href or not text or len(text) < 5:
                    continue
                if href.startswith("#") or href.lower().startswith("javascript"):
                    continue
                lower_href = href.lower()
                if not any(k in lower_href for k in ["news", "press", "media", "info", "article", "xinwen", "gonggao"]) and not any(tag in text for tag in ["æ–°é—»", "å…¬å‘Š", "åŠ¨æ€", "èµ„è®¯", "åª’ä½“"]):
                    continue
                full_url = urljoin(page_url, href)
                if text in seen_titles or full_url in seen_urls:
                    continue
                seen_titles.add(text)
                seen_urls.add(full_url)
                results.append({
                    "title": text[:180],
                    "url": full_url,
                    "source": f"{company} å®˜ç½‘",
                    "publish_time": "",
                    "content": ""
                })
                if len(seen_titles) >= max_per_company:
                    break
        except Exception:
            continue

    return results


def _background_crawl_news(cache_key: str, category: str, include_custom: bool = True):
    """åå°çˆ¬å–æ–°é—»å¹¶æ›´æ–°ç¼“å­˜"""
    try:
        print(f"ğŸ”„ [åå°] å¼€å§‹çˆ¬å– {category}...")
        result = _crawl_news(category, include_custom)
        result["cached"] = False
        result["background_refresh"] = True
        
        # 1. å…ˆåŒæ­¥å†™å…¥ MongoDB å†å²å½’æ¡£ï¼ˆç¡®ä¿æ•°æ®å…ˆå…¥åº“ï¼Œè¿™æ · _try_get_from_mongodb_daily æ‰èƒ½æŸ¥åˆ°æœ€æ–°ï¼‰
        try:
            from database.manager import db_manager
            if db_manager.mongodb_enabled:
                if result.get("data"):
                    from database.models import News
                    news_objects = []
                    for item in result["data"]:
                        # å¤„ç†æ—¶é—´
                        p_time = item.get("time") or item.get("publish_time")
                        published_at = None
                        if p_time:
                            try:
                                if isinstance(p_time, str):
                                    # å°è¯•å¤„ç†ä¸åŒæ ¼å¼
                                    try:
                                        published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                                    except:
                                        # å°è¯•è§£æ "YYYY-MM-DD HH:MM" æ ¼å¼
                                        published_at = datetime.strptime(p_time, "%Y-%m-%d %H:%M")
                                else:
                                    published_at = p_time
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()

                        news_objects.append(News(
                            platform_id=item.get("platform", "unknown"),
                            title=item.get("title", ""),
                            url=item.get("url", ""),
                            published_at=published_at,
                            category=category,
                            extra_data=item,
                            source=item.get("source", ""),
                            platform_name=item.get("platform_name") or item.get("source", ""),
                            summary=item.get("summary", "") or item.get("content", "")[:200]
                        ))
                    
                    inserted, updated = db_manager.news_repo.insert_batch(news_objects)
                    print(f"âœ… [åå°] {category} å½’æ¡£åˆ° MongoDB: æ–°å¢ {inserted}, æ›´æ–° {updated}")
        except Exception as e:
            print(f"âš ï¸ [åå°] MongoDB å½’æ¡£å¤±è´¥: {e}")
        
        # é»˜è®¤å…ˆè®¾ç½®å½“å‰æŠ“å–ç»“æœä¸ºç¼“å­˜
        final_result = result
        
        # 2. å°è¯•ä» MongoDB è·å–æœ€è¿‘ 7 å¤©çš„å…¨é‡æ•°æ®ï¼ˆæ­¤æ—¶å·²ç»åŒ…å«äº†åˆšåˆšå…¥åº“çš„æ•°æ®ï¼‰
        daily_data = _try_get_from_mongodb_daily(category)
        if daily_data and daily_data.get("total", 0) > len(result.get("data", [])):
            print(f"ğŸ”„ [åå°] {category} ä½¿ç”¨ MongoDB æœ€è¿‘7å¤©å…¨é‡æ•°æ®æ›´æ–°ç¼“å­˜ ({daily_data.get('total')} æ¡)")
            final_result = daily_data
        
        # 3. å†™å…¥ MongoDB å¿«ç…§ (ä½¿ç”¨å…¨é‡æ•°æ®)
        try:
            if db_manager.mongodb_enabled:
                db_manager.news_repo.save_snapshot(cache_key, final_result)
                print(f"âœ… [åå°] {category} å¿«ç…§å·²ä¿å­˜åˆ° MongoDB (åŒ…å« {final_result.get('total', 0)} æ¡)")
        except Exception as e:
            print(f"âš ï¸ [åå°] MongoDB å¿«ç…§ä¿å­˜å¤±è´¥: {e}")

        # 4. æœ€åå†™å…¥ Redis (ä½¿ç”¨å…¨é‡æ•°æ®)
        cache.set(cache_key, final_result, ttl=CACHE_TTL)
            
        print(f"âœ… [åå°] {category} çˆ¬å–å®Œæˆ: {result['total']} æ¡")
    except Exception as e:
        print(f"âŒ [åå°] {category} çˆ¬å–å¤±è´¥: {e}")
    finally:
        _pending_refreshes.discard(cache_key)


def _background_fetch_realtime(cache_key: str, keywords: list, category: str = None):
    """åå°æ‹“å–å®æ—¶æ–°é—»å¹¶æ›´æ–°ç¼“å­˜"""
    try:
        from .analysis import fetch_realtime_news
        print(f"ğŸ”„ [åå°] å¼€å§‹æ‹“å– {cache_key}...")
        news = fetch_realtime_news(keywords)
        # è¿‡æ»¤ä¸éœ€è¦çš„æ¥æº
        news = [n for n in news if n.get("source") not in ["é›ªçƒ", "ç™¾åº¦"]]
        
        # è¡¥å…¨ platform_name (ç¡®ä¿å…¥åº“å’Œç¼“å­˜éƒ½æœ‰è¯¥å­—æ®µ)
        for item in news:
            if not item.get("platform_name") and item.get("source"):
                item["platform_name"] = item["source"]
        
        # é’ˆå¯¹ supply-chain å¢è¡¥ç”µæºå‹å•†å®šå‘æŠ“å–ï¼ˆå¸¦æ­£æ–‡ï¼‰
        if category == "supply-chain":
            power_news = _fetch_power_partner_news()
            official_news = _fetch_power_official_announcements()
            # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ï¼‰
            seen = {n.get("title") for n in news}
            for item in power_news + official_news:
                if item.get("title") and item["title"] not in seen:
                    seen.add(item["title"])
                    news.append(item)
        
        # ç»Ÿè®¡æ•°æ®æ¥æºåˆ†å¸ƒ
        sources = {}
        for item in news:
            # æŒ‰ç…§ä¼˜å…ˆçº§æå–æ¥æºï¼šplatform_name > source > platform > æœªçŸ¥
            # ä¿®å¤é€»è¾‘ï¼šä¼˜å…ˆå– platform_name æˆ– sourceï¼Œé¿å…å–åˆ° newsnow
            source_name = item.get('platform_name') or item.get('source') or item.get('platform') or 'æœªçŸ¥'
            sources[source_name] = sources.get(source_name, 0) + 1
        
        # 1. å†™å…¥ MongoDB
        try:
            from database.manager import db_manager
            if db_manager.mongodb_enabled and category:
                from database.models import News
                news_objects = []
                for item in news:
                    # å¤„ç†æ—¶é—´
                    p_time = item.get("time")
                    published_at = None
                    if p_time:
                        try:
                            if isinstance(p_time, str):
                                published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                            else:
                                published_at = p_time
                        except:
                            published_at = datetime.now()
                    else:
                        published_at = datetime.now()

                    news_objects.append(News(
                        platform_id=item.get("platform", "unknown"),
                        title=item.get("title", ""),
                        url=item.get("url", ""),
                        published_at=published_at,
                        category=category,
                        extra_data=item,
                        source=item.get("source", ""),
                        platform_name=item.get("platform_name", ""),
                        summary=item.get("summary", "") or item.get("content", "")[:200]
                    ))
                
                inserted, updated = db_manager.news_repo.insert_batch(news_objects)
                print(f"âœ… [åå°] {category} (å®æ—¶) å½’æ¡£åˆ° MongoDB: æ–°å¢ {inserted}, æ›´æ–° {updated}")
        except Exception as e:
            print(f"âš ï¸ [åå°] MongoDB å½’æ¡£å¤±è´¥: {e}")

        result = {
            "status": "success",
            "data": news,
            "timestamp": datetime.now().isoformat(),
            "total": len(news),
            "sources": sources,  # æ·»åŠ æ¥æºç»Ÿè®¡
            "cached": False,
            "background_refresh": True
        }
        if category:
            result["category"] = category
        
        # é»˜è®¤å…ˆè®¾ç½®å½“å‰æŠ“å–ç»“æœä¸ºç¼“å­˜
        final_result = result
        
        # 2. å°è¯•ä» MongoDB è·å–æœ€è¿‘ 7 å¤©çš„å…¨é‡æ•°æ®ï¼ˆæ­¤æ—¶å·²ç»åŒ…å«äº†åˆšåˆšå…¥åº“çš„æ•°æ®ï¼‰
        daily_data = _try_get_from_mongodb_daily(category, keywords)
        if daily_data and daily_data.get("total", 0) > len(news):
            print(f"ğŸ”„ [åå°] {cache_key} ä½¿ç”¨ MongoDB æœ€è¿‘7å¤©å…¨é‡æ•°æ®æ›´æ–°ç¼“å­˜ ({daily_data.get('total')} æ¡)")
            final_result = daily_data
        
        # 3. å†™å…¥ MongoDB å¿«ç…§ (ä½¿ç”¨å…¨é‡æ•°æ®)
        try:
            if db_manager.mongodb_enabled:
                db_manager.news_repo.save_snapshot(cache_key, final_result)
                print(f"âœ… [åå°] {cache_key} å¿«ç…§å·²ä¿å­˜åˆ° MongoDB (åŒ…å« {final_result.get('total', 0)} æ¡)")
        except Exception as e:
            print(f"âš ï¸ [åå°] MongoDB å¿«ç…§ä¿å­˜å¤±è´¥: {e}")

        # 4. æœ€åå†™å…¥ Redis (ä½¿ç”¨å…¨é‡æ•°æ®)
        cache.set(cache_key, final_result, ttl=CACHE_TTL)
        print(f"âœ… [åå°] {cache_key} æ‹“å–å®Œæˆ: {len(news)} æ¡ (æœ€ç»ˆç¼“å­˜: {final_result.get('total')} æ¡)")
                    
    except Exception as e:
        print(f"âŒ [åå°] {cache_key} æ‹“å–å¤±è´¥: {e}")
    finally:
        with _refresh_lock:
            _pending_refreshes.discard(cache_key)


def _trigger_background_refresh(cache_key: str, task_func, *args):
    """è§¦å‘åå°åˆ·æ–°ä»»åŠ¡ï¼ˆå»é‡ï¼‰"""
    with _refresh_lock:
        if cache_key in _pending_refreshes:
            print(f"â³ {cache_key} å·²æœ‰åå°ä»»åŠ¡è¿›è¡Œä¸­ï¼Œè·³è¿‡")
            return False
        _pending_refreshes.add(cache_key)
    
    # åœ¨æµ‹è¯•ç¯å¢ƒä¸‹è·³è¿‡çœŸå®çš„åå°çº¿ç¨‹ï¼Œé¿å…å ç”¨ mock side effectã€å‡å°å¹²æ‰°
    if _TEST_ENV:
        return True
    _executor.submit(task_func, cache_key, *args)
    return True


def _try_get_from_snapshot(cache_key: str, category: str) -> Dict:
    """
    å°è¯•ä» MongoDB å¿«ç…§è·å–æ•°æ® (å¿«ç…§å›æº)
    å¦‚æœæˆåŠŸï¼Œä¼šè‡ªåŠ¨å›å†™åˆ° Redis
    """
    try:
        from database.manager import db_manager
        if db_manager.mongodb_enabled:
            # ä¼˜å…ˆå°è¯•è¯»å–å¿«ç…§
            snapshot = db_manager.news_repo.get_snapshot(cache_key)
            if snapshot and snapshot.get("data"):
                print(f"ğŸ”„ [API] {category} Redis Missï¼Œä» MongoDB å¿«ç…§æ¢å¤")
                result = snapshot["data"]
                
                # ä¿®å¤: ç¡®ä¿ snapshot æ ¼å¼æ­£ç¡®
                if isinstance(result, list):
                    result = {
                        "status": "success",
                        "category": category,
                        "data": result,
                        "timestamp": datetime.now().isoformat(),
                        "total": len(result),
                        "cached": False,
                        "from_snapshot": True
                    }
                else:
                    result["from_snapshot"] = True
                    result["cached"] = False
                
                # è¡¥å…… sources ç»Ÿè®¡
                if "sources" not in result or not result["sources"]:
                    sources = {}
                    for item in result.get("data", []):
                        # è¡¥å…¨ platform_name
                        if not item.get("platform_name") and item.get("source"):
                            item["platform_name"] = item["source"]
                            
                        source_name = item.get('platform_name') or item.get('source') or item.get('platform') or 'æœªçŸ¥'
                        sources[source_name] = sources.get(source_name, 0) + 1
                    result["sources"] = sources
                else:
                    # å³ä½¿ sources å­˜åœ¨ï¼Œä¹Ÿæ£€æŸ¥ä¸€é data ä¸­çš„ platform_name
                    if result.get("data"):
                        for item in result["data"]:
                            if not item.get("platform_name") and item.get("source"):
                                item["platform_name"] = item["source"]
                
                # å›å†™ Redis
                try:
                    cache.set(cache_key, result, ttl=CACHE_TTL)
                    print(f"âœ… [API] {category} å¿«ç…§æ•°æ®å·²å›å†™ Redis")
                except Exception as redis_err:
                    print(f"âš ï¸ [API] å¿«ç…§å›å†™ Redis å¤±è´¥: {redis_err}")
                    
                return result
            else:
                 print(f"âš ï¸ [API] {category} MongoDB å¿«ç…§ä¸å­˜åœ¨æˆ–ä¸ºç©º")
    except Exception as e:
        print(f"âš ï¸ [API] MongoDB å¿«ç…§æ¢å¤å¤±è´¥: {e}")
    return None


def _try_get_from_mongodb_daily(category: str, keywords: list = None, days: int = 7) -> Dict:
    """
    å°è¯•ä» MongoDB news é›†åˆæŸ¥è¯¢æœ€è¿‘ N å¤©çš„æ•°æ®
    """
    try:
        from database.manager import db_manager
        if db_manager.mongodb_enabled:
            print(f"ğŸ”„ [API] {category} å°è¯•æŸ¥è¯¢ MongoDB æœ€è¿‘ {days} å¤©æ•°æ®...")
            
            # è·å–èµ·å§‹æ—¶é—´ï¼ˆæœ€è¿‘ N å¤©çš„é›¶ç‚¹ï¼‰
            start_date = (datetime.now() - timedelta(days=days)).replace(hour=0, minute=0, second=0, microsecond=0)
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {
                "category": category,
                "published_at": {"$gte": start_date}
            }
            
            # å¦‚æœæä¾›äº†å…³é”®è¯ï¼Œå¯ä»¥å¢åŠ æ ‡é¢˜åŒ¹é…ï¼ˆå¯é€‰ï¼Œè§†éœ€æ±‚è€Œå®šï¼Œè¿™é‡Œæš‚æ—¶åªæŒ‰ category æŸ¥ï¼‰
            # if keywords:
            #     query["title"] = {"$regex": "|".join(keywords)}

            # æŸ¥è¯¢æ•°æ®ï¼ŒæŒ‰æ—¶é—´å€’åºï¼Œé™åˆ¶ 500 æ¡
            news_items = list(db_manager.news_repo._col.find(
                query, 
                {"_id": 0}  # ä¸è¿”å› _id
            ).sort("published_at", -1).limit(500))  # æ‰©å¤§é™åˆ¶ä»¥å®¹çº³å¤šå¤©æ•°æ®
            
            if news_items:
                print(f"âœ… [API] {category} ä» MongoDB æŸ¥åˆ° {len(news_items)} æ¡æœ€è¿‘æ•°æ®")
                
                # æ ¼å¼åŒ–æ•°æ®ä»¥åŒ¹é… API è¿”å›æ ¼å¼
                data = []
                seen_titles = set()  # ç”¨äºå»é‡
                sources = {}
                
                for item in news_items:
                    # å»é‡é€»è¾‘
                    title = item.get("title", "")
                    if title in seen_titles:
                        continue
                    seen_titles.add(title)

                    # è½¬æ¢ datetime å¯¹è±¡ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
                    if isinstance(item.get("published_at"), datetime):
                        item["published_at"] = item["published_at"].isoformat()
                    
                    # è¡¥å…¨ platform_name
                    extra_data = item.get('extra_data', {}) or {}  # ç¡®ä¿ extra_data æ˜¯å­—å…¸
                    platform_name = item.get('platform_name') or extra_data.get('platform_name')
                    source = item.get('source') or extra_data.get('source')
                     
                    if not platform_name and source:
                        platform_name = source
                    
                    if platform_name:
                        item["platform_name"] = platform_name
                    

                    data.append(item)
                    
                    # ç»Ÿè®¡æ¥æº
                    source_name = item.get('platform_name') or item.get('source') or item.get('platform') or 'æœªçŸ¥'
                    sources[source_name] = sources.get(source_name, 0) + 1
                
                result = {
                    "status": "success",
                    "category": category,
                    "data": data,
                    "timestamp": datetime.now().isoformat(),
                    "total": len(data),
                    "sources": sources,
                    "cached": False,
                    "from_mongodb_daily": True
                }
                return result
            else:
                print(f"âš ï¸ [API] {category} MongoDB å½“æ—¥æ— æ•°æ®")
    except Exception as e:
        print(f"âš ï¸ [API] MongoDB å½“æ—¥æ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")
    return None


@router.get("/api/commodity-news")
def get_commodity_news(refresh: bool = False):
    """
    è·å–å¤§å®—å•†å“æ–°é—»
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    - refresh=false: ç›´æ¥è¿”å›ç¼“å­˜ï¼ˆ<50msï¼‰
    - refresh=true: ç«‹å³è¿”å›ç¼“å­˜ + åå°å¼‚æ­¥åˆ·æ–°
    """
    cache_key = "news:commodity"
    cached = cache.get(cache_key)
    
    if refresh:
        # è§¦å‘åå°åˆ·æ–°
        triggered = _trigger_background_refresh(cache_key, _background_crawl_news, "commodity", True)
        
        # ç«‹å³è¿”å›ç°æœ‰ç¼“å­˜
        if cached:
            cached["cached"] = True
            cached["refreshing"] = triggered
            cached["message"] = "æ•°æ®æ­£åœ¨åå°åˆ·æ–°ï¼Œç¨åé‡æ–°åŠ è½½è·å–æœ€æ–°æ•°æ®" if triggered else "åˆ·æ–°ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­"
            return cached
        
        # æ— ç¼“å­˜æ—¶è¿”å›ç©ºæ•°æ® + åˆ·æ–°çŠ¶æ€
        return {
            "status": "success",
            "category": "commodity",
            "data": [],
            "sources": {},
            "timestamp": None,
            "cached": False,
            "total": 0,
            "refreshing": triggered,
            "message": "æ•°æ®æ­£åœ¨åå°åŠ è½½ï¼Œè¯·ç¨ååˆ·æ–°é¡µé¢"
        }
    
    # æ­£å¸¸è¯·æ±‚ï¼šç›´æ¥è¿”å›ç¼“å­˜
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)

        # æ£€æŸ¥å¹¶è¡¥å…¨ platform_name
        if cached.get("data"):
            for item in cached["data"]:
                if not item.get("platform_name") and item.get("source"):
                    item["platform_name"] = item["source"]

        return cached
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•ä» MongoDB å¿«ç…§è·å–æœ€æ–°æ•°æ® (å¿«ç…§å›æº)
    snapshot_data = _try_get_from_snapshot(cache_key, "commodity")
    if snapshot_data:
        return snapshot_data
    
    # æ— ç¼“å­˜ä¸”æœªå¼ºåˆ¶åˆ·æ–°æ—¶ï¼Œè‡ªåŠ¨è§¦å‘åå°åˆ·æ–°
    triggered = _trigger_background_refresh(cache_key, _background_crawl_news, "commodity", True)
    
    return {
        "status": "success",
        "category": "commodity",
        "data": [],
        "sources": {},
        "timestamp": None,
        "cached": False,
        "total": 0,
        "refreshing": triggered,
        "message": "æš‚æ— ç¼“å­˜æ•°æ®ï¼Œæ­£åœ¨åå°è·å–..." if triggered else "æ•°æ®æ­£åœ¨åå°åŠ è½½"
    }


@router.get("/api/news/supply-chain")
def get_supply_chain_news(refresh: bool = False):
    """
    è·å–ä¾›åº”é“¾ç›¸å…³æ–°é—»
    
    ä¼˜åŒ–ç­–ç•¥ï¼šç¼“å­˜ä¼˜å…ˆ + åå°å¼‚æ­¥åˆ·æ–°
    """
    cache_key = "news:supply-chain"
    cached = cache.get(cache_key)
    
    if refresh:
        triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, SUPPLY_CHAIN_KEYWORDS, "supply-chain")
        
        if cached:
            cached["cached"] = True
            cached["refreshing"] = triggered
            cached["message"] = "æ•°æ®æ­£åœ¨åå°åˆ·æ–°" if triggered else "åˆ·æ–°ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­"
            
            # è¡¥å…¨ platform_name
            if cached.get("data"):
                for item in cached["data"]:
                    extra = item.get("extra_data", {}) or {}
                    p_name = item.get("platform_name") or extra.get("platform_name")
                    src = item.get("source") or extra.get("source")
                    if not p_name and src:
                        p_name = src
                    if p_name:
                        item["platform_name"] = p_name

            return cached
        
        return {
            "status": "success",
            "data": [],
            "sources": {},
            "timestamp": None,
            "cached": False,
            "total": 0,
            "refreshing": triggered,
            "message": "æ•°æ®æ­£åœ¨åå°åŠ è½½"
        }
    
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)
        return cached

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•ä» MongoDB å¿«ç…§è·å–æœ€æ–°æ•°æ® (å¿«ç…§å›æº)
    snapshot_data = _try_get_from_snapshot(cache_key, "supply-chain")
    if snapshot_data:
        return snapshot_data
    
    return {
        "status": "success",
        "data": [],
        "sources": {},
        "timestamp": None,
        "cached": False,
        "total": 0,
        "message": "æš‚æ— ç¼“å­˜æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–æœ€æ–°æ•°æ®"
    }


@router.get("/api/news/tariff")
def get_tariff_news(refresh: bool = False):
    """
    è·å–å…³ç¨æ”¿ç­–ç›¸å…³æ–°é—»
    
    ä¼˜åŒ–ç­–ç•¥ï¼šç¼“å­˜ä¼˜å…ˆ + åå°å¼‚æ­¥åˆ·æ–°
    """
    cache_key = "news:tariff"
    cached = cache.get(cache_key)
    
    if refresh:
        triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, TARIFF_KEYWORDS, "tariff")
        
        if cached:
            cached["cached"] = True
            cached["refreshing"] = triggered
            cached["message"] = "æ•°æ®æ­£åœ¨åå°åˆ·æ–°" if triggered else "åˆ·æ–°ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­"
            return cached
        
        return {
            "status": "success",
            "category": "tariff",
            "data": [],
            "sources": {},
            "timestamp": None,
            "cached": False,
            "total": 0,
            "refreshing": triggered,
            "message": "æ•°æ®æ­£åœ¨åå°åŠ è½½"
        }
    
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)
        return cached

    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•ä» MongoDB å¿«ç…§è·å–æœ€æ–°æ•°æ® (å¿«ç…§å›æº)
    snapshot_data = _try_get_from_snapshot(cache_key, "tariff")
    if snapshot_data:
        return snapshot_data
    
    return {
        "status": "success",
        "category": "tariff",
        "data": [],
        "sources": {},
        "timestamp": None,
        "cached": False,
        "total": 0,
        "message": "æš‚æ— å…³ç¨æ”¿ç­–ç¼“å­˜æ•°æ®ï¼Œè¯·ç‚¹å‡»åˆ·æ–°æŒ‰é’®è·å–æœ€æ–°æ•°æ®"
    }


@router.get("/api/news/plastics")
def get_plastics_news(refresh: bool = False):
    """
    è·å–å¡‘æ–™ç›¸å…³æ–°é—»
    
    ä¼˜åŒ–ç­–ç•¥ï¼šç¼“å­˜ä¼˜å…ˆ + åå°å¼‚æ­¥åˆ·æ–°
    """
    cache_key = "news:plastics"
    cached = cache.get(cache_key)
    
    if refresh:
        triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, PLASTICS_KEYWORDS, "plastics")
        
        if cached:
            cached["cached"] = True
            cached["refreshing"] = triggered
            cached["message"] = "æ•°æ®æ­£åœ¨åå°åˆ·æ–°" if triggered else "åˆ·æ–°ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­"
            
            # è¡¥å…¨ platform_name
            if cached.get("data"):
                for item in cached["data"]:
                    extra = item.get("extra_data", {}) or {}
                    p_name = item.get("platform_name") or extra.get("platform_name")
                    src = item.get("source") or extra.get("source")
                    if not p_name and src:
                        p_name = src
                    if p_name:
                        item["platform_name"] = p_name

            return cached
        
        return {
            "status": "success",
            "category": "plastics",
            "data": [],
            "sources": {},
            "timestamp": None,
            "cached": False,
            "total": 0,
            "refreshing": triggered,
            "message": "æ•°æ®æ­£åœ¨åå°åŠ è½½"
        }
    
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)

                    
        return cached
    
    # ä» MongoDB æŸ¥è¯¢å½“æ—¥æ•°æ®
    daily_data = _try_get_from_mongodb_daily("plastics", PLASTICS_KEYWORDS)
    if daily_data:
        # å›å†™ Redis
        cache.set(cache_key, daily_data, ttl=CACHE_TTL)
        return daily_data

    # æ— ç¼“å­˜ä¸”æœªå¼ºåˆ¶åˆ·æ–°æ—¶ï¼Œè‡ªåŠ¨è§¦å‘åå°åˆ·æ–°
    triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, PLASTICS_KEYWORDS, "plastics")
    
    return {
        "status": "success",
        "category": "plastics",
        "data": [],
        "sources": {},
        "timestamp": None,
        "cached": False,
        "total": 0,
        "refreshing": triggered,
        "message": "æš‚æ— ç¼“å­˜æ•°æ®ï¼Œæ­£åœ¨åå°è·å–..." if triggered else "æ•°æ®æ­£åœ¨åå°åŠ è½½"
    }


@router.get("/api/news/{category}")
def get_news(category: str, include_custom: bool = True, refresh: bool = False):
    """
    è·å–æŒ‡å®šåˆ†ç±»çš„æ–°é—»
    
    ä¼˜åŒ–ç­–ç•¥ï¼šç¼“å­˜ä¼˜å…ˆ + åå°å¼‚æ­¥åˆ·æ–°
    å“åº”æ—¶é—´ï¼š<50msï¼ˆä»ç¼“å­˜è¯»å–ï¼‰
    """
    cache_key = f"news:{category}"
    cached = cache.get(cache_key)
    
    if refresh:
        triggered = _trigger_background_refresh(cache_key, _background_crawl_news, category, include_custom)
        
        if cached:
            cached["cached"] = True
            cached["refreshing"] = triggered
            cached["message"] = f"{category} æ•°æ®æ­£åœ¨åå°åˆ·æ–°" if triggered else "åˆ·æ–°ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­"
            return cached
        
        return {
            "status": "success",
            "category": category,
            "data": [],
            "sources": {},  # æ·»åŠ ç©ºsourceså­—æ®µ
            "timestamp": None,
            "cached": False,
            "total": 0,
            "refreshing": triggered,
            "message": f"{category} æ•°æ®æ­£åœ¨åå°åŠ è½½"
        }
    
    if cached:
        cached["cached"] = True
        cached["cache_ttl"] = cache.get_ttl(cache_key)
        
        # è¡¥å…¨ platform_name (é˜²æ­¢ç¼“å­˜æ•°æ®ç¼ºå¤±)
        if cached.get("data"):
            for item in cached["data"]:
                extra = item.get("extra_data", {}) or {}
                p_name = item.get("platform_name") or extra.get("platform_name")
                src = item.get("source") or extra.get("source")
                if not p_name and src:
                    p_name = src
                if p_name:
                    item["platform_name"] = p_name
        
        return cached
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå°è¯•ä» MongoDB å¿«ç…§è·å–æœ€æ–°æ•°æ® (å¿«ç…§å›æº)
    snapshot_data = _try_get_from_snapshot(cache_key, category)
    if snapshot_data:
        return snapshot_data

    # æ— ç¼“å­˜ä¸”æœªå¼ºåˆ¶åˆ·æ–°æ—¶ï¼Œè‡ªåŠ¨è§¦å‘åå°åˆ·æ–°
    triggered = _trigger_background_refresh(cache_key, _background_crawl_news, category, include_custom)
    
    return {
        "status": "success",
        "category": category,
        "data": [],
        "sources": {},  # æ·»åŠ ç©ºsourceså­—æ®µ
        "timestamp": None,
        "cached": False,
        "total": 0,
        "refreshing": triggered,
        "message": f"æš‚æ—  {category} ç¼“å­˜æ•°æ®ï¼Œæ­£åœ¨åå°è·å–..." if triggered else f"{category} æ•°æ®æ­£åœ¨åå°åŠ è½½"
    }


@router.post("/api/crawl")
def trigger_crawl(request: CrawlRequest, background_tasks: BackgroundTasks):
    """
    è§¦å‘çˆ¬å–ä»»åŠ¡ï¼ˆåå°å¼‚æ­¥æ‰§è¡Œï¼‰
    
    ç«‹å³è¿”å›å“åº”ï¼Œçˆ¬å–åœ¨åå°è¿›è¡Œ
    """
    if request.category in ["supply-chain", "supply_chain"]:
        cache_key = "news:supply-chain"
        triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, SUPPLY_CHAIN_KEYWORDS, None)
    elif request.category == "tariff":
        cache_key = "news:tariff"
        triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, TARIFF_KEYWORDS, "tariff")
    elif request.category == "plastics":
        cache_key = "news:plastics"
        triggered = _trigger_background_refresh(cache_key, _background_fetch_realtime, PLASTICS_KEYWORDS, "plastics")
    else:
        cache_key = f"news:{request.category}"
        triggered = _trigger_background_refresh(cache_key, _background_crawl_news, request.category, request.include_custom)
    
    return {
        "status": "success",
        "category": request.category,
        "triggered": triggered,
        "message": f"{request.category} çˆ¬å–ä»»åŠ¡å·²æäº¤åå°æ‰§è¡Œ" if triggered else f"{request.category} çˆ¬å–ä»»åŠ¡å·²åœ¨è¿›è¡Œä¸­"
    }


@router.get("/api/refresh-status")
def get_refresh_status():
    """è·å–åå°åˆ·æ–°ä»»åŠ¡çŠ¶æ€"""
    return {
        "pending_tasks": list(_pending_refreshes),
        "count": len(_pending_refreshes)
    }


# ==================== å®¢æˆ·é…ç½® ====================
CUSTOMERS = {
    "è‹¹æœ": ["è‹¹æœ", "Apple", "iPhone", "AirPods", "Apple Watch", "Vision Pro", "iPad", "Mac", "AAPL"],
    "åä¸º": ["åä¸º", "Huawei", "é¸¿è’™", "Mate", "è£è€€", "Honor"],
    "Meta": ["Meta", "Quest", "VR", "Facebook", "Oculus"],
    "å°ç±³": ["å°ç±³", "Xiaomi", "çº¢ç±³", "Redmi"],
    "OPPO": ["OPPO", "ä¸€åŠ ", "OnePlus", "realme"],
    "vivo": ["vivo", "iQOO"],
    "ä¸‰æ˜Ÿ": ["ä¸‰æ˜Ÿ", "Samsung", "Galaxy"],
    "å¥‡ç‘æ±½è½¦": ["å¥‡ç‘", "Chery", "æ˜Ÿé€”", "æ·é€”"],
    "ç‰¹æ–¯æ‹‰": ["ç‰¹æ–¯æ‹‰", "Tesla", "Model"],
    "æ¯”äºšè¿ª": ["æ¯”äºšè¿ª", "BYD", "ä»°æœ›", "è…¾åŠ¿"],
}

# ==================== ä¾›åº”å•†é…ç½® ====================
SUPPLIERS = {
    "ICèŠ¯ç‰‡": {
        "Marvell": ["Marvell", "è¿ˆå¨å°”"],
        "Broadcom": ["Broadcom", "åšé€š"],
        "ADI": ["ADI", "äºšå¾·è¯º", "Analog Devices"],
        "TI": ["TI", "å¾·å·ä»ªå™¨", "Texas Instruments"],
        "ST": ["ST", "æ„æ³•åŠå¯¼ä½“", "STMicroelectronics"],
    },
    "PCB": {
        "é¹é¼æ§è‚¡": ["é¹é¼", "002938"],
        "ä¸œå±±ç²¾å¯†": ["ä¸œå±±ç²¾å¯†", "002384"],
        "æ·±å—ç”µè·¯": ["æ·±å—ç”µè·¯", "002916"],
    },
    "è¢«åŠ¨å…ƒä»¶": {
        "æ‘ç”°": ["æ‘ç”°", "Murata"],
        "å›½å·¨": ["å›½å·¨", "Yageo"],
        "é£åé«˜ç§‘": ["é£åé«˜ç§‘", "000636"],
    },
    "æ³¨å¡‘/å‹é“¸": {
        "é¢†ç›Šæ™ºé€ ": ["é¢†ç›Šæ™ºé€ ", "002600"],
        "é•¿ç›ˆç²¾å¯†": ["é•¿ç›ˆç²¾å¯†", "300115"],
    },
}

# ==================== ç‰©æ–™å“ç±»é…ç½® ====================
MATERIALS = {
    "IC/èŠ¯ç‰‡": ["IC", "èŠ¯ç‰‡", "MCU", "CPU", "GPU", "SoC", "FPGA", "å­˜å‚¨", "å†…å­˜"],
    "PCB/ç”µè·¯æ¿": ["PCB", "ç”µè·¯æ¿", "FPC", "æŸ”æ€§ç”µè·¯", "HDI"],
    "è¿æ¥å™¨": ["è¿æ¥å™¨", "æ¥æ’ä»¶", "ç«¯å­", "FFC", "FPCè¿æ¥å™¨"],
    "è¢«åŠ¨å…ƒä»¶": ["ç”µé˜»", "ç”µå®¹", "ç”µæ„Ÿ", "MLCC", "è´´ç‰‡ç”µé˜»"],
    "ä¼ æ„Ÿå™¨": ["ä¼ æ„Ÿå™¨", "æ‘„åƒå¤´", "CIS", "é™€èºä»ª", "åŠ é€Ÿåº¦è®¡"],
    "ç”µæ± ": ["ç”µæ± ", "é”‚ç”µ", "ç”µèŠ¯", "BMS", "å……ç”µ"],
    "æ˜¾ç¤ºå±": ["æ˜¾ç¤ºå±", "LCD", "OLED", "AMOLED", "é¢æ¿"],
    "ç»“æ„ä»¶": ["ç»“æ„ä»¶", "å¤–å£³", "ä¸­æ¡†", "æ•£çƒ­", "å‹é“¸", "æ³¨å¡‘"],
}


def _match_news(news_list, entity_config, website_map=None):
    """é€šç”¨æ–°é—»åŒ¹é…å‡½æ•°"""
    stats = {}
    for name, keywords in entity_config.items():
        count = 0
        matched_news = []
        for news in news_list:
            title = news.get("title", "")
            summary = news.get("summary", "") or news.get("content", "")
            text = f"{title} {summary}"
            
            for kw in keywords:
                if kw in text:
                    count += 1
                    matched_news.append({
                        "title": title,
                        "url": news.get("url", ""),
                        "source": news.get("source", ""),
                        "publish_time": news.get("publish_time", ""),
                        "matched_keyword": kw
                    })
                    break
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        matched_news.sort(key=lambda x: x.get("publish_time", ""), reverse=True)
        
        stats[name] = {
            "keywords": keywords,
            "news_count": count,
            "news": matched_news[:10],  # æœ€å¤šè¿”å›10æ¡
            "website": (website_map or {}).get(name)
        }
    return stats


@router.get("/api/partner-news-stats")
def get_partner_news_stats():
    """è·å–å‹å•†æ–°é—»ç»Ÿè®¡"""
    cached = cache.get("news:supply-chain")
    news_list = cached.get("data", []) if cached else []
    
    categories = {
        "å…‰ç”µæ¨¡å—": OPTICAL_PARTNERS,
        "è¿æ¥å™¨": CONNECTOR_PARTNERS,
        "ç”µæº": POWER_PARTNERS,
    }
    
    stats = {}
    for category_name, partners in categories.items():
        website_map = POWER_PARTNER_SITES if category_name == "ç”µæº" else None
        stats[category_name] = _match_news(news_list, partners, website_map)
    
    total_partners = sum(len(p) for p in categories.values())
    partners_with_news = sum(
        1 for cat in stats.values() 
        for p in cat.values() 
        if p["news_count"] > 0
    )
    
    # è®¡ç®—åŒ¹é…çš„æ–°é—»æ€»æ•°
    matched_total = sum(
        p["news_count"] for cat in stats.values() for p in cat.values()
    )
    
    return {
        "status": "success",
        "total_news": matched_total,
        "total_partners": total_partners,
        "partners_with_news": partners_with_news,
        "stats": stats
    }


@router.get("/api/customer-news-stats")
def get_customer_news_stats():
    """è·å–å®¢æˆ·æ–°é—»ç»Ÿè®¡"""
    cached = cache.get("news:supply-chain")
    news_list = cached.get("data", []) if cached else []
    
    stats = _match_news(news_list, CUSTOMERS)
    
    customers_with_news = sum(1 for c in stats.values() if c["news_count"] > 0)
    matched_total = sum(c["news_count"] for c in stats.values())
    
    return {
        "status": "success",
        "total_news": matched_total,
        "total_customers": len(CUSTOMERS),
        "customers_with_news": customers_with_news,
        "stats": stats
    }


@router.get("/api/supplier-news-stats")
def get_supplier_news_stats():
    """è·å–ä¾›åº”å•†æ–°é—»ç»Ÿè®¡"""
    cached = cache.get("news:supply-chain")
    news_list = cached.get("data", []) if cached else []
    
    stats = {}
    for category, suppliers in SUPPLIERS.items():
        stats[category] = _match_news(news_list, suppliers)
    
    total_suppliers = sum(len(s) for s in SUPPLIERS.values())
    suppliers_with_news = sum(
        1 for cat in stats.values()
        for s in cat.values()
        if s["news_count"] > 0
    )
    matched_total = sum(
        s["news_count"] for cat in stats.values() for s in cat.values()
    )
    
    return {
        "status": "success",
        "total_news": matched_total,
        "total_suppliers": total_suppliers,
        "suppliers_with_news": suppliers_with_news,
        "stats": stats
    }


@router.get("/api/material-news-stats")
def get_material_news_stats():
    """è·å–ç‰©æ–™å“ç±»æ–°é—»ç»Ÿè®¡"""
    cached = cache.get("news:supply-chain")
    news_list = cached.get("data", []) if cached else []
    
    stats = _match_news(news_list, MATERIALS)
    
    materials_with_news = sum(1 for m in stats.values() if m["news_count"] > 0)
    matched_total = sum(m["news_count"] for m in stats.values())
    
    return {
        "status": "success",
        "total_news": matched_total,
        "total_materials": len(MATERIALS),
        "materials_with_news": materials_with_news,
        "stats": stats
    }


@router.get("/api/tariff-news-stats")
def get_tariff_news_stats():
    """è·å–å…³ç¨æ”¿ç­–æ–°é—»ç»Ÿè®¡ï¼ˆAIæ™ºèƒ½åˆ†ç±»ï¼‰"""
    cached = cache.get("news:tariff")
    news_list = cached.get("data", []) if cached else []
    
    # æ™ºèƒ½åˆ†ç±»è§„åˆ™
    categories = {
        "ä¸­ç¾å…³ç¨": ["ä¸­ç¾", "ç¾å›½", "ç‰¹æœ—æ™®", "æ‹œç™»", "301", "è´¸æ˜“æˆ˜", "ç¾ä¸­"],
        "æ¬§ç›Ÿæ”¿ç­–": ["æ¬§ç›Ÿ", "æ¬§æ´²", "CBAM", "ç¢³å…³ç¨", "ç¢³è¾¹å¢ƒ"],
        "å‡ºå£ç®¡åˆ¶": ["å‡ºå£ç®¡åˆ¶", "å®ä½“æ¸…å•", "åˆ¶è£", "ç¦ä»¤", "ç®¡åˆ¶"],
        "è¿›å£å…³ç¨": ["è¿›å£", "åŠ å¾", "å…³ç¨", "ç¨ç‡", "æµ·å…³"],
        "è‡ªè´¸åå®š": ["è‡ªè´¸", "FTA", "RCEP", "åå®š", "å‡å…"],
        "å…¶ä»–æ”¿ç­–": []  # å…œåº•åˆ†ç±»
    }
    
    stats = {}
    used_news = set()
    
    for cat_name, keywords in categories.items():
        matched_news = []
        for news in news_list:
            if id(news) in used_news:
                continue
            title = news.get("title", "")
            
            if keywords:  # æœ‰å…³é”®è¯çš„åˆ†ç±»
                if any(kw in title for kw in keywords):
                    matched_news.append({
                        "title": title,
                        "url": news.get("url", ""),
                        "source": news.get("source", ""),
                        "publish_time": news.get("publish_time", "")
                    })
                    used_news.add(id(news))
            else:  # "å…¶ä»–æ”¿ç­–" å…œåº•
                matched_news.append({
                    "title": title,
                    "url": news.get("url", ""),
                    "source": news.get("source", ""),
                    "publish_time": news.get("publish_time", "")
                })
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        matched_news.sort(key=lambda x: x.get("publish_time", ""), reverse=True)
        
        stats[cat_name] = {
            "news_count": len(matched_news),
            "news": matched_news[:10]
        }
    
    matched_total = sum(s["news_count"] for s in stats.values())
    
    return {
        "status": "success",
        "total_news": matched_total,
        "stats": stats
    }


@router.get("/api/reader/{news_id}")
def read_news_article(news_id: str):
    """
    æ–°é—»é˜…è¯»å™¨ - æ˜¾ç¤ºä¿å­˜çš„æ–‡ç« å†…å®¹
    
    ç”¨äºéœ€è¦ç™»å½•çš„å¤–éƒ¨ç½‘ç«™ï¼ˆå¦‚ Plaswayï¼‰ï¼Œç›´æ¥æ˜¾ç¤ºå·²çˆ¬å–çš„å†…å®¹
    """
    from fastapi.responses import HTMLResponse
    
    # ç›´æ¥ä½¿ç”¨å…¨å±€ç¼“å­˜è·å–å†…å®¹
    # cache.get ä¼šè‡ªåŠ¨å¤„ç† key å‰ç¼€ (trendradar:) å’Œ JSON è§£æ
    news = cache.get(f"reader:{news_id}")
    
    if not news:
        raise HTTPException(status_code=404, detail="æ–‡ç« å†…å®¹ä¸å¯ç”¨æˆ–å·²è¿‡æœŸ")
    
    # è¿”å›ç¾åŒ–çš„ HTML é¡µé¢
    html_template = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{news.get('title', 'æ–‡ç« é˜…è¯»')}</title>
            <style>
                * {{ margin: 0; padding: 0; box-sizing: border-box; }}
                body {{ 
                    max-width: 800px; 
                    margin: 0 auto; 
                    padding: 40px 20px;
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
                    line-height: 1.8;
                    background: #f9fafb;
                    color: #1f2937;
                }}
                .container {{
                    background: white;
                    border-radius: 12px;
                    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
                    padding: 40px;
                }}
                .header {{
                    border-bottom: 2px solid #e5e7eb;
                    padding-bottom: 24px;
                    margin-bottom: 32px;
                }}
                h1 {{ 
                    font-size: 28px; 
                    font-weight: 700;
                    margin-bottom: 16px;
                    color: #111827;
                    line-height: 1.4;
                }}
                .meta {{ 
                    color: #6b7280; 
                    font-size: 14px;
                    display: flex;
                    align-items: center;
                    gap: 12px;
                    flex-wrap: wrap;
                }}
                .source-badge {{
                    display: inline-block;
                    background: linear-gradient(135deg, #3b82f6, #2563eb);
                    color: white;
                    padding: 4px 12px;
                    border-radius: 20px;
                    font-size: 12px;
                    font-weight: 500;
                }}
                .section-badge {{
                    display: inline-block;
                    background: #f3f4f6;
                    color: #4b5563;
                    padding: 4px 12px;
                    border-radius: 20px;
                    font-size: 12px;
                }}
                .content {{
                    font-size: 17px;
                    color: #374151;
                    white-space: pre-wrap;
                }}
                .content p {{
                    margin-bottom: 16px;
                }}
                .footer {{
                    margin-top: 40px;
                    padding-top: 20px;
                    border-top: 1px solid #e5e7eb;
                    font-size: 13px;
                    color: #9ca3af;
                    text-align: center;
                }}
                .footer a {{
                    color: #3b82f6;
                    text-decoration: none;
                }}
                .footer a:hover {{
                    text-decoration: underline;
                }}
                .back-btn {{
                    display: inline-flex;
                    align-items: center;
                    gap: 6px;
                    color: #3b82f6;
                    text-decoration: none;
                    font-size: 14px;
                    margin-bottom: 20px;
                }}
                .back-btn:hover {{
                    text-decoration: underline;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <a href="javascript:history.back()" class="back-btn">
                    â† è¿”å›åˆ—è¡¨
                </a>
                <div class="header">
                    <h1>{news.get('title', '')}</h1>
                    <div class="meta">
                        <span class="source-badge">{news.get('source', 'Plasway')}</span>
                        <span class="section-badge">{news.get('section', 'è¡Œä¸šèµ„è®¯')}</span>
                        <span>{news.get('timestamp', '')[:10] if news.get('timestamp') else ''}</span>
                    </div>
                </div>
                <div class="content">
                    {news.get('content', 'å†…å®¹åŠ è½½ä¸­...')}
                </div>
                <div class="footer">
                    å†…å®¹æ¥æºï¼š<a href="{news.get('original_url', '#')}" target="_blank" rel="noopener">åŸæ–‡é“¾æ¥</a>
                    <br>
                    ç”± TrendRadar è‡ªåŠ¨æŠ“å–å¹¶ç¼“å­˜
                </div>
            </div>
        </body>
        </html>
        """
        
    return HTMLResponse(content=html_template)
