"""
æ¨¡å—åŒ–åˆ†æ API V4 - åŠ¨æ€å…³ç¨åˆ†ç±» + åŸææ–™åˆ†ç¦»åˆ†æ

æ ¸å¿ƒæ”¹è¿›ï¼ˆåŸºäºé¢†å¯¼åé¦ˆï¼‰ï¼š
1. å…³ç¨æ”¿ç­–ï¼šAI å…ˆåˆ†ç±»ï¼Œç„¶åé€ä¸€å•ç‹¬åˆ†æ
2. åŸææ–™ï¼šæ•°æ®ç›´æ¥åµŒå…¥ + æˆæœ¬åˆ†æå•ç‹¬èµ°å¤§æ¨¡å‹
3. æ¨¡å—å®Œå…¨ç‹¬ç«‹ï¼Œæœ€åæ‹¼è£…æ•´åˆ

æµç¨‹ï¼š
    ç¬¬ä¸€è½®ï¼ˆå¹¶è¡Œï¼‰ï¼šå®¢æˆ·åˆ†æã€å‹å•†åˆ†æã€å…³ç¨åˆ†ç±»
    ç¬¬äºŒè½®ï¼ˆå¹¶è¡Œï¼‰ï¼šå…³ç¨å„åˆ†ç±»å•ç‹¬åˆ†æã€åŸææ–™æˆæœ¬åˆ†æ
    ç¬¬ä¸‰è½®ï¼šæ‰§è¡Œæ‘˜è¦æ•´åˆ
    æœ€ç»ˆï¼šæ‹¼è£…æŠ¥å‘Š
"""
import asyncio
import aiohttp
import json
import re
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
from fastapi import APIRouter, HTTPException
from fastapi.concurrency import run_in_threadpool
import yaml
import os

from ..cache import cache
from ..models import AnalysisRequest

from prompts.analysis_prompts_v4 import (
    CUSTOMER_MODULE,
    COMPETITOR_MODULE,
    TARIFF_CLASSIFIER_MODULE,
    MATERIAL_ANALYSIS_MODULE,
    SUMMARY_MODULE,
    get_module_prompt,
    get_tariff_analysis_prompt,
    build_material_section,
    fetch_news_full_content,
    filter_tariff_news,
    filter_news_by_category,
    precheck_news_quality,
    assemble_final_report_v4
)

router = APIRouter()
BASE_DIR = Path(__file__).parent.parent.parent


def get_ai_config():
    """è·å– AI é…ç½®"""
    config_path = BASE_DIR / "config" / "config.yaml"
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    ai_config = config.get("ai", {})
    external = ai_config.get("external", {})
    return {
        "api_key": external.get("api_key", "") or os.environ.get("AI_API_KEY", ""),
        "api_base": external.get("api_base", "https://generativelanguage.googleapis.com/v1beta"),
        "model": external.get("model", "gemini-3-pro-preview"),
    }


async def call_ai_async(
    session: aiohttp.ClientSession,
    api_base: str, api_key: str, model: str,
    system_prompt: str, user_prompt: str,
    max_tokens: int = 1500, timeout: int = 90
) -> str:
    """å¼‚æ­¥è°ƒç”¨ AI API"""
    is_google_api = "generativelanguage.googleapis.com" in api_base
    
    if is_google_api:
        headers = {"Content-Type": "application/json", "x-goog-api-key": api_key}
        payload = {
            "system_instruction": {"parts": [{"text": system_prompt}]},
            "contents": [{"role": "user", "parts": [{"text": user_prompt}]}],
            "generation_config": {"temperature": 1.0, "max_output_tokens": max_tokens}
        }
        url = f"{api_base.rstrip('/')}/models/{model}:generateContent?key={api_key}"
    else:
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.7,
            "max_tokens": max_tokens
        }
        url = f"{api_base.rstrip('/')}/chat/completions"
    
    async with session.post(url, headers=headers, json=payload, 
                           timeout=aiohttp.ClientTimeout(total=timeout)) as response:
        if response.status != 200:
            text = await response.text()
            raise Exception(f"API error {response.status}: {text[:200]}")
        
        result = await response.json()
        
        if is_google_api:
            candidates = result.get("candidates", [])
            for c in candidates:
                parts = c.get("content", {}).get("parts", [])
                texts = [p.get("text", "") for p in parts if isinstance(p, dict)]
                if texts:
                    return "".join(texts).strip()
            raise Exception("No response from Gemini")
        else:
            if "choices" in result and result["choices"]:
                return result["choices"][0]["message"]["content"]
            raise Exception("No response")


def parse_tariff_categories(response: str) -> List[str]:
    """
    è§£æå…³ç¨åˆ†ç±»ç»“æœ
    
    Args:
        response: AI è¿”å›çš„ JSON æ•°ç»„å­—ç¬¦ä¸²
    
    Returns:
        åˆ†ç±»åˆ—è¡¨
    """
    # å°è¯•ç›´æ¥è§£æ JSON
    try:
        # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—
        cleaned = response.strip()
        if cleaned.startswith("```"):
            # å»æ‰ä»£ç å—æ ‡è®°
            cleaned = re.sub(r'^```json?\s*', '', cleaned)
            cleaned = re.sub(r'\s*```$', '', cleaned)
        
        categories = json.loads(cleaned)
        if isinstance(categories, list):
            return [str(c) for c in categories if c]
        return []
    except json.JSONDecodeError:
        pass
    
    # å°è¯•ç”¨æ­£åˆ™æå–
    try:
        # æ‰¾åˆ°ç±»ä¼¼ ["xxx", "yyy"] çš„æ¨¡å¼
        match = re.search(r'\[([^\]]*)\]', response)
        if match:
            content = match.group(1)
            # æå–å¼•å·å†…çš„å†…å®¹
            items = re.findall(r'"([^"]+)"', content)
            if items:
                return items
    except Exception:
        pass
    
    # æœ€åå°è¯•æŒ‰è¡Œåˆ†å‰²
    lines = response.strip().split('\n')
    categories = []
    for line in lines:
        line = line.strip().strip('-').strip('â€¢').strip()
        if line and not line.startswith('[') and not line.startswith('{'):
            categories.append(line)
    
    return categories[:10]  # æœ€å¤šè¿”å›10ä¸ªåˆ†ç±»


async def run_first_round(
    news_summary: str,
    news_with_content: str,
    ai_config: dict
) -> Dict[str, any]:
    """
    ç¬¬ä¸€è½®ï¼šå¹¶è¡Œè¿è¡Œå®¢æˆ·ã€å‹å•†ã€å…³ç¨åˆ†ç±»æ¨¡å—
    
    Returns:
        {
            "customer": "å®¢æˆ·åˆ†æç»“æœ",
            "competitor": "å‹å•†åˆ†æç»“æœ", 
            "tariff_categories": ["ä¸­ç¾", "ä¸­æ¬§", ...]
        }
    """
    results = {}
    
    async with aiohttp.ClientSession() as session:
        # å‡†å¤‡ä»»åŠ¡
        tasks = []
        task_names = []
        
        # å®¢æˆ·æ¨¡å—
        customer_prompt = get_module_prompt(CUSTOMER_MODULE, news_summary=news_summary)
        tasks.append(call_ai_async(
            session, ai_config["api_base"], ai_config["api_key"], ai_config["model"],
            customer_prompt["system"], customer_prompt["user"], customer_prompt["max_tokens"]
        ))
        task_names.append("customer")
        
        # å‹å•†æ¨¡å—
        competitor_prompt = get_module_prompt(COMPETITOR_MODULE, news_summary=news_summary)
        tasks.append(call_ai_async(
            session, ai_config["api_base"], ai_config["api_key"], ai_config["model"],
            competitor_prompt["system"], competitor_prompt["user"], competitor_prompt["max_tokens"]
        ))
        task_names.append("competitor")
        
        # å…³ç¨åˆ†ç±»æ¨¡å—
        classifier_prompt = get_module_prompt(TARIFF_CLASSIFIER_MODULE, news_with_content=news_with_content)
        tasks.append(call_ai_async(
            session, ai_config["api_base"], ai_config["api_key"], ai_config["model"],
            classifier_prompt["system"], classifier_prompt["user"], classifier_prompt["max_tokens"]
        ))
        task_names.append("tariff_classifier")
        
        print(f"ğŸš€ [V4] ç¬¬ä¸€è½®ï¼šå¹¶è¡Œè°ƒç”¨ {len(tasks)} ä¸ªæ¨¡å—...")
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        for name, resp in zip(task_names, responses):
            if isinstance(resp, Exception):
                print(f"âš ï¸ æ¨¡å— {name} å¤±è´¥: {resp}")
                if name == "tariff_classifier":
                    results["tariff_categories"] = []
                else:
                    results[name] = f"*{name} æ¨¡å—åˆ†æå¤±è´¥*"
            else:
                print(f"âœ… æ¨¡å— {name} å®Œæˆ")
                if name == "tariff_classifier":
                    categories = parse_tariff_categories(resp)
                    results["tariff_categories"] = categories
                    print(f"   ğŸ“‚ è¯†åˆ«åˆ° {len(categories)} ä¸ªå…³ç¨åˆ†ç±»: {categories}")
                else:
                    results[name] = resp
    
    return results


async def run_second_round(
    tariff_categories: List[str],
    tariff_news: List[Dict],
    material_data_section: str,
    ai_config: dict
) -> Dict[str, any]:
    """
    ç¬¬äºŒè½®ï¼šå¹¶è¡Œè¿è¡Œå„å…³ç¨åˆ†ç±»åˆ†æ + åŸææ–™æˆæœ¬åˆ†æ
    
    Returns:
        {
            "tariff_sections": {"ä¸­ç¾": "åˆ†æå†…å®¹", "ä¸­æ¬§": "åˆ†æå†…å®¹"},
            "material_analysis": "æˆæœ¬åˆ†æå†…å®¹"
        }
    """
    results = {
        "tariff_sections": {},
        "material_analysis": ""
    }
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        task_info = []  # (type, name)
        
        # ä¸ºæ¯ä¸ªå…³ç¨åˆ†ç±»åˆ›å»ºåˆ†æä»»åŠ¡
        for category in tariff_categories:
            # ç­›é€‰è¯¥åˆ†ç±»ç›¸å…³çš„æ–°é—»
            category_news = filter_news_by_category(tariff_news, category)
            
            if not category_news:
                print(f"   â­ï¸ åˆ†ç±» [{category}] æ— ç›¸å…³æ–°é—»ï¼Œè·³è¿‡")
                continue
            
            # æ„å»ºæ–°é—»å†…å®¹
            news_content = "\n\n".join([
                f"### {n.get('title', '')}\n**æ¥æº**: {n.get('source', '')} | [é“¾æ¥]({n.get('url', '')})\n**å†…å®¹**: {n.get('content', 'æ— å…¨æ–‡')[:800]}"
                for n in category_news[:5]
            ])
            
            prompt = get_tariff_analysis_prompt(category, news_content)
            tasks.append(call_ai_async(
                session, ai_config["api_base"], ai_config["api_key"], ai_config["model"],
                prompt["system"], prompt["user"], prompt["max_tokens"]
            ))
            task_info.append(("tariff", category))
        
        # åŸææ–™æˆæœ¬åˆ†æä»»åŠ¡
        material_prompt = get_module_prompt(MATERIAL_ANALYSIS_MODULE, material_data=material_data_section)
        tasks.append(call_ai_async(
            session, ai_config["api_base"], ai_config["api_key"], ai_config["model"],
            material_prompt["system"], material_prompt["user"], material_prompt["max_tokens"]
        ))
        task_info.append(("material", "analysis"))
        
        if tasks:
            print(f"ğŸš€ [V4] ç¬¬äºŒè½®ï¼šå¹¶è¡Œè°ƒç”¨ {len(tasks)} ä¸ªæ¨¡å—ï¼ˆ{len(tariff_categories)} ä¸ªå…³ç¨åˆ†ç±» + 1 ä¸ªåŸææ–™åˆ†æï¼‰...")
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            for (task_type, name), resp in zip(task_info, responses):
                if isinstance(resp, Exception):
                    print(f"âš ï¸ {task_type}/{name} å¤±è´¥: {resp}")
                    if task_type == "tariff":
                        results["tariff_sections"][name] = f"*{name} åˆ†æå¤±è´¥*"
                    else:
                        results["material_analysis"] = "*åŸææ–™æˆæœ¬åˆ†æå¤±è´¥*"
                else:
                    print(f"âœ… {task_type}/{name} å®Œæˆ")
                    if task_type == "tariff":
                        results["tariff_sections"][name] = resp
                    else:
                        results["material_analysis"] = resp
    
    return results


async def run_third_round(
    today: str,
    customer_analysis: str,
    competitor_analysis: str,
    tariff_sections: Dict[str, str],
    material_analysis: str,
    ai_config: dict
) -> str:
    """
    ç¬¬ä¸‰è½®ï¼šç”Ÿæˆæ‰§è¡Œæ‘˜è¦
    """
    # æ±‡æ€»å…³ç¨åˆ†æ
    tariff_summary = ""
    if tariff_sections:
        for category, analysis in tariff_sections.items():
            tariff_summary += f"### {category}\n{analysis}\n\n"
    else:
        tariff_summary = "æœ¬å‘¨æš‚æ— é‡å¤§å…³ç¨æ”¿ç­–å˜åŒ–ã€‚"
    
    summary_prompt = get_module_prompt(
        SUMMARY_MODULE,
        today=today,
        customer_analysis=customer_analysis,
        competitor_analysis=competitor_analysis,
        tariff_analysis=tariff_summary,
        material_analysis=material_analysis
    )
    
    async with aiohttp.ClientSession() as session:
        print("ğŸ”„ [V4] ç¬¬ä¸‰è½®ï¼šç”Ÿæˆæ‰§è¡Œæ‘˜è¦...")
        result = await call_ai_async(
            session, ai_config["api_base"], ai_config["api_key"], ai_config["model"],
            summary_prompt["system"], summary_prompt["user"], summary_prompt["max_tokens"], 120
        )
        print("âœ… æ‰§è¡Œæ‘˜è¦å®Œæˆ")
        return result


def fetch_realtime_news(keywords: list, max_news: int = 30) -> list:
    """è·å–å®æ—¶æ–°é—»"""
    import requests as req
    all_news = []
    
    # åŒèŠ±é¡º
    try:
        url = "https://news.10jqka.com.cn/tapp/news/push/stock/?page=1&tag=&track=website&pagesize=50"
        resp = req.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        if resp.status_code == 200:
            data = resp.json()
            for item in data.get("data", {}).get("list", [])[:30]:
                title = item.get("title", "")
                if any(kw in title for kw in keywords):
                    all_news.append({
                        "title": title,
                        "url": item.get("url", ""),
                        "source": "åŒèŠ±é¡º"
                    })
    except Exception as e:
        print(f"âš ï¸ åŒèŠ±é¡ºå¤±è´¥: {e}")
    
    return all_news[:max_news]




def get_price_history() -> Dict[str, List[Dict]]:
    """è·å–ä»·æ ¼å†å²æ•°æ®"""
    try:
        from core.price_history import price_history
        return price_history.get_all_commodities_history(days=395)
    except Exception as e:
        print(f"âš ï¸ è·å–ä»·æ ¼å†å²å¤±è´¥: {e}")
        return {}


def get_today_key() -> str:
    """è·å–å½“å¤©çš„æ—¥æœŸé”®ï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰"""
    return datetime.now().strftime("%Y%m%d")


def get_cache_keys(date_key: str) -> Dict[str, str]:
    """
    è·å–ç¼“å­˜é”®
    
    Args:
        date_key: æ—¥æœŸé”®ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
    
    Returns:
        åŒ…å«çŠ¶æ€é”®å’Œæ•°æ®é”®çš„å­—å…¸
    """
    return {
        "status": f"analysis-v4-status-{date_key}",
        "data": f"analysis-v4-{date_key}"
    }


def check_analysis_status(date_key: str) -> Dict[str, any]:
    """
    æ£€æŸ¥åˆ†æçŠ¶æ€
    
    Args:
        date_key: æ—¥æœŸé”®ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
    
    Returns:
        {
            "status": "none|pending|completed",
            "data": None æˆ–å®Œæ•´çš„åˆ†ææŠ¥å‘Šæ•°æ®
        }
    """
    keys = get_cache_keys(date_key)
    
    # æ£€æŸ¥çŠ¶æ€ç¼“å­˜
    status_cache = cache.get(keys["status"])
    
    if not status_cache:
        # çŠ¶æ€ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ£€æŸ¥æ•°æ®ç¼“å­˜æ˜¯å¦å­˜åœ¨
        data_cache = cache.get(keys["data"])
        if data_cache:
            return {"status": "completed", "data": data_cache}
        return {"status": "none", "data": None}
    
    # çŠ¶æ€ç¼“å­˜å­˜åœ¨
    current_status = status_cache.get("status", "none")
    
    if current_status == "completed":
        # å°è¯•è·å–æ•°æ®ç¼“å­˜
        data_cache = cache.get(keys["data"])
        if data_cache:
            return {"status": "completed", "data": data_cache}
        else:
            # æ•°æ®ç¼“å­˜å·²è¿‡æœŸï¼Œé‡ç½®çŠ¶æ€
            return {"status": "none", "data": None}
    
    return {"status": current_status, "data": None}


def set_analysis_status(date_key: str, status: str, data: Optional[Dict] = None) -> bool:
    """
    è®¾ç½®åˆ†æçŠ¶æ€
    
    Args:
        date_key: æ—¥æœŸé”®ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
        status: çŠ¶æ€ï¼ˆpending|completedï¼‰
        data: åˆ†ææŠ¥å‘Šæ•°æ®ï¼ˆå½“çŠ¶æ€ä¸ºcompletedæ—¶å¿…é¡»æä¾›ï¼‰
    
    Returns:
        æ˜¯å¦è®¾ç½®æˆåŠŸ
    """
    keys = get_cache_keys(date_key)
    
    # è®¾ç½®çŠ¶æ€ç¼“å­˜ï¼ˆ10åˆ†é’Ÿï¼‰
    status_data = {
        "status": status,
        "started_at": datetime.now().isoformat()
    }
    cache.set(keys["status"], status_data, ttl=600)  # 10åˆ†é’Ÿ
    
    # å¦‚æœçŠ¶æ€ä¸ºcompletedï¼Œä¿å­˜æ•°æ®ç¼“å­˜ï¼ˆ36å°æ—¶ï¼‰
    if status == "completed" and data:
        cache.set(keys["data"], data, ttl=129600)  # 36å°æ—¶
        return True
    
    return True


def try_acquire_lock(date_key: str) -> bool:
    """
    å°è¯•è·å–åˆ†å¸ƒå¼é”ï¼ˆä½¿ç”¨Redis SETNXï¼‰
    
    Args:
        date_key: æ—¥æœŸé”®
    
    Returns:
        æ˜¯å¦æˆåŠŸè·å–é”
    """
    keys = get_cache_keys(date_key)
    status_cache = cache.get(keys["status"])
    
    # å¦‚æœçŠ¶æ€ç¼“å­˜ä¸å­˜åœ¨ï¼Œå°è¯•è®¾ç½®ä¸ºpending
    if not status_cache:
        return set_analysis_status(date_key, "pending")
    
    # çŠ¶æ€ç¼“å­˜å·²å­˜åœ¨ï¼Œæ— æ³•è·å–é”
    return False




@router.post("/api/generate-analysis-v4")
async def generate_analysis_v4(request: AnalysisRequest):
    """
    V4 æ¨¡å—åŒ–åˆ†æ APIï¼ˆå‡çº§ç‰ˆï¼‰
    
    æµç¨‹ï¼š
    1. æ•°æ®å‡†å¤‡ï¼ˆæ–°é—»ã€åŸææ–™ï¼‰
    2. ç¬¬ä¸€è½®ï¼šå®¢æˆ·ã€å‹å•†ã€å…³ç¨åˆ†ç±»ï¼ˆå¹¶è¡Œï¼‰
    3. ç¬¬äºŒè½®ï¼šå…³ç¨å„åˆ†ç±»åˆ†æã€åŸææ–™æˆæœ¬åˆ†æï¼ˆå¹¶è¡Œï¼‰
    4. ç¬¬ä¸‰è½®ï¼šæ‰§è¡Œæ‘˜è¦æ•´åˆ
    5. æ‹¼è£…æœ€ç»ˆæŠ¥å‘Š
    
    ç¼“å­˜æœºåˆ¶ï¼š
    - çŠ¶æ€ç¼“å­˜ï¼š10åˆ†é’Ÿï¼ˆé˜²æ­¢é‡å¤è¯·æ±‚ï¼‰
    - æ•°æ®ç¼“å­˜ï¼š36å°æ—¶ï¼ˆæå‡å“åº”é€Ÿåº¦ï¼‰
    """
    # ========== ç¼“å­˜æ£€æŸ¥ ==========
    date_key = get_today_key()
    cache_status = check_analysis_status(date_key)
    
    print(f"ğŸ” [ç¼“å­˜æ£€æŸ¥] æ—¥æœŸ: {date_key}, çŠ¶æ€: {cache_status['status']}, å¼ºåˆ¶åˆ·æ–°: {request.force_refresh}")
    
    # å¼ºåˆ¶åˆ·æ–°ï¼šè·³è¿‡ç¼“å­˜ï¼Œç›´æ¥ç”Ÿæˆæ–°æ•°æ®
    if request.force_refresh:
        print(f"ğŸ”„ [å¼ºåˆ¶åˆ·æ–°] è·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼Œç”Ÿæˆæœ€æ–°æ•°æ®")
        # æ¸…é™¤æ—§çš„çŠ¶æ€ç¼“å­˜ï¼Œé¿å…é”å†²çª
        cache.delete(f"analysis-v4-status-{date_key}")
    else:
        # å¦‚æœçŠ¶æ€ä¸º pendingï¼Œè¿”å›æç¤º
        if cache_status["status"] == "pending":
            return {
                "status": "pending",
                "message": "åˆ†ææŠ¥å‘Šæ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åå†è¯•ï¼ˆçº¦éœ€3-5åˆ†é’Ÿï¼‰",
                "date": date_key,
                "timestamp": datetime.now().isoformat()
            }
        
        # å¦‚æœçŠ¶æ€ä¸º completed ä¸”æœ‰ç¼“å­˜æ•°æ®ï¼Œç›´æ¥è¿”å›
        if cache_status["status"] == "completed" and cache_status["data"]:
            print(f"âœ… [ç¼“å­˜å‘½ä¸­] ç›´æ¥è¿”å›ç¼“å­˜æ•°æ®")
            cached_data = cache_status["data"]
            cached_data["from_cache"] = True
            cached_data["cache_date"] = date_key
            return cached_data
    
    # ========== è·å–åˆ†å¸ƒå¼é” ==========
    if not try_acquire_lock(date_key):
        # é”è·å–å¤±è´¥ï¼Œè¯´æ˜æœ‰å…¶ä»–è¯·æ±‚æ­£åœ¨å¤„ç†
        return {
            "status": "pending",
            "message": "åˆ†ææŠ¥å‘Šæ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨åå†è¯•",
            "date": date_key,
            "timestamp": datetime.now().isoformat()
        }
    
    print(f"ğŸ”’ [å·²è·å–é”] å¼€å§‹ç”Ÿæˆåˆ†ææŠ¥å‘Š")
    
    # ========== AI é…ç½®éªŒè¯ ==========
    ai_config = get_ai_config()
    if request.model:
        ai_config["model"] = request.model.strip()
    if not ai_config["api_key"]:
        # é‡Šæ”¾é”ï¼ˆåˆ é™¤çŠ¶æ€ç¼“å­˜ï¼‰
        cache.delete(f"analysis-v4-status-{date_key}")
        raise HTTPException(status_code=400, detail="æœªé…ç½® AI API Key")
    
    print("=" * 60)
    print("ğŸ“¡ [V4] å¼€å§‹æ¨¡å—åŒ–åˆ†æï¼ˆåŠ¨æ€å…³ç¨åˆ†ç±» + åŸææ–™åˆ†ç¦»åˆ†æï¼‰")
    print("=" * 60)
    
    # ========== æ•°æ®å‡†å¤‡ ==========
    print("\nğŸ“¦ [æ•°æ®å‡†å¤‡]")
    
    # è·å–æ–°é—»
    keywords = ["ç«‹è®¯", "è‹¹æœ", "åä¸º", "å…³ç¨", "è´¸æ˜“", "ä¸­ç¾", "ä¸­æ¬§", "è¶Šå—", "å°åº¦", "é“œ", "å¡‘æ–™", "ABS"]
    # å¼‚æ­¥æ‰§è¡ŒåŒæ­¥çš„çˆ¬è™«å‡½æ•°
    realtime_news = await run_in_threadpool(fetch_realtime_news, keywords)
    all_news = list(request.news) if request.news else []
    all_news.extend(realtime_news)
    
    # ä»ç¼“å­˜è·å–
    cached = cache.get("news:supply-chain")
    if cached:
        all_news.extend(cached.get("data", []))
    
    # å»é‡
    seen = set()
    unique_news = [n for n in all_news if n.get("title") and n.get("title") not in seen and not seen.add(n.get("title"))]
    print(f"   ğŸ“° æ–°é—»æ€»æ•°: {len(unique_news)} æ¡")
    
    # æ–°é—»è´¨é‡é¢„æ£€
    quality = await run_in_threadpool(precheck_news_quality, unique_news)
    print(f"   ğŸ“Š æ–°é—»è´¨é‡: {quality['quality_score']}/100")
    if quality['suggestions']:
        print(f"   ğŸ’¡ å»ºè®®: {', '.join(quality['suggestions'])}")
    
    # ç­›é€‰å…³ç¨æ–°é—»å¹¶è·å–å…¨æ–‡
    tariff_news = await run_in_threadpool(filter_tariff_news, unique_news)
    print(f"   ğŸŒ å…³ç¨ç›¸å…³: {len(tariff_news)} æ¡")
    
    if tariff_news:
        print("   ğŸ“„ è·å–æ–°é—»å…¨æ–‡...")
        tariff_news = await run_in_threadpool(fetch_news_full_content, tariff_news, max_items=20)
        content_count = len([n for n in tariff_news if n.get('content')])
        print(f"   âœ… æˆåŠŸè·å– {content_count} æ¡å…¨æ–‡")
    
    # è·å–åŸææ–™æ•°æ®ï¼ˆä» MySQL è¯»å–æ ‡å‡†åŒ–æ•°æ®ï¼‰
    commodity_data = []
    try:
        from scrapers.commodity import CommodityScraper
        from database.manager import db_manager
        
        # 1. çˆ¬å–æœ€æ–°æ•°æ®å¹¶å…¥åº“
        scraper = CommodityScraper()
        raw_data = await run_in_threadpool(scraper.scrape)
        print(f"   ğŸ“ˆ åŸææ–™çˆ¬å–: {len(raw_data)} æ¡")
        
        # 2. å†™å…¥ MySQLï¼ˆå»é‡å’Œæ ‡å‡†åŒ–ï¼‰
        try:
            stats_by_source = {}
            sources = set(item.get("source", "unknown") for item in raw_data)
            for src in sources:
                src_records = [item for item in raw_data if item.get("source", "unknown") == src]
                if not src_records:
                    continue
                db_stats = db_manager.write_commodity(src_records, source=src)
                if db_stats:
                    stats_by_source[src] = db_stats
            if stats_by_source:
                print(f"   âœ… MySQL å…¥åº“: {stats_by_source}")
        except Exception as e:
            print(f"   âš ï¸ MySQL å…¥åº“å¤±è´¥: {e}")
        
        # 3. ä» MySQL è¯»å–æ ‡å‡†åŒ–æ•°æ®ï¼ˆä»¥ MySQL ä¸ºå‡†ï¼‰
        try:
            commodity_data = db_manager.get_commodity_latest()
            if not commodity_data:
                print("   âš ï¸ MySQL commodity_latest ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                commodity_data = raw_data
            else:
                print(f"   âœ… ä» MySQL è¯»å–: {len(commodity_data)} æ¡æ ‡å‡†åŒ–æ•°æ®")
                # å­—æ®µæ˜ å°„ï¼šMySQL â†’ API æ ¼å¼
                for item in commodity_data:
                    # 1. åˆå¹¶ unit
                    price_unit = item.get('price_unit', '')
                    weight_unit = item.get('weight_unit', '')
                    if price_unit and weight_unit:
                        item['unit'] = f"{price_unit}/{weight_unit}"
                    else:
                        item['unit'] = price_unit or weight_unit or 'USD'
                    
                    # 2. current_price
                    if 'price' in item and 'current_price' not in item:
                        item['current_price'] = item['price']
                    
                    # 3. url
                    if 'url' not in item or not item['url']:
                        item['url'] = item.get('source_url', '')
                    
                    # 4. cleanup
                    for k in ['id', 'price_unit', 'weight_unit', 'version_ts', 'source_url']:
                        item.pop(k, None)
        except Exception as e:
            print(f"   âš ï¸ ä» MySQL è¯»å–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
            commodity_data = raw_data
    except Exception as e:
        print(f"   âš ï¸ åŸææ–™è·å–å¤±è´¥: {e}")
    
    # è·å–ä»·æ ¼å†å²
    price_history_data = await run_in_threadpool(get_price_history)
    print(f"   ğŸ“œ å†å²æ•°æ®: {len(price_history_data)} ä¸ªå“ç§")
    
    # æ„å»ºæ–°é—»æ‘˜è¦
    news_summary = "\n".join([
        f"- [{n.get('title', '')}]({n.get('url', '')}) ã€{n.get('source', '')}ã€‘"
        for n in unique_news[:50]
    ])
    
    # æ„å»ºå…³ç¨æ–°é—»å…¨æ–‡
    news_with_content = "\n\n".join([
        f"### {n.get('title', '')}\n**æ¥æº**: {n.get('source', '')} | [é“¾æ¥]({n.get('url', '')})\n**å†…å®¹**: {n.get('content', 'æ— å…¨æ–‡')[:800]}"
        for n in tariff_news[:15]
    ]) if tariff_news else "æš‚æ— å…³ç¨ç›¸å…³æ–°é—»"
    
    # æ„å»ºåŸææ–™æ•°æ®éƒ¨åˆ†ï¼ˆä¸èµ°å¤§æ¨¡å‹ï¼‰
    material_data_section = await run_in_threadpool(build_material_section, commodity_data, price_history_data)
    
    today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    
    try:
        # ========== ç¬¬ä¸€è½® ==========
        print("\nğŸ”„ [ç¬¬ä¸€è½®] å®¢æˆ·ã€å‹å•†ã€å…³ç¨åˆ†ç±»")
        first_round = await run_first_round(news_summary, news_with_content, ai_config)
        
        tariff_categories = first_round.get("tariff_categories", [])
        
        # ========== ç¬¬äºŒè½® ==========
        print(f"\nğŸ”„ [ç¬¬äºŒè½®] å…³ç¨åˆ†ç±»åˆ†æ({len(tariff_categories)}ä¸ª) + åŸææ–™æˆæœ¬åˆ†æ")
        second_round = await run_second_round(
            tariff_categories,
            tariff_news,
            material_data_section,
            ai_config
        )
        
        # ========== ç¬¬ä¸‰è½® ==========
        print("\nğŸ”„ [ç¬¬ä¸‰è½®] æ‰§è¡Œæ‘˜è¦æ•´åˆ")
        summary = await run_third_round(
            today,
            first_round.get("customer", ""),
            first_round.get("competitor", ""),
            second_round.get("tariff_sections", {}),
            second_round.get("material_analysis", ""),
            ai_config
        )
        
        # ========== æ‹¼è£…æŠ¥å‘Š ==========
        print("\nğŸ“ [æ‹¼è£…] ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        final_report = assemble_final_report_v4(
            summary,
            first_round.get("customer", ""),
            first_round.get("competitor", ""),
            material_data_section,
            second_round.get("material_analysis", ""),
            second_round.get("tariff_sections", {}),
            today
        )
        
        print("\n" + "=" * 60)
        print("âœ… [V4] åˆ†æå®Œæˆ!")
        print("=" * 60)
        
        # ========== æ„å»ºè¿”å›æ•°æ® ==========
        result_data = {
            "status": "success",
            "content": final_report,
            "model": ai_config["model"],
            "version": "V4-modular-upgraded",
            "stats": {
                "news_count": len(unique_news),
                "tariff_news_count": len(tariff_news),
                "tariff_categories": tariff_categories,
                "commodity_count": len(commodity_data),
                "news_quality_score": quality["quality_score"]
            },
            "modules_completed": {
                "first_round": ["customer", "competitor", "tariff_classifier"],
                "second_round": list(second_round.get("tariff_sections", {}).keys()) + ["material_analysis"],
                "third_round": ["summary"]
            },
            "timestamp": datetime.now().isoformat(),
            "from_cache": False,
            "cache_date": date_key
        }
        
        # ========== ä¿å­˜åˆ°ç¼“å­˜ ==========
        print(f"ğŸ’¾ [ä¿å­˜ç¼“å­˜] æ—¥æœŸ: {date_key}")
        set_analysis_status(date_key, "completed", result_data)
        
        return result_data
        
    except Exception as e:
        # ========== å¼‚å¸¸å¤„ç†ï¼šæ¸…é™¤çŠ¶æ€é” ==========
        print(f"âŒ [å¼‚å¸¸] æ¸…é™¤çŠ¶æ€é”: {date_key}")
        cache.delete(f"analysis-v4-status-{date_key}")
        
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


async def generate_analysis_task():
    """
    å®šæ—¶ä»»åŠ¡ä¸“ç”¨ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Šå¹¶ç¼“å­˜
    
    ä¸æ¥å£ç‰ˆæœ¬çš„åŒºåˆ«ï¼š
    - ä¸éœ€è¦ Request å‚æ•°
    - ä½¿ç”¨é»˜è®¤é…ç½®
    - å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼ˆforce_refresh=Trueï¼‰
    - ä¸“æ³¨äºç¼“å­˜ç”Ÿæˆï¼Œä¸è¿”å›å…·ä½“æ•°æ®
    """
    try:
        # æ„é€ é»˜è®¤è¯·æ±‚ï¼Œforce_refresh=True ç¡®ä¿æ€»æ˜¯ç”Ÿæˆæœ€æ–°æ•°æ®
        request = AnalysisRequest(news=[], model=None, force_refresh=True)
        
        # è°ƒç”¨ç”Ÿæˆå‡½æ•°
        result = await generate_analysis_v4(request)
        
        print(f"âœ… [å®šæ—¶ä»»åŠ¡] Analysis V4 æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        return result
    except Exception as e:
        print(f"âŒ [å®šæ—¶ä»»åŠ¡] Analysis V4 æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise


@router.get("/api/analysis-v4/status")
async def get_v4_status():
    """è·å– V4 API çŠ¶æ€"""
    ai_config = get_ai_config()
    return {
        "version": "V4-upgraded",
        "features": [
            "åŠ¨æ€å…³ç¨åˆ†ç±»ï¼ˆAIè‡ªåŠ¨è¯†åˆ«å›½å®¶/åœ°åŒºç»„åˆï¼‰",
            "å…³ç¨åˆ†ç±»å•ç‹¬åˆ†æï¼ˆæ¯ä¸ªåˆ†ç±»ç‹¬ç«‹è°ƒç”¨ï¼‰",
            "åŸææ–™æ•°æ®ç›´æ¥åµŒå…¥ï¼ˆä¸èµ°å¤§æ¨¡å‹ï¼‰",
            "åŸææ–™æˆæœ¬åˆ†æï¼ˆå•ç‹¬èµ°å¤§æ¨¡å‹ï¼‰",
            "ä¸‰è½®æ¨¡å—åŒ–è°ƒç”¨æ¶æ„",
            "å®šæ—¶ä»»åŠ¡è‡ªåŠ¨ç¼“å­˜ï¼ˆæ¯4å°æ—¶åˆ·æ–°ï¼‰"
        ],
        "model": ai_config.get("model", "æœªé…ç½®"),
        "api_configured": bool(ai_config.get("api_key"))
    }
