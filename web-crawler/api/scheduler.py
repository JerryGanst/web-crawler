# coding=utf-8
"""
åå°å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨

åŠŸèƒ½ï¼š
1. æœåŠ¡å¯åŠ¨æ—¶è‡ªåŠ¨é¢„çƒ­ç¼“å­˜
2. å®šæœŸåˆ·æ–°å„åˆ†ç±»æ•°æ®
3. é¿å…ç”¨æˆ·ç­‰å¾…çˆ¬è™«

è°ƒåº¦ç­–ç•¥ï¼š
- å¯åŠ¨æ—¶ï¼šé¢„çƒ­æ‰€æœ‰åˆ†ç±»ç¼“å­˜
- æ¯ 30 åˆ†é’Ÿï¼šåˆ·æ–°è´¢ç»ã€ç§‘æŠ€ã€æ–°é—»
- æ¯ 15 åˆ†é’Ÿï¼šåˆ·æ–°å¤§å®—å•†å“æ•°æ®
- æ¯ 10 åˆ†é’Ÿï¼šåˆ·æ–°ä¾›åº”é“¾ã€å…³ç¨æ–°é—»
"""

import asyncio
import os
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Callable, Dict, List

from .cache import cache, CACHE_TTL


class BackgroundScheduler:
    """åå°ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self):
        self._executor = ThreadPoolExecutor(max_workers=5, thread_name_prefix="scheduler")
        self._running = False
        self._tasks: Dict[str, dict] = {}
        self._test_env = "PYTEST_CURRENT_TEST" in os.environ
    
    def _crawl_category(self, category: str, include_custom: bool = True):
        """çˆ¬å–æŒ‡å®šåˆ†ç±»"""
        try:
            from scrapers.unified import UnifiedDataSource
            
            print(f"â° [å®šæ—¶] å¼€å§‹çˆ¬å– {category}...")
            unified = UnifiedDataSource()
            data = unified.crawl_category(category, include_custom=include_custom)
            
            # 1. å†™å…¥ MongoDB
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled:
                    # å°†æ™®é€šå­—å…¸è½¬æ¢ä¸º News å¯¹è±¡
                    from database.models import News
                    news_objects = []
                    for item in data:
                        # å¤„ç†æ—¶é—´
                        p_time = item.get("time")
                        published_at = None
                        if p_time:
                            try:
                                if isinstance(p_time, str):
                                    published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                                else:
                                    published_at = p_time
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()

                        news_objects.append(News(
                            platform_id=item.get("platform", "unknown"),
                            title=item.get("title", ""),
                            url=item.get("url", ""),
                            published_at=published_at,
                            category=category,
                            extra_data=item,
                            source=item.get("source", ""),
                            platform_name=item.get("platform_name") or item.get("source", ""),
                            summary=item.get("summary", "") or item.get("content", "")[:200]
                        ))
                    
                    inserted, updated = db_manager.news_repo.insert_batch(news_objects)
                    print(f"âœ… [å®šæ—¶] {category} å½’æ¡£åˆ° MongoDB: æ–°å¢ {inserted}, æ›´æ–° {updated}")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å½’æ¡£å¤±è´¥: {e}")

            result = {
                "status": "success",
                "category": category,
                "data": data,
                "timestamp": datetime.now().isoformat(),
                "total": len(data),
                "cached": False,
                "scheduled_refresh": True
            }
            
            # é»˜è®¤å…ˆè®¾ç½®å½“å‰æŠ“å–ç»“æœä¸ºç¼“å­˜
            final_result = result
            cache_key = f"news:{category}"

            # å°è¯•ä» MongoDB è·å–æœ€è¿‘ 7 å¤©çš„å…¨é‡æ•°æ®
            # åªè¦å¯ç”¨äº† MongoDBï¼Œå°±å°è¯•åˆå¹¶å†å²æ•°æ®ï¼Œç¡®ä¿å±•ç¤ºå®Œæ•´
            from api.routes.news import _try_get_from_mongodb_daily
            daily_data = _try_get_from_mongodb_daily(category)
            
            if daily_data and daily_data.get("total", 0) > len(data):
                print(f"ğŸ”„ [å®šæ—¶] {category} ä½¿ç”¨ MongoDB æœ€è¿‘7å¤©å…¨é‡æ•°æ®æ›´æ–°ç¼“å­˜ ({daily_data.get('total')} æ¡)")
                final_result = daily_data
            
            # 2. å†™å…¥ MongoDB å¿«ç…§ (ä½œä¸º Redis çš„æŒä¹…åŒ–å¤‡ä»½)
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled:
                    # å¿«ç…§ä¹Ÿä½¿ç”¨ final_result (å…¨é‡æ•°æ®)
                    db_manager.news_repo.save_snapshot(cache_key, final_result)
                    print(f"âœ… [å®šæ—¶] {category} å¿«ç…§å·²ä¿å­˜åˆ° MongoDB (åŒ…å« {final_result.get('total', 0)} æ¡)")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å¿«ç…§ä¿å­˜å¤±è´¥: {e}")
            
            # 3. å†™å…¥ Redis
            cache.set(cache_key, final_result, ttl=CACHE_TTL)
                
            print(f"â° [å®šæ—¶] {category} å®Œæˆ: {len(data)} æ¡ (æœ€ç»ˆç¼“å­˜: {final_result.get('total', 0)} æ¡)")
        except Exception as e:
            print(f"â° [å®šæ—¶] {category} å¤±è´¥: {e}")
    
    def _crawl_commodity_data(self):
        """çˆ¬å–å¤§å®—å•†å“æ•°æ®"""
        try:
            from scrapers.commodity import CommodityScraper
            from database.manager import db_manager
            
            print(f"â° [å®šæ—¶] å¼€å§‹çˆ¬å–å¤§å®—å•†å“æ•°æ®...")
            scraper = CommodityScraper()
            raw_data = scraper.scrape()
            print(f"âœ… [Scheduler] Scraped {len(raw_data)} commodity items")
            
            # 1. å†™å…¥ MySQLï¼ˆPipeline ä¼šè‡ªåŠ¨å»é‡ï¼‰ï¼ŒæŒ‰æ¥æºåˆ†ç»„
            try:
                stats_by_source = {}
                sources = set(item.get("source", "unknown") for item in raw_data)
                for src in sources:
                    src_records = [item for item in raw_data if item.get("source", "unknown") == src]
                    if not src_records:
                        continue
                    db_stats = db_manager.write_commodity(src_records, source=src)
                    if db_stats:
                        stats_by_source[src] = db_stats
                if stats_by_source:
                    print(f"âœ… [å®šæ—¶] MySQL å…¥åº“å®Œæˆ: {stats_by_source}")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MySQL å…¥åº“å¤±è´¥: {e}")

            # 2. ä» MySQL commodity_latest è¯»å–å»é‡åçš„æ•°æ®ï¼ˆä»¥ MySQL ä¸ºå‡†ï¼‰
            try:
                latest_data = db_manager.get_commodity_latest()
                if not latest_data:
                    print("âš ï¸ [å®šæ—¶] MySQL commodity_latest ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                    latest_data = raw_data
                else:
                    print(f"âœ… [å®šæ—¶] ä» MySQL è¯»å–: {len(latest_data)} æ¡å»é‡æ•°æ®")
                    # å­—æ®µæ˜ å°„ï¼šMySQL â†’ API æ ¼å¼ (å†…è”å®ç°ï¼Œé¿å…å¾ªç¯ä¾èµ–)
                    for item in latest_data:
                        # 1. åˆå¹¶ unit
                        price_unit = item.get('price_unit', '')
                        weight_unit = item.get('weight_unit', '')
                        if price_unit and weight_unit:
                            item['unit'] = f"{price_unit}/{weight_unit}"
                        else:
                            item['unit'] = price_unit or weight_unit or 'USD'
                        
                        # 2. current_price
                        if 'price' in item and 'current_price' not in item:
                            item['current_price'] = item['price']
                        
                        # 3. url
                        if 'url' not in item or not item['url']:
                            item['url'] = item.get('source_url', '')

                        # 4. cleanup
                        for k in ['id', 'price_unit', 'weight_unit', 'version_ts', 'source_url']:
                            item.pop(k, None)

            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] ä» MySQL è¯»å–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                latest_data = raw_data
            
            # 3. æ’åº
            category_order = {'è´µé‡‘å±': 0, 'èƒ½æº': 1, 'å·¥ä¸šé‡‘å±': 2, 'å†œäº§å“': 3, 'å…¶ä»–': 4}
            latest_data.sort(key=lambda x: category_order.get(x.get('category', 'å…¶ä»–'), 4))
            
            # 4. å†™å…¥ Redis
            result = {
                "data": latest_data,
                "source": "TrendRadar Commodity",
                "timestamp": datetime.now().isoformat(),
                "cached": False,
                "scheduled_refresh": True,
                "from_mysql": True,
                "categories": list(set(item.get('category', 'å…¶ä»–') for item in latest_data)),
                "total": len(latest_data)
            }
            cache.set("data:commodity", result, ttl=CACHE_TTL)
            
            print(f"â° [å®šæ—¶] å¤§å®—å•†å“æ•°æ®å®Œæˆ: {len(latest_data)} æ¡")
        except Exception as e:
            print(f"â° [å®šæ—¶] å¤§å®—å•†å“æ•°æ®å¤±è´¥: {e}")
    
    def _fetch_realtime_news(self, cache_key: str, keywords: list, category: str = None):
        """æŠ“å–å®æ—¶æ–°é—»"""
        try:
            from api.routes.analysis import fetch_realtime_news
            from api.routes.news import _fetch_power_partner_news, _fetch_power_official_announcements
            
            print(f"â° [å®šæ—¶] å¼€å§‹æŠ“å– {cache_key}...")
            news = fetch_realtime_news(keywords)
            if category == "supply-chain":
                power_news = _fetch_power_partner_news()
                official_news = _fetch_power_official_announcements()
                seen = {n.get("title") for n in news}
                for item in power_news + official_news:
                    if item.get("title") and item["title"] not in seen:
                        seen.add(item["title"])
                        news.append(item)
            
            # è¡¥å…¨ platform_name (ç¡®ä¿å…¥åº“å’Œç¼“å­˜éƒ½æœ‰è¯¥å­—æ®µ)
            for item in news:
                if not item.get("platform_name") and item.get("source"):
                    item["platform_name"] = item["source"]
            
            # ç»Ÿè®¡æ•°æ®æ¥æºåˆ†å¸ƒ
            sources = {}
            for item in news:
                # æŒ‰ç…§ä¼˜å…ˆçº§æå–æ¥æºï¼šplatform_name > source > platform > æœªçŸ¥
                # ä¿®å¤é€»è¾‘ï¼šä¼˜å…ˆå– platform_name æˆ– sourceï¼Œé¿å…å–åˆ° newsnow
                source_name = item.get('platform_name') or item.get('source') or item.get('platform') or 'æœªçŸ¥'
                sources[source_name] = sources.get(source_name, 0) + 1
            
            # 1. å†™å…¥ MongoDB
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled and category:
                    from database.models import News
                    news_objects = []
                    for item in news:
                        # å¤„ç†æ—¶é—´
                        p_time = item.get("time")
                        published_at = None
                        if p_time:
                            try:
                                if isinstance(p_time, str):
                                    published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                                else:
                                    published_at = p_time
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()

                        news_objects.append(News(
                            platform_id=item.get("platform", "unknown"),
                            title=item.get("title", ""),
                            url=item.get("url", ""),
                            # source removed
                            published_at=published_at,
                            category=category,
                            extra_data=item,
                            source=item.get("source", ""),
                            platform_name=item.get("platform_name", ""),
                            summary=item.get("summary", "") or item.get("content", "")[:200]
                        ))
                    
                    inserted, updated = db_manager.news_repo.insert_batch(news_objects)
                    print(f"âœ… [å®šæ—¶] {category} (å®æ—¶) å½’æ¡£åˆ° MongoDB: æ–°å¢ {inserted}, æ›´æ–° {updated}")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å½’æ¡£å¤±è´¥: {e}")
            
            result = {
                "status": "success",
                "data": news,
                "timestamp": datetime.now().isoformat(),
                "total": len(news),
                "sources": sources,  # æ·»åŠ  sources ç»Ÿè®¡
                "cached": False,
                "scheduled_refresh": True
            }

            # å°è¯•ä» MongoDB è·å–æœ€è¿‘ 7 å¤©çš„å…¨é‡æ•°æ®
            # åªè¦å¯ç”¨äº† MongoDBï¼Œå°±å°è¯•åˆå¹¶å†å²æ•°æ®ï¼Œç¡®ä¿å±•ç¤ºå®Œæ•´
            from api.routes.news import _try_get_from_mongodb_daily
            daily_data = _try_get_from_mongodb_daily(category, keywords) # keywords å¯èƒ½ä¸ºç©ºï¼Œä½†åœ¨ _try_get_from_mongodb_daily å†…éƒ¨å¤„ç†äº†
            
            if daily_data and daily_data.get("total", 0) > len(news):
                print(f"ğŸ”„ [å®šæ—¶] {cache_key} ä½¿ç”¨ MongoDB æœ€è¿‘7å¤©å…¨é‡æ•°æ®æ›´æ–°ç¼“å­˜ ({daily_data.get('total')} æ¡)")
                final_result = daily_data
            
            # 2. å†™å…¥ MongoDB å¿«ç…§ (ä½œä¸º Redis çš„æŒä¹…åŒ–å¤‡ä»½)
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled:
                    # å¿«ç…§ä¹Ÿä½¿ç”¨ final_result (å…¨é‡æ•°æ®)
                    db_manager.news_repo.save_snapshot(cache_key, final_result)
                    print(f"âœ… [å®šæ—¶] {cache_key} å¿«ç…§å·²ä¿å­˜åˆ° MongoDB (åŒ…å« {final_result.get('total', 0)} æ¡)")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å¿«ç…§ä¿å­˜å¤±è´¥: {e}")

            # 3. å†™å…¥ Redis
            cache.set(cache_key, final_result, ttl=CACHE_TTL)
            
            print(f"â° [å®šæ—¶] {cache_key} å®Œæˆ: {len(news)} æ¡ (æœ€ç»ˆç¼“å­˜: {final_result.get('total', 0)} æ¡)")
        except Exception as e:
            print(f"â° [å®šæ—¶] {cache_key} å¤±è´¥: {e}")
    
    def _generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå®šæ—¶ä»»åŠ¡ä¸“ç”¨ï¼‰"""
        try:
            import asyncio
            from api.routes.analysis_v4 import generate_analysis_task
            
            print("â° [å®šæ—¶] å¼€å§‹ç”Ÿæˆ analysis-v4 æŠ¥å‘Š...")
            
            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(generate_analysis_task())
                print("âœ… [å®šæ—¶] analysis-v4 æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            finally:
                loop.close()
        except Exception as e:
            print(f"âŒ [å®šæ—¶] analysis-v4 æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    def _get_week_key(self) -> str:
        """è·å–å½“å‰å‘¨é”®ï¼ˆæ ¼å¼ï¼š2026W01ï¼‰"""
        from datetime import datetime
        now = datetime.now()
        year = now.year
        week = now.isocalendar()[1]
        return f"{year}W{week:02d}"
    
    def _get_push_status(self, week_key: str) -> dict:
        """
        è·å–æ¨é€çŠ¶æ€
        
        Args:
            week_key: å‘¨é”®ï¼ˆå¦‚ï¼š2026W01ï¼‰
        
        Returns:
            {"status": "success|failed|pending", "retry_count": 0, "last_attempt": "..."}
        """
        status_key = f"weekly-push-status-{week_key}"
        status = cache.get(status_key)
        
        if not status:
            return {"status": "none", "retry_count": 0, "last_attempt": None}
        
        return status
    
    def _set_push_status(self, week_key: str, status: str, retry_count: int = 0):
        """
        è®¾ç½®æ¨é€çŠ¶æ€
        
        Args:
            week_key: å‘¨é”®
            status: æ¨é€çŠ¶æ€ï¼ˆsuccess|failed|pendingï¼‰
            retry_count: é‡è¯•æ¬¡æ•°
        """
        status_key = f"weekly-push-status-{week_key}"
        status_data = {
            "status": status,
            "retry_count": retry_count,
            "last_attempt": datetime.now().isoformat()
        }
        # TTL=7å¤©
        cache.set(status_key, status_data, ttl=7 * 24 * 3600)
        print(f"ğŸ’¾ [æ¨é€çŠ¶æ€] {week_key}: {status} (é‡è¯•:{retry_count})")
    
    def _weekly_push_report(self):
        """æ¯å‘¨ä¸€å®šæ—¶æ¨é€åˆ†ææŠ¥å‘Šåˆ°ä¼å¾®"""
        try:
            import asyncio
            from api.routes.analysis_v4 import get_today_key, check_analysis_status, generate_analysis_task
            from api.routes.reports import push_report_internal
            
            week_key = self._get_week_key()
            push_status = self._get_push_status(week_key)
            
            # æ£€æŸ¥æ¨é€çŠ¶æ€
            if push_status["status"] == "success":
                print(f"âœ… [å‘¨æ¨é€] {week_key} å·²æ¨é€æˆåŠŸï¼Œè·³è¿‡")
                return
            
            if push_status["retry_count"] >= 3:
                print(f"âš ï¸ [å‘¨æ¨é€] {week_key} é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼ˆ3æ¬¡ï¼‰ï¼Œè·³è¿‡")
                return
            
            print(f"ğŸ“¤ [å‘¨æ¨é€] å¼€å§‹æ¨é€ {week_key} æŠ¥å‘Šï¼ˆé‡è¯•: {push_status['retry_count']}/3ï¼‰")
            
            # 1. è·å–æœ€æ–°æŠ¥å‘Š
            date_key = get_today_key()
            cache_status = check_analysis_status(date_key)
            
            content = None
            if cache_status["status"] == "completed" and cache_status["data"]:
                content = cache_status["data"].get("content", "")
                print(f"âœ… [å‘¨æ¨é€] ä½¿ç”¨ç¼“å­˜æŠ¥å‘Šï¼ˆ{date_key}ï¼‰")
            else:
                # ç¼“å­˜å¤±æ•ˆï¼Œå®æ—¶ç”Ÿæˆ
                print(f"â³ [å‘¨æ¨é€] ç¼“å­˜å¤±æ•ˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(generate_analysis_task())
                    content = result.get("content", "") if isinstance(result, dict) else ""
                    print(f"âœ… [å‘¨æ¨é€] æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
                finally:
                    loop.close()
            
            # 2. è°ƒç”¨æ¨é€å‡½æ•°
            if not content:
                print(f"âŒ [å‘¨æ¨é€] æŠ¥å‘Šå†…å®¹ä¸ºç©ºï¼Œè·³è¿‡æ¨é€")
                self._set_push_status(week_key, "failed", push_status["retry_count"] + 1)
                return
            
            print(f"ğŸ“¨ [å‘¨æ¨é€] å¼€å§‹æ¨é€åˆ°ä¼å¾®...")
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(push_report_internal(
                    title="ç«‹è®¯æŠ€æœ¯äº§ä¸šé“¾åˆ†ææŠ¥å‘Š",
                    content=content
                ))
                
                if result.get("status") in ["success", "partial"]:
                    print(f"âœ… [å‘¨æ¨é€] æ¨é€æˆåŠŸ: {result.get('message')}")
                    self._set_push_status(week_key, "success", 0)
                else:
                    print(f"âŒ [å‘¨æ¨é€] æ¨é€å¤±è´¥: {result.get('message')}")
                    self._set_push_status(week_key, "failed", push_status["retry_count"] + 1)
            finally:
                loop.close()
                
        except Exception as e:
            import traceback
            print(f"âŒ [å‘¨æ¨é€] æ‰§è¡Œå¤±è´¥: {e}")
            traceback.print_exc()
            
            # æ›´æ–°å¤±è´¥çŠ¶æ€
            week_key = self._get_week_key()
            push_status = self._get_push_status(week_key)
            self._set_push_status(week_key, "failed", push_status["retry_count"] + 1)

    
    def warmup_cache(self):
        """é¢„çƒ­ç¼“å­˜ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
        if self._test_env:
            print("ğŸ”¥ è·³è¿‡æµ‹è¯•ç¯å¢ƒä¸‹çš„é¢„çƒ­ä»»åŠ¡")
            return
        print("ğŸ”¥ å¼€å§‹é¢„çƒ­ç¼“å­˜...")
        
        # ä» news.py å¯¼å…¥ç»Ÿä¸€çš„å…³é”®è¯é…ç½®
        from .routes.news import SUPPLY_CHAIN_KEYWORDS, TARIFF_KEYWORDS, PLASTICS_KEYWORDS
        supply_chain_keywords = SUPPLY_CHAIN_KEYWORDS
        tariff_keywords = TARIFF_KEYWORDS
        plastics_keywords = PLASTICS_KEYWORDS
        
        # é¢„çƒ­ä»»åŠ¡åˆ—è¡¨
        warmup_tasks = [
            ("å¤§å®—å•†å“æ•°æ®", self._crawl_commodity_data),
            ("è´¢ç»æ–°é—»", lambda: self._crawl_category("finance")),
            ("ç§‘æŠ€æ–°é—»", lambda: self._crawl_category("tech")),
            ("ä¾›åº”é“¾æ–°é—»", lambda: self._fetch_realtime_news("news:supply-chain", supply_chain_keywords, "supply-chain")),
            ("å…³ç¨æ–°é—»", lambda: self._fetch_realtime_news("news:tariff", tariff_keywords, "tariff")),
            ("å¡‘æ–™æ–°é—»", lambda: self._fetch_realtime_news("news:plastics", plastics_keywords, "plastics")),
            ("å¤§å®—å•†å“æ–°é—»", lambda: self._crawl_category("commodity")),
            ("åˆ†ææŠ¥å‘ŠV4", self._generate_analysis_report),
        ]
        
        for name, task in warmup_tasks:
            try:
                self._executor.submit(task)
                print(f"  ğŸ“Œ å·²æäº¤: {name}")
            except Exception as e:
                print(f"  âŒ æäº¤å¤±è´¥ {name}: {e}")
        
        print("ğŸ”¥ é¢„çƒ­ä»»åŠ¡å·²å…¨éƒ¨æäº¤")
    
    def start_scheduled_tasks(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼ˆéé˜»å¡ï¼‰"""
        if self._running:
            return
        
        self._running = True
        if self._test_env:
            # æµ‹è¯•ç¯å¢ƒä¸å¯åŠ¨åå°å¾ªç¯ï¼Œé¿å…å¹²æ‰°å…¶å®ƒç”¨ä¾‹çš„ mock
            return
        
        def scheduler_loop():
            """è°ƒåº¦å¾ªç¯"""
            # ä» news.py å¯¼å…¥ç»Ÿä¸€çš„å…³é”®è¯é…ç½®
            from .routes.news import SUPPLY_CHAIN_KEYWORDS, TARIFF_KEYWORDS
            supply_chain_keywords = SUPPLY_CHAIN_KEYWORDS
            tariff_keywords = TARIFF_KEYWORDS
            
            # ä»»åŠ¡é…ç½®ï¼š(é—´éš”ç§’æ•°, ä¸Šæ¬¡æ‰§è¡Œæ—¶é—´, ä»»åŠ¡å‡½æ•°)
            tasks = {
                "commodity_data": {
                    "interval": 15 * 60,  # 15åˆ†é’Ÿ
                    "last_run": 0,
                    "func": self._crawl_commodity_data
                },
                "finance_news": {
                    "interval": 30 * 60,  # 30åˆ†é’Ÿ
                    "last_run": 0,
                    "func": lambda: self._crawl_category("finance")
                },
                "tech_news": {
                    "interval": 30 * 60,
                    "last_run": 0,
                    "func": lambda: self._crawl_category("tech")
                },
                "supply_chain": {
                    "interval": 10 * 60,  # 10åˆ†é’Ÿ
                    "last_run": 0,
                    "func": lambda: self._fetch_realtime_news("news:supply-chain", supply_chain_keywords, "supply-chain")
                },
                "tariff": {
                    "interval": 10 * 60,
                    "last_run": 0,
                    "func": lambda: self._fetch_realtime_news("news:tariff", tariff_keywords, "tariff")
                },
                "analysis_v4": {
                    "interval": 4 * 60 * 60,  # 4å°æ—¶
                    "last_run": 0,
                    "func": self._generate_analysis_report
                },
                # ========== å‘¨æ¨é€ä»»åŠ¡é…ç½® ==========
                # ã€å½“å‰æ¨¡å¼ï¼šæ­£å¼æ¨¡å¼ - æ¯å‘¨ä¸€æ—©ä¸Š8ç‚¹æ¨é€ã€‘
                "weekly_report_push": {
                    "interval": 30 * 60,          # 30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                    "last_run": 0,
                    "func": self._weekly_push_report,
                    "schedule_check": "weekly",   # æ ‡è®°ä¸ºå‘¨ä»»åŠ¡
                    "weekday": 0,                 # å‘¨ä¸€
                    "hour_range": (8, 10)         # ä»…åœ¨8-10ç‚¹æ—¶æ®µæ£€æŸ¥
                }
            }
            
            import time
            while self._running:
                now = time.time()
                current_datetime = datetime.now()
                
                for name, config in tasks.items():
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—¶é—´åˆ¤æ–­
                    if config.get("schedule_check") == "weekly":
                        # å‘¨ä»»åŠ¡æ—¶é—´åˆ¤æ–­
                        if current_datetime.weekday() != config.get("weekday", 0):
                            continue
                        hour_range = config.get("hour_range", (0, 24))
                        if not (hour_range[0] <= current_datetime.hour < hour_range[1]):
                            continue
                    
                    if now - config["last_run"] >= config["interval"]:
                        try:
                            self._executor.submit(config["func"])
                            config["last_run"] = now
                            print(f"â° [è°ƒåº¦] å·²è§¦å‘: {name}")
                        except Exception as e:
                            print(f"â° [è°ƒåº¦] è§¦å‘å¤±è´¥ {name}: {e}")
                
                # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                time.sleep(60)
        
        # åœ¨åå°çº¿ç¨‹è¿è¡Œè°ƒåº¦å¾ªç¯
        thread = threading.Thread(target=scheduler_loop, daemon=True, name="scheduler-main")
        thread.start()
        print("â° å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self._running = False
        self._executor.shutdown(wait=False)
        print("â° å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = BackgroundScheduler()
