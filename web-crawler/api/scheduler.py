# coding=utf-8
"""
åå°å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨

åŠŸèƒ½ï¼š
1. æœåŠ¡å¯åŠ¨æ—¶è‡ªåŠ¨é¢„çƒ­ç¼“å­˜
2. å®šæœŸåˆ·æ–°å„åˆ†ç±»æ•°æ®
3. é¿å…ç”¨æˆ·ç­‰å¾…çˆ¬è™«

è°ƒåº¦ç­–ç•¥ï¼š
- å¯åŠ¨æ—¶ï¼šé¢„çƒ­æ‰€æœ‰åˆ†ç±»ç¼“å­˜
- æ¯ 30 åˆ†é’Ÿï¼šåˆ·æ–°è´¢ç»ã€ç§‘æŠ€ã€æ–°é—»
- æ¯ 15 åˆ†é’Ÿï¼šåˆ·æ–°å¤§å®—å•†å“æ•°æ®
- æ¯ 10 åˆ†é’Ÿï¼šåˆ·æ–°ä¾›åº”é“¾ã€å…³ç¨æ–°é—»
"""

import asyncio
import os
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from typing import Callable, Dict, List

from .cache import cache, CACHE_TTL


class BackgroundScheduler:
    """åå°ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self):
        self._executor = ThreadPoolExecutor(max_workers=5, thread_name_prefix="scheduler")
        self._running = False
        self._tasks: Dict[str, dict] = {}
        self._test_env = "PYTEST_CURRENT_TEST" in os.environ
    
    def _crawl_category(self, category: str, include_custom: bool = True):
        """çˆ¬å–æŒ‡å®šåˆ†ç±»"""
        try:
            from scrapers.unified import UnifiedDataSource
            
            print(f"â° [å®šæ—¶] å¼€å§‹çˆ¬å– {category}...")
            unified = UnifiedDataSource()
            data = unified.crawl_category(category, include_custom=include_custom)
            
            # 1. å†™å…¥ MongoDB
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled:
                    # å°†æ™®é€šå­—å…¸è½¬æ¢ä¸º News å¯¹è±¡
                    from database.models import News
                    news_objects = []
                    for item in data:
                        # å¤„ç†æ—¶é—´
                        p_time = item.get("time")
                        published_at = None
                        if p_time:
                            try:
                                if isinstance(p_time, str):
                                    published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                                else:
                                    published_at = p_time
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()

                        news_objects.append(News(
                            platform_id=item.get("platform", "unknown"),
                            title=item.get("title", ""),
                            url=item.get("url", ""),
                            published_at=published_at,
                            category=category,
                            extra_data=item,
                            source=item.get("source", ""),
                            platform_name=item.get("platform_name") or item.get("source", ""),
                            summary=item.get("summary", "") or item.get("content", "")[:200]
                        ))
                    
                    inserted, updated = db_manager.news_repo.insert_batch(news_objects)
                    print(f"âœ… [å®šæ—¶] {category} å½’æ¡£åˆ° MongoDB: æ–°å¢ {inserted}, æ›´æ–° {updated}")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å½’æ¡£å¤±è´¥: {e}")

            result = {
                "status": "success",
                "category": category,
                "data": data,
                "timestamp": datetime.now().isoformat(),
                "total": len(data),
                "cached": False,
                "scheduled_refresh": True
            }
            
            # é»˜è®¤å…ˆè®¾ç½®å½“å‰æŠ“å–ç»“æœä¸ºç¼“å­˜
            final_result = result
            cache_key = f"news:{category}"

            # å°è¯•ä» MongoDB è·å–æœ€è¿‘ 7 å¤©çš„å…¨é‡æ•°æ®
            # åªè¦å¯ç”¨äº† MongoDBï¼Œå°±å°è¯•åˆå¹¶å†å²æ•°æ®ï¼Œç¡®ä¿å±•ç¤ºå®Œæ•´
            from api.routes.news import _try_get_from_mongodb_daily
            daily_data = _try_get_from_mongodb_daily(category)
            
            if daily_data and daily_data.get("total", 0) > len(data):
                print(f"ğŸ”„ [å®šæ—¶] {category} ä½¿ç”¨ MongoDB æœ€è¿‘7å¤©å…¨é‡æ•°æ®æ›´æ–°ç¼“å­˜ ({daily_data.get('total')} æ¡)")
                final_result = daily_data
            
            # 2. å†™å…¥ MongoDB å¿«ç…§ (ä½œä¸º Redis çš„æŒä¹…åŒ–å¤‡ä»½)
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled:
                    # å¿«ç…§ä¹Ÿä½¿ç”¨ final_result (å…¨é‡æ•°æ®)
                    db_manager.news_repo.save_snapshot(cache_key, final_result)
                    print(f"âœ… [å®šæ—¶] {category} å¿«ç…§å·²ä¿å­˜åˆ° MongoDB (åŒ…å« {final_result.get('total', 0)} æ¡)")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å¿«ç…§ä¿å­˜å¤±è´¥: {e}")
            
            # 3. å†™å…¥ Redis
            cache.set(cache_key, final_result, ttl=CACHE_TTL)
                
            print(f"â° [å®šæ—¶] {category} å®Œæˆ: {len(data)} æ¡ (æœ€ç»ˆç¼“å­˜: {final_result.get('total', 0)} æ¡)")
        except Exception as e:
            print(f"â° [å®šæ—¶] {category} å¤±è´¥: {e}")
    
    def _crawl_commodity_data(self):
        """çˆ¬å–å¤§å®—å•†å“æ•°æ®"""
        try:
            from scrapers.commodity import CommodityScraper
            from database.manager import db_manager
            
            print(f"â° [å®šæ—¶] å¼€å§‹çˆ¬å–å¤§å®—å•†å“æ•°æ®...")
            scraper = CommodityScraper()
            raw_data = scraper.scrape()
            print(f"âœ… [Scheduler] Scraped {len(raw_data)} commodity items")
            
            # 1. å†™å…¥ MySQLï¼ˆPipeline ä¼šè‡ªåŠ¨å»é‡ï¼‰ï¼ŒæŒ‰æ¥æºåˆ†ç»„
            try:
                stats_by_source = {}
                sources = set(item.get("source", "unknown") for item in raw_data)
                for src in sources:
                    src_records = [item for item in raw_data if item.get("source", "unknown") == src]
                    if not src_records:
                        continue
                    db_stats = db_manager.write_commodity(src_records, source=src)
                    if db_stats:
                        stats_by_source[src] = db_stats
                if stats_by_source:
                    print(f"âœ… [å®šæ—¶] MySQL å…¥åº“å®Œæˆ: {stats_by_source}")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MySQL å…¥åº“å¤±è´¥: {e}")

            # 2. ä» MySQL commodity_latest è¯»å–å»é‡åçš„æ•°æ®ï¼ˆä»¥ MySQL ä¸ºå‡†ï¼‰
            try:
                latest_data = db_manager.get_commodity_latest()
                if not latest_data:
                    print("âš ï¸ [å®šæ—¶] MySQL commodity_latest ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                    latest_data = raw_data
                else:
                    print(f"âœ… [å®šæ—¶] ä» MySQL è¯»å–: {len(latest_data)} æ¡å»é‡æ•°æ®")
                    # å­—æ®µæ˜ å°„ï¼šMySQL â†’ API æ ¼å¼ (å†…è”å®ç°ï¼Œé¿å…å¾ªç¯ä¾èµ–)
                    for item in latest_data:
                        # 1. åˆå¹¶ unit
                        price_unit = item.get('price_unit', '')
                        weight_unit = item.get('weight_unit', '')
                        if price_unit and weight_unit:
                            item['unit'] = f"{price_unit}/{weight_unit}"
                        else:
                            item['unit'] = price_unit or weight_unit or 'USD'
                        
                        # 2. current_price
                        if 'price' in item and 'current_price' not in item:
                            item['current_price'] = item['price']
                        
                        # 3. url
                        if 'url' not in item or not item['url']:
                            item['url'] = item.get('source_url', '')

                        # 4. cleanup
                        for k in ['id', 'price_unit', 'weight_unit', 'version_ts', 'source_url']:
                            item.pop(k, None)

            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] ä» MySQL è¯»å–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                latest_data = raw_data
            
            # 3. æ’åº
            category_order = {'è´µé‡‘å±': 0, 'èƒ½æº': 1, 'å·¥ä¸šé‡‘å±': 2, 'å†œäº§å“': 3, 'å…¶ä»–': 4}
            latest_data.sort(key=lambda x: category_order.get(x.get('category', 'å…¶ä»–'), 4))
            
            # 4. å†™å…¥ Redis
            result = {
                "data": latest_data,
                "source": "TrendRadar Commodity",
                "timestamp": datetime.now().isoformat(),
                "cached": False,
                "scheduled_refresh": True,
                "from_mysql": True,
                "categories": list(set(item.get('category', 'å…¶ä»–') for item in latest_data)),
                "total": len(latest_data)
            }
            cache.set("data:commodity", result, ttl=CACHE_TTL)
            
            print(f"â° [å®šæ—¶] å¤§å®—å•†å“æ•°æ®å®Œæˆ: {len(latest_data)} æ¡")
        except Exception as e:
            print(f"â° [å®šæ—¶] å¤§å®—å•†å“æ•°æ®å¤±è´¥: {e}")
    
    def _fetch_realtime_news(self, cache_key: str, keywords: list, category: str = None):
        """æŠ“å–å®æ—¶æ–°é—»"""
        try:
            from api.routes.analysis import fetch_realtime_news
            from api.routes.news import _fetch_power_partner_news, _fetch_power_official_announcements
            
            print(f"â° [å®šæ—¶] å¼€å§‹æŠ“å– {cache_key}...")
            news = fetch_realtime_news(keywords)
            if category == "supply-chain":
                power_news = _fetch_power_partner_news()
                official_news = _fetch_power_official_announcements()
                seen = {n.get("title") for n in news}
                for item in power_news + official_news:
                    if item.get("title") and item["title"] not in seen:
                        seen.add(item["title"])
                        news.append(item)
            
            # è¡¥å…¨ platform_name (ç¡®ä¿å…¥åº“å’Œç¼“å­˜éƒ½æœ‰è¯¥å­—æ®µ)
            for item in news:
                if not item.get("platform_name") and item.get("source"):
                    item["platform_name"] = item["source"]
            
            # ç»Ÿè®¡æ•°æ®æ¥æºåˆ†å¸ƒ
            sources = {}
            for item in news:
                # æŒ‰ç…§ä¼˜å…ˆçº§æå–æ¥æºï¼šplatform_name > source > platform > æœªçŸ¥
                # ä¿®å¤é€»è¾‘ï¼šä¼˜å…ˆå– platform_name æˆ– sourceï¼Œé¿å…å–åˆ° newsnow
                source_name = item.get('platform_name') or item.get('source') or item.get('platform') or 'æœªçŸ¥'
                sources[source_name] = sources.get(source_name, 0) + 1
            
            # 1. å†™å…¥ MongoDB
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled and category:
                    from database.models import News
                    news_objects = []
                    for item in news:
                        # å¤„ç†æ—¶é—´
                        p_time = item.get("time")
                        published_at = None
                        if p_time:
                            try:
                                if isinstance(p_time, str):
                                    published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                                else:
                                    published_at = p_time
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()

                        news_objects.append(News(
                            platform_id=item.get("platform", "unknown"),
                            title=item.get("title", ""),
                            url=item.get("url", ""),
                            # source removed
                            published_at=published_at,
                            category=category,
                            extra_data=item,
                            source=item.get("source", ""),
                            platform_name=item.get("platform_name", ""),
                            summary=item.get("summary", "") or item.get("content", "")[:200]
                        ))
                    
                    inserted, updated = db_manager.news_repo.insert_batch(news_objects)
                    print(f"âœ… [å®šæ—¶] {category} (å®æ—¶) å½’æ¡£åˆ° MongoDB: æ–°å¢ {inserted}, æ›´æ–° {updated}")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å½’æ¡£å¤±è´¥: {e}")
            
            result = {
                "status": "success",
                "data": news,
                "timestamp": datetime.now().isoformat(),
                "total": len(news),
                "sources": sources,  # æ·»åŠ  sources ç»Ÿè®¡
                "cached": False,
                "scheduled_refresh": True
            }

            # å°è¯•ä» MongoDB è·å–æœ€è¿‘ 7 å¤©çš„å…¨é‡æ•°æ®
            # åªè¦å¯ç”¨äº† MongoDBï¼Œå°±å°è¯•åˆå¹¶å†å²æ•°æ®ï¼Œç¡®ä¿å±•ç¤ºå®Œæ•´
            from api.routes.news import _try_get_from_mongodb_daily
            daily_data = _try_get_from_mongodb_daily(category, keywords) # keywords å¯èƒ½ä¸ºç©ºï¼Œä½†åœ¨ _try_get_from_mongodb_daily å†…éƒ¨å¤„ç†äº†
            
            if daily_data and daily_data.get("total", 0) > len(news):
                print(f"ğŸ”„ [å®šæ—¶] {cache_key} ä½¿ç”¨ MongoDB æœ€è¿‘7å¤©å…¨é‡æ•°æ®æ›´æ–°ç¼“å­˜ ({daily_data.get('total')} æ¡)")
                final_result = daily_data
            
            # 2. å†™å…¥ MongoDB å¿«ç…§ (ä½œä¸º Redis çš„æŒä¹…åŒ–å¤‡ä»½)
            try:
                from database.manager import db_manager
                if db_manager.mongodb_enabled:
                    # å¿«ç…§ä¹Ÿä½¿ç”¨ final_result (å…¨é‡æ•°æ®)
                    db_manager.news_repo.save_snapshot(cache_key, final_result)
                    print(f"âœ… [å®šæ—¶] {cache_key} å¿«ç…§å·²ä¿å­˜åˆ° MongoDB (åŒ…å« {final_result.get('total', 0)} æ¡)")
            except Exception as e:
                print(f"âš ï¸ [å®šæ—¶] MongoDB å¿«ç…§ä¿å­˜å¤±è´¥: {e}")

            # 3. å†™å…¥ Redis
            cache.set(cache_key, final_result, ttl=CACHE_TTL)
            
            print(f"â° [å®šæ—¶] {cache_key} å®Œæˆ: {len(news)} æ¡ (æœ€ç»ˆç¼“å­˜: {final_result.get('total', 0)} æ¡)")
        except Exception as e:
            print(f"â° [å®šæ—¶] {cache_key} å¤±è´¥: {e}")
    
    def warmup_cache(self):
        """é¢„çƒ­ç¼“å­˜ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
        if self._test_env:
            print("ğŸ”¥ è·³è¿‡æµ‹è¯•ç¯å¢ƒä¸‹çš„é¢„çƒ­ä»»åŠ¡")
            return
        print("ğŸ”¥ å¼€å§‹é¢„çƒ­ç¼“å­˜...")
        
        # ä» news.py å¯¼å…¥ç»Ÿä¸€çš„å…³é”®è¯é…ç½®
        from .routes.news import SUPPLY_CHAIN_KEYWORDS, TARIFF_KEYWORDS, PLASTICS_KEYWORDS
        supply_chain_keywords = SUPPLY_CHAIN_KEYWORDS
        tariff_keywords = TARIFF_KEYWORDS
        plastics_keywords = PLASTICS_KEYWORDS
        
        # é¢„çƒ­ä»»åŠ¡åˆ—è¡¨
        warmup_tasks = [
            ("å¤§å®—å•†å“æ•°æ®", self._crawl_commodity_data),
            ("è´¢ç»æ–°é—»", lambda: self._crawl_category("finance")),
            ("ç§‘æŠ€æ–°é—»", lambda: self._crawl_category("tech")),
            ("ä¾›åº”é“¾æ–°é—»", lambda: self._fetch_realtime_news("news:supply-chain", supply_chain_keywords, "supply-chain")),
            ("å…³ç¨æ–°é—»", lambda: self._fetch_realtime_news("news:tariff", tariff_keywords, "tariff")),
            ("å¡‘æ–™æ–°é—»", lambda: self._fetch_realtime_news("news:plastics", plastics_keywords, "plastics")),
            ("å¤§å®—å•†å“æ–°é—»", lambda: self._crawl_category("commodity")),
        ]
        
        for name, task in warmup_tasks:
            try:
                self._executor.submit(task)
                print(f"  ğŸ“Œ å·²æäº¤: {name}")
            except Exception as e:
                print(f"  âŒ æäº¤å¤±è´¥ {name}: {e}")
        
        print("ğŸ”¥ é¢„çƒ­ä»»åŠ¡å·²å…¨éƒ¨æäº¤")
    
    def start_scheduled_tasks(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼ˆéé˜»å¡ï¼‰"""
        if self._running:
            return
        
        self._running = True
        if self._test_env:
            # æµ‹è¯•ç¯å¢ƒä¸å¯åŠ¨åå°å¾ªç¯ï¼Œé¿å…å¹²æ‰°å…¶å®ƒç”¨ä¾‹çš„ mock
            return
        
        def scheduler_loop():
            """è°ƒåº¦å¾ªç¯"""
            # ä» news.py å¯¼å…¥ç»Ÿä¸€çš„å…³é”®è¯é…ç½®
            from .routes.news import SUPPLY_CHAIN_KEYWORDS, TARIFF_KEYWORDS
            supply_chain_keywords = SUPPLY_CHAIN_KEYWORDS
            tariff_keywords = TARIFF_KEYWORDS
            
            # ä»»åŠ¡é…ç½®ï¼š(é—´éš”ç§’æ•°, ä¸Šæ¬¡æ‰§è¡Œæ—¶é—´, ä»»åŠ¡å‡½æ•°)
            tasks = {
                "commodity_data": {
                    "interval": 15 * 60,  # 15åˆ†é’Ÿ
                    "last_run": 0,
                    "func": self._crawl_commodity_data
                },
                "finance_news": {
                    "interval": 30 * 60,  # 30åˆ†é’Ÿ
                    "last_run": 0,
                    "func": lambda: self._crawl_category("finance")
                },
                "tech_news": {
                    "interval": 30 * 60,
                    "last_run": 0,
                    "func": lambda: self._crawl_category("tech")
                },
                "supply_chain": {
                    "interval": 10 * 60,  # 10åˆ†é’Ÿ
                    "last_run": 0,
                    "func": lambda: self._fetch_realtime_news("news:supply-chain", supply_chain_keywords, "supply-chain")
                },
                "tariff": {
                    "interval": 10 * 60,
                    "last_run": 0,
                    "func": lambda: self._fetch_realtime_news("news:tariff", tariff_keywords, "tariff")
                }
            }
            
            import time
            while self._running:
                now = time.time()
                
                for name, config in tasks.items():
                    if now - config["last_run"] >= config["interval"]:
                        try:
                            self._executor.submit(config["func"])
                            config["last_run"] = now
                            print(f"â° [è°ƒåº¦] å·²è§¦å‘: {name}")
                        except Exception as e:
                            print(f"â° [è°ƒåº¦] è§¦å‘å¤±è´¥ {name}: {e}")
                
                # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                time.sleep(60)
        
        # åœ¨åå°çº¿ç¨‹è¿è¡Œè°ƒåº¦å¾ªç¯
        thread = threading.Thread(target=scheduler_loop, daemon=True, name="scheduler-main")
        thread.start()
        print("â° å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self._running = False
        self._executor.shutdown(wait=False)
        print("â° å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = BackgroundScheduler()
