import asyncio
import json
import logging
import sys
import os
from datetime import datetime
from typing import List, Dict

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from redis import Redis
from database.manager import db_manager
from database.models import News
from api.cache import CACHE_TTL

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("MigrateHistory")

# Redis é…ç½® (éœ€ä¸ database.yaml ä¿æŒä¸€è‡´)
REDIS_HOST = "10.180.248.145"
REDIS_PORT = 6379
REDIS_DB = 0

class HistoryMigrator:
    """å†å²æ•°æ®è¿ç§»å™¨ï¼šå°† Redis ä¸­çš„æ—§æ•°æ®åŒæ­¥åˆ° MongoDB"""
    
    def __init__(self):
        self.redis = Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)
        if not db_manager.mongodb_enabled:
            raise RuntimeError("MongoDB æœªå¯ç”¨ï¼Œæ— æ³•è¿ç§»")
        self.news_repo = db_manager.news_repo
        self.commodity_repo = db_manager.commodity_repo

    def migrate_news(self):
        """è¿ç§»æ–°é—»æ•°æ® (news:*)"""
        logger.info("ğŸ“° å¼€å§‹è¿ç§»æ–°é—»æ•°æ®...")
        cursor = '0'
        total = 0
        
        while True:
            cursor, keys = self.redis.scan(cursor=cursor, match="news:*", count=100)
            if not keys:
                if cursor == '0':
                    break
                continue
                
            for key in keys:
                try:
                    # è¯»å– Redis æ•°æ®
                    raw_data = self.redis.get(key)
                    if not raw_data:
                        continue
                    
                    data = json.loads(raw_data)
                    category = data.get("category", key.split(":")[-1])
                    items = data.get("data", [])
                    
                    if not items:
                        continue

                    # è½¬æ¢ä¸º News å¯¹è±¡
                    news_objects = []
                    for item in items:
                        # å¤„ç†æ—¶é—´
                        p_time = item.get("time")
                        published_at = None
                        if p_time:
                            try:
                                if isinstance(p_time, str):
                                    published_at = datetime.fromisoformat(p_time.replace('Z', '+00:00'))
                                else:
                                    published_at = datetime.now() # æ— æ³•è§£æåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
                            except:
                                published_at = datetime.now()
                        else:
                            published_at = datetime.now()

                        # source å­—æ®µå…¼å®¹
                        source = item.get("source", "")
                        extra_data = item.copy()
                        extra_data["source"] = source

                        news = News(
                            platform_id=item.get("platform", "unknown"),
                            title=item.get("title", ""),
                            url=item.get("url", ""),
                            published_at=published_at,
                            category=category,
                            extra_data=extra_data
                        )
                        news_objects.append(news)
                    
                    # æ‰¹é‡å†™å…¥ MongoDB (ä¼šè‡ªåŠ¨å»é‡)
                    inserted, updated = self.news_repo.insert_batch(news_objects)
                    logger.info(f"âœ… å¤„ç† Key {key}: {len(items)} æ¡ -> æ–°å¢ {inserted}, æ›´æ–° {updated}")
                    total += len(items)
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç† Key {key} å¤±è´¥: {e}")
            
            if cursor == '0':
                break
        
        logger.info(f"ğŸ“° æ–°é—»æ•°æ®è¿ç§»å®Œæˆï¼Œå…±å¤„ç† {total} æ¡è®°å½•")

    def migrate_commodity(self):
        """è¿ç§»å¤§å®—å•†å“æ•°æ® (data:commodity)"""
        logger.info("ğŸ“Š å¼€å§‹è¿ç§»å¤§å®—å•†å“æ•°æ®...")
        key = "data:commodity"
        
        try:
            raw_data = self.redis.get(key)
            if not raw_data:
                logger.warning("âš ï¸ Redis ä¸­æœªæ‰¾åˆ° data:commodity")
                return
                
            data = json.loads(raw_data)
            items = data.get("data", [])
            
            if items:
                # å†™å…¥ MongoDB
                count = self.commodity_repo.save_batch(items)
                logger.info(f"âœ… å¤§å®—å•†å“æ•°æ®è¿ç§»å®Œæˆ: {count} æ¡")
            else:
                logger.info("âš ï¸ å¤§å®—å•†å“æ•°æ®ä¸ºç©º")
                
        except Exception as e:
            logger.error(f"âŒ å¤§å®—å•†å“æ•°æ®è¿ç§»å¤±è´¥: {e}")

    def run(self):
        try:
            self.migrate_news()
            self.migrate_commodity()
            logger.info("ğŸ‰ æ‰€æœ‰å†å²æ•°æ®è¿ç§»ä»»åŠ¡å®Œæˆï¼")
        except Exception as e:
            logger.critical(f"â›” è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

if __name__ == "__main__":
    migrator = HistoryMigrator()
    migrator.run()
