#!/usr/bin/env python3
"""
ä¸­å¡‘åœ¨çº¿ 21CP å¡‘æ–™ä»·æ ¼å†å²æ•°æ®å…¨é‡å…¥åº“è„šæœ¬

ç”¨æ³•:
    python scripts/fetch_plastic_full.py                     # è·å–æ‰€æœ‰äº§å“
    python scripts/fetch_plastic_full.py --product abs_south # æŒ‡å®šäº§å“
    python scripts/fetch_plastic_full.py --start 2020-01-01  # æŒ‡å®šå¼€å§‹æ—¥æœŸ
    python scripts/fetch_plastic_full.py --discover          # å‘ç°æ›´å¤šäº§å“SID
    python scripts/fetch_plastic_full.py --dry-run           # åªé¢„è§ˆï¼Œä¸å…¥åº“
"""
import sys
import argparse
from pathlib import Path
from datetime import date

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT_DIR))

from scrapers.plastic21cp import Plastic21CPScraper


def discover_product_sids():
    """
    ä½¿ç”¨ Playwright å‘ç°æ›´å¤šäº§å“çš„ SID
    è®¿é—®é¡µé¢å¹¶æå– API è°ƒç”¨ä¸­çš„ avgMarketAreaProductSid
    """
    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… playwright: pip install playwright && playwright install chromium")
        return []
    
    import time
    import re
    
    # å·²çŸ¥çš„äº§å“é¡µé¢
    product_pages = [
        ("ABS", "https://quote.21cp.com/avg_area/list/303561829995569152-ABS.html"),
        # å¯ä»¥æ·»åŠ æ›´å¤šé¡µé¢
    ]
    
    discovered = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        )
        
        for product_name, url in product_pages:
            page = context.new_page()
            sids = []
            
            def on_response(response):
                if 'avgMarketAreaProduct/api/listHistory' in response.url:
                    try:
                        match = re.search(r'avgMarketAreaProductSid=(\d+)', response.url)
                        if match:
                            sid = match.group(1)
                            # è·å–å“åº”æ•°æ®ä»¥æå–åŒºåŸŸåç§°
                            data = response.json()
                            if data.get('code') == 200 and data.get('data'):
                                area = data['data'][0].get('marketAreaName', 'æœªçŸ¥åŒºåŸŸ')
                                sids.append({
                                    'sid': sid,
                                    'area': area,
                                    'product': product_name
                                })
                    except:
                        pass
            
            page.on('response', on_response)
            
            print(f"ğŸ“¡ æ‰«æ {product_name} é¡µé¢...")
            page.goto(url, timeout=30000)
            time.sleep(5)
            
            discovered.extend(sids)
            page.close()
        
        browser.close()
    
    return discovered


def main():
    parser = argparse.ArgumentParser(
        description="ä¸­å¡‘åœ¨çº¿ 21CP å¡‘æ–™ä»·æ ¼å…¨é‡å†å²æ•°æ®å…¥åº“"
    )
    parser.add_argument(
        "--product",
        default=None,
        help="äº§å“ç±»å‹ (å¦‚ abs_south)ï¼Œé»˜è®¤å¤„ç†æ‰€æœ‰äº§å“"
    )
    parser.add_argument(
        "--start", 
        default="2020-01-01",
        help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD), é»˜è®¤: 2020-01-01"
    )
    parser.add_argument(
        "--end",
        default=None,
        help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD), é»˜è®¤: ä»Šå¤©"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=500,
        help="æ¯æ‰¹å¤„ç†è®°å½•æ•°, é»˜è®¤: 500"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…å…¥åº“"
    )
    parser.add_argument(
        "--discover",
        action="store_true",
        help="å‘ç°æ›´å¤šäº§å“ SID"
    )
    parser.add_argument(
        "--to-redis",
        action="store_true",
        help="åŒæ—¶å†™å…¥ Redis å†å²ç¼“å­˜"
    )
    
    args = parser.parse_args()
    
    # å‘ç°æ¨¡å¼
    if args.discover:
        print("ğŸ” å‘ç°äº§å“ SID...")
        discovered = discover_product_sids()
        if discovered:
            print(f"\nå‘ç° {len(discovered)} ä¸ªäº§å“:")
            for item in discovered:
                print(f"  {item['product']}({item['area']}): {item['sid']}")
            print("\nå°†ä»¥ä¸Š SID æ·»åŠ åˆ° scrapers/plastic21cp.py çš„ PRODUCTS å­—å…¸ä¸­")
        else:
            print("æœªå‘ç°æ–°äº§å“")
        return 0
    
    end_date = args.end or date.today().isoformat()
    scraper = Plastic21CPScraper()
    
    # ç¡®å®šè¦å¤„ç†çš„äº§å“
    if args.product:
        products = [args.product]
    else:
        products = scraper.list_products()
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ä¸­å¡‘åœ¨çº¿ 21CP å¡‘æ–™ä»·æ ¼å…¨é‡å†å²æ•°æ®å…¥åº“            â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  äº§å“: {', '.join(products):<45}â•‘
â•‘  æ—¶é—´èŒƒå›´: {args.start} â†’ {end_date:<25}â•‘
â•‘  æ‰¹æ¬¡å¤§å°: {args.batch_size:<41}â•‘
â•‘  æ¨¡å¼: {'é¢„è§ˆ (DRY RUN)' if args.dry_run else 'æ­£å¼å…¥åº“':<43}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    total_inserted = 0
    total_updated = 0
    total_records = 0
    
    for product in products:
        print(f"\nğŸ“¥ æ­£åœ¨è·å– {product} å†å²æ•°æ®...")
        records = scraper.fetch(product, start_date=args.start, end_date=end_date)
        
        if not records:
            print(f"  âš ï¸ æœªè·å–åˆ°æ•°æ®")
            continue
        
        total_records += len(records)
        
        # æŒ‰æ—¥æœŸæ’åº
        records.sort(key=lambda x: x.get("price_date", ""))
        
        # é¢„è§ˆå‰å‡ æ¡
        print(f"  ğŸ“‹ æ•°æ®é¢„è§ˆ (å‰3æ¡):")
        for r in records[:3]:
            change = r.get('change_percent') or 0
            print(f"    {r['price_date']}: Â¥{r['price']:.2f} ({change:+.2f}%)")
        if len(records) > 3:
            print(f"    ... è¿˜æœ‰ {len(records) - 3} æ¡")
        
        if args.dry_run:
            continue
        
        # å…¥åº“ MySQL
        print(f"\n  ğŸ’¾ å¼€å§‹å…¥åº“...")
        try:
            from database.mysql.pipeline import CommodityPipeline
            pipeline = CommodityPipeline()
            
            for i in range(0, len(records), args.batch_size):
                batch = records[i:i + args.batch_size]
                batch_num = i // args.batch_size + 1
                total_batches = (len(records) + args.batch_size - 1) // args.batch_size
                
                print(f"    å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡)...", end=" ")
                
                stats = pipeline.process_batch(batch, source="ä¸­å¡‘åœ¨çº¿")
                total_inserted += stats.get("inserted", 0)
                total_updated += stats.get("updated", 0)
                print(f"âœ“ æ–°å¢:{stats.get('inserted', 0)} æ›´æ–°:{stats.get('updated', 0)}")
                
        except Exception as e:
            print(f"  âŒ å…¥åº“å¤±è´¥: {e}")
        
        # å†™å…¥ Redis
        if args.to_redis:
            print(f"\n  ğŸ“¡ å†™å…¥ Redis...")
            try:
                from core.price_history import PriceHistoryManager
                pm = PriceHistoryManager()
                
                product_info = scraper.PRODUCTS.get(product, {})
                redis_name = product_info.get("name", product)
                
                for r in records:
                    pm.save_daily_price(
                        commodity_name=redis_name,
                        price=r["price"],
                        change_percent=r.get("change_percent") or 0,
                        source="ä¸­å¡‘åœ¨çº¿",
                        date=r["price_date"]
                    )
                print(f"    âœ… å·²å†™å…¥ {len(records)} æ¡åˆ° Redis")
            except Exception as e:
                print(f"    âŒ Redis å†™å…¥å¤±è´¥: {e}")
    
    if args.dry_run:
        print(f"\nğŸ” é¢„è§ˆæ¨¡å¼ï¼Œæœªæ‰§è¡Œå…¥åº“æ“ä½œ")
        print(f"   å…±è·å– {total_records} æ¡è®°å½•")
    else:
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    å…¥åº“å®Œæˆ                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æ€»è®°å½•æ•°: {total_records:<41}â•‘
â•‘  æ–°å¢: {total_inserted:<45}â•‘
â•‘  æ›´æ–°: {total_updated:<45}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
