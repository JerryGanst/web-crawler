"""
æŒ‰åˆ†ç±»çˆ¬å–æ–°é—»å¹¶æ¨é€ï¼ˆç»Ÿä¸€æ•°æ®æºç‰ˆæœ¬ï¼‰
ç”¨æ³•:
  python crawl_by_category.py finance   # åªçˆ¬è´¢ç»ç±»ï¼ˆå«è‡ªå®šä¹‰æ•°æ®æºï¼‰
  python crawl_by_category.py news      # åªçˆ¬æ–°é—»ç±»
  python crawl_by_category.py tech      # åªçˆ¬ç§‘æŠ€ç±»ï¼ˆå« Hacker Newsï¼‰
  python crawl_by_category.py all       # çˆ¬æ‰€æœ‰åˆ†ç±»
  
  python crawl_by_category.py finance --no-custom  # ä¸å«è‡ªå®šä¹‰æ•°æ®æº
"""
import sys
import yaml

# åŠ è½½é…ç½®
with open("config/config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# ä¼ä¸šå¾®ä¿¡ webhook
WEWORK_URL = config.get("notification", {}).get("webhooks", {}).get("wework_url", "")

def get_platforms_by_category(category: str) -> list:
    """æ ¹æ®åˆ†ç±»è·å–å¹³å°åˆ—è¡¨"""
    platforms = config.get("platforms", [])
    if category == "all":
        return platforms
    return [p for p in platforms if p.get("category") == category]

def fetch_data(platform_id: str, max_retries: int = 2) -> dict:
    """ä» API è·å–æ•°æ®ï¼Œæ”¯æŒé‡è¯•"""
    url = f"https://newsnow.busiyi.world/api/s?id={platform_id}&latest"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "application/json, text/plain, */*",
    }
    
    for retry in range(max_retries + 1):
        try:
            resp = requests.get(url, headers=headers, timeout=15)
            resp.raise_for_status()
            data = resp.json()
            if data.get("status") in ["success", "cache"]:
                # è½¬æ¢ items ä¸º data æ ¼å¼
                if "items" in data:
                    data["data"] = data["items"]
                return data
        except Exception as e:
            if retry < max_retries:
                wait = random.uniform(2, 4) + retry * 2
                time.sleep(wait)
    return {}

def crawl_category(category: str) -> list:
    """çˆ¬å–æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰å¹³å°"""
    platforms = get_platforms_by_category(category)
    if not platforms:
        print(f"âŒ æœªæ‰¾åˆ°åˆ†ç±»: {category}")
        return []
    
    category_name = config.get("categories", {}).get(category, {}).get("name", category)
    print(f"\nğŸ“‚ æ­£åœ¨çˆ¬å–ã€{category_name}ã€‘åˆ†ç±» ({len(platforms)} ä¸ªå¹³å°)")
    print("=" * 50)
    
    all_news = []
    for p in platforms:
        pid = p["id"]
        pname = p["name"]
        print(f"  ğŸ”„ {pname} ({pid})...", end=" ")
        
        data = fetch_data(pid)
        if data and "data" in data:
            items = data["data"]
            for item in items:
                item["platform"] = pid
                item["platform_name"] = pname
                item["category"] = category
            all_news.extend(items)
            status = "æœ€æ–°" if data.get("status") == "success" else "ç¼“å­˜"
            print(f"âœ… {len(items)} æ¡ ({status})")
        else:
            print("âŒ å¤±è´¥")
        
        time.sleep(random.uniform(0.5, 1.5))
    
    print(f"\nğŸ“Š å…±è·å– {len(all_news)} æ¡æ–°é—»")
    return all_news

def push_to_wework(news_list: list, category: str):
    """æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆåˆ†æ‰¹å‘é€ï¼Œä¿ç•™é“¾æ¥ï¼‰"""
    if not WEWORK_URL:
        print("âŒ æœªé…ç½®ä¼ä¸šå¾®ä¿¡ webhook")
        return
    
    if not news_list:
        print("âŒ æ²¡æœ‰æ–°é—»å¯æ¨é€")
        return
    
    category_name = config.get("categories", {}).get(category, {}).get("name", category)
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    # æŒ‰å¹³å°åˆ†ç»„
    by_platform = {}
    for news in news_list:
        pname = news.get("platform_name", "æœªçŸ¥")
        if pname not in by_platform:
            by_platform[pname] = []
        by_platform[pname].append(news)
    
    # æ„å»ºåˆ†æ‰¹æ¶ˆæ¯ï¼ˆæ¯æ‰¹ä¸€ä¸ªå¹³å°ï¼Œä¿ç•™é“¾æ¥ï¼‰
    batches = []
    batch_num = 1
    
    for pname, items in by_platform.items():
        lines = [f"## ğŸ“Š {category_name}çƒ­ç‚¹ ({now}) [{batch_num}]\n"]
        lines.append(f"### ğŸ“° {pname}")
        for i, item in enumerate(items[:10], 1):
            title = item.get("title", "")
            url = item.get("url") or item.get("mobileUrl") or ""
            if url:
                lines.append(f"{i}. [{title}]({url})")
            else:
                lines.append(f"{i}. {title}")
        batches.append("\n".join(lines))
        batch_num += 1
    
    print(f"\nğŸ“¤ æ­£åœ¨æ¨é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆå…± {len(batches)} æ‰¹ï¼‰...")
    
    success_count = 0
    for i, batch in enumerate(batches, 1):
        resp = requests.post(WEWORK_URL, json={
            "msgtype": "markdown",
            "markdown": {"content": batch}
        })
        
        if resp.status_code == 200 and resp.json().get("errcode") == 0:
            success_count += 1
        else:
            print(f"  âŒ ç¬¬ {i} æ‰¹å¤±è´¥: {resp.text}")
        
        if i < len(batches):
            time.sleep(1)  # é¿å…å‘é€è¿‡å¿«
    
    print(f"âœ… æ¨é€å®Œæˆï¼æˆåŠŸ {success_count}/{len(batches)} æ‰¹")

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python crawl_by_category.py <category> [--no-custom]")
        print("å¯ç”¨åˆ†ç±»: finance, news, social, tech, all")
        print("\né€‰é¡¹:")
        print("  --no-custom    ä¸ä½¿ç”¨è‡ªå®šä¹‰çˆ¬è™«ï¼ˆä»… newsnow APIï¼‰")
        print("  --unified      ä½¿ç”¨ç»Ÿä¸€æ•°æ®æºï¼ˆæ¨èï¼Œå«è‡ªå®šä¹‰çˆ¬è™«ï¼‰")
        return
    
    category = sys.argv[1].lower()
    include_custom = "--no-custom" not in sys.argv
    use_unified = "--unified" in sys.argv or include_custom
    
    if use_unified:
        # ä½¿ç”¨ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨ï¼ˆæ–°æ–¹å¼ï¼‰
        from scrapers.unified import UnifiedDataSource
        ds = UnifiedDataSource()
        
        if category == "all":
            for cat in ["finance", "news", "social", "tech"]:
                data = ds.crawl_category(cat, include_custom=include_custom)
                if data:
                    ds.push_to_wework(data, cat, WEWORK_URL)
        else:
            data = ds.crawl_category(category, include_custom=include_custom)
            if data:
                ds.push_to_wework(data, category, WEWORK_URL)
    else:
        # æ—§æ–¹å¼ï¼ˆä»… newsnowï¼‰
        if category == "all":
            for cat in ["finance", "news", "social", "tech"]:
                news = crawl_category(cat)
                if news:
                    push_to_wework(news, cat)
        else:
            news = crawl_category(category)
            if news:
                push_to_wework(news, category)

if __name__ == "__main__":
    main()
